{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ijGzTHJJUCPY"
   },
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VEqbX8OhE8y9"
   },
   "source": [
    "# Gemini: An Overview of Multimodal Use Cases\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/intro_multimodal_use_cases.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fuse-cases%2Fintro_multimodal_use_cases.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Run in Colab Enterprise\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/use-cases/intro_multimodal_use_cases.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/intro_multimodal_use_cases.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "\n",
    "<div style=\"clear: both;\"></div>\n",
    "\n",
    "<b>Share to:</b>\n",
    "\n",
    "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/intro_multimodal_use_cases.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/intro_multimodal_use_cases.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/intro_multimodal_use_cases.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/53/X_logo_2023_original.svg\" alt=\"X logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://reddit.com/submit?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/intro_multimodal_use_cases.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/intro_multimodal_use_cases.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
    "</a>            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8HKLOuOlJutv"
   },
   "source": [
    "| | |\n",
    "|-|-|\n",
    "|Author(s) | [Saeed Aghabozorgi](https://github.com/saeedaghabozorgi) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VK1Q5ZYdVL4Y"
   },
   "source": [
    "## Overview\n",
    "\n",
    "In this notebook, you will explore a variety of different use cases enabled by multimodality with Gemini 1.5 Flash.\n",
    "\n",
    "### Gemini\n",
    "\n",
    "Gemini is a family of generative AI models developed by Google DeepMind that is designed for multimodal use cases. The Gemini API gives you access to the Gemini 1.0 Pro Vision, Gemini 1.0 Pro, Gemini 1.5 Pro and Gemini 1.5 Flash models.\n",
    "\n",
    "### Gemini API in Vertex AI\n",
    "\n",
    "The Gemini API in Vertex AI provides a unified interface for interacting with Gemini models. There are currently four models available in the Gemini API:\n",
    "\n",
    "- **Gemini 1.0 Pro model** (`gemini-1.0-pro`): Designed to handle natural language tasks, multiturn text and code chat, and code generation.\n",
    "- **Gemini 1.0 Pro Vision model** (`gemini-1.0-pro-vision`): Supports multimodal prompts. You can include text, images, and video in your prompt requests and get text or code responses.\n",
    "- **Gemini 1.5 Pro model** (`gemini-1.5-pro`): A foundation model that performs well at a variety of multimodal tasks such as visual understanding, classification, summarization, and creating content from image, audio and video..\n",
    "- **Gemini 1.5 Flash model** (`gemini-1.5-flash`): A purpose-built multimodal model that provides speed and efficiency for high-volume, quality, cost-effective apps.\n",
    "\n",
    "For more information, see the [Generative AI on Vertex AI](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/overview) documentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RQT500QqVPIb"
   },
   "source": [
    "### Objectives\n",
    "\n",
    "This notebook demonstrates a variety of multimodal use cases that Gemini can be used for.\n",
    "\n",
    "#### Multimodal use cases\n",
    "\n",
    "Compared to text-only LLMs, Gemini 1.5's multimodality can be used for many new use-cases:\n",
    "\n",
    "Example use cases with **text and image(s)** as input:\n",
    "\n",
    "- Detecting objects in photos\n",
    "- Understanding screens and interfaces\n",
    "- Understanding of drawing and abstraction\n",
    "- Understanding charts and diagrams\n",
    "- Recommendation of images based on user preferences\n",
    "- Comparing images for similarities, anomalies, or differences\n",
    "\n",
    "Example use cases with **text and video** as input:\n",
    "\n",
    "- Generating a video description\n",
    "- Extracting tags of objects throughout a video\n",
    "- Extracting highlights/messaging of a video\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZhsUe0fyc-ER"
   },
   "source": [
    "### Costs\n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "- Vertex AI\n",
    "\n",
    "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing) and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QDU0XJ1xRDlL"
   },
   "source": [
    "## Getting Started\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5afkyDMSBW5"
   },
   "source": [
    "### Install Vertex AI SDK for Python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kc4WxYmLSBW5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade --user google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5Xep4W9lq-Z"
   },
   "source": [
    "### Restart current runtime\n",
    "\n",
    "To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which will restart the current kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XRvKdaPDTznN"
   },
   "outputs": [],
   "source": [
    "# Restart kernel after installs so that your environment can access the new packages\n",
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SbmM4z7FOBpM"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ The kernel is going to restart. Please wait until it is finished before continuing to the next step. ⚠️</b>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Fom0ZkMSBW6"
   },
   "source": [
    "### Authenticate your notebook environment (Colab only)\n",
    "\n",
    "If you are running this notebook on Google Colab, run the following cell to authenticate your environment. This step is not required if you are using [Vertex AI Workbench](https://cloud.google.com/vertex-ai-workbench).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LCaCx6PLSBW6"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Additional authentication is required for Google Colab\n",
    "if \"google.colab\" in sys.modules:\n",
    "    # Authenticate user to Google Cloud\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QGB8Txa_e4V0"
   },
   "source": [
    "### Define Google Cloud project information and initialize Vertex AI\n",
    "\n",
    "Initialize the Vertex AI SDK for Python for your project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JGOJHtgDe5-r"
   },
   "outputs": [],
   "source": [
    "# Define project information. \n",
    "# These variables store your Google Cloud project ID and the desired location for resources.\n",
    "PROJECT_ID = \"[your project here]\"  # @param {type:\"string\"} \n",
    "# Replace \"[your project here]\" with your actual project ID.\n",
    "# The @param tag indicates that this is a parameter that could be set in an interactive environment (like Colab).\n",
    "\n",
    "LOCATION = \"[your location here]\"  # @param {type:\"string\"}\n",
    "# Replace \"[your location here]\" with the region where you want to use Vertex AI (e.g., \"us-central1\").\n",
    "\n",
    "# Import the Vertex AI library\n",
    "import vertexai\n",
    "\n",
    "# Initialize the Vertex AI SDK with your project and location information.\n",
    "# This sets up the connection to your Google Cloud project for using Vertex AI services.\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BuQwwRiniVFG"
   },
   "source": [
    "### Import libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "JTk488WDPBtQ"
   },
   "outputs": [],
   "source": [
    "# Import necessary classes from the vertexai.generative_models module.\n",
    "from vertexai.generative_models import (\n",
    "    GenerationConfig,  # Used to configure the generation process (e.g., temperature, top_k).\n",
    "    GenerativeModel,   # Represents a generative model (e.g., Gemini Pro, Gemini Pro Vision).\n",
    "    Image,             # Represents an image, used for multimodal models.\n",
    "    Part              # Represents a part of a multimodal input (can be text or image).\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N7rZuTClfNs0"
   },
   "source": [
    "## Use the Gemini 1.5 Flash model\n",
    "\n",
    "Gemini 1.5 Flash (`gemini-1.5-flash`) is a multimodal model that supports multimodal prompts. You can include text, image(s), and video in your prompt requests and get text or code responses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eTNnM-lqfQRo"
   },
   "source": [
    "### Load Gemini 1.5 Flash model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "2998506fe6d1"
   },
   "outputs": [],
   "source": [
    "# Create a GenerativeModel instance for the \"gemini-1.5-flash\" model.\n",
    "multimodal_model = GenerativeModel(\"gemini-1.5-flash\")\n",
    "\n",
    "# \"gemini-1.5-flash\" is the model identifier for a specific version of Google's Gemini model, \n",
    "# optimized for multimodal tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MpL3OkSCfIAR"
   },
   "source": [
    "### Define helper functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "S7QMAHXse339"
   },
   "outputs": [],
   "source": [
    "import http.client  # Provides classes for working with HTTP connections (used for downloading image data).\n",
    "import typing  # Used for type hinting, which helps with code readability and error checking.\n",
    "import urllib.request  # Used for fetching data from URLs.\n",
    "\n",
    "import IPython.display  # Used for displaying rich content (images, videos, etc.) in Jupyter environments.\n",
    "from PIL import Image as PIL_Image  # PIL (Pillow) is a library for image manipulation. We import Image as PIL_Image to avoid naming conflicts.\n",
    "from PIL import ImageOps as PIL_ImageOps  # ImageOps provides helpful image operations like resizing while maintaining aspect ratio.\n",
    "\n",
    "\n",
    "def display_images(\n",
    "    images: typing.Iterable[Image],\n",
    "    max_width: int = 600,\n",
    "    max_height: int = 350,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Displays a sequence of images. Resizes images if they are larger than the specified maximum width or height.\n",
    "\n",
    "    Args:\n",
    "        images: An iterable (e.g., list, tuple) containing Image objects (from the Vertex AI SDK).\n",
    "        max_width: The maximum width (in pixels) for displayed images.\n",
    "        max_height: The maximum height (in pixels) for displayed images.\n",
    "    \"\"\"\n",
    "    for image in images:\n",
    "        # Access the underlying PIL Image object from the Vertex AI Image object.\n",
    "        # The _pil_image attribute holds the actual image data.\n",
    "        pil_image = typing.cast(PIL_Image.Image, image._pil_image)\n",
    "\n",
    "        # Check if the image is in RGB format (most common for display).\n",
    "        if pil_image.mode != \"RGB\":\n",
    "            # Convert the image to RGB mode.\n",
    "            # RGB is widely supported in Jupyter and web environments.\n",
    "            pil_image = pil_image.convert(\"RGB\")\n",
    "\n",
    "        # Get the width and height of the image.\n",
    "        image_width, image_height = pil_image.size\n",
    "\n",
    "        # Check if the image needs to be resized.\n",
    "        if max_width < image_width or max_height < image_height:\n",
    "            # Resize the image while preserving its aspect ratio using PIL_ImageOps.contain().\n",
    "            # This ensures the image fits within the specified dimensions without distortion.\n",
    "            pil_image = PIL_ImageOps.contain(pil_image, (max_width, max_height))\n",
    "\n",
    "        # Display the image in the Jupyter Notebook using IPython.display.display().\n",
    "        IPython.display.display(pil_image)\n",
    "\n",
    "\n",
    "def get_image_bytes_from_url(image_url: str) -> bytes:\n",
    "    \"\"\"\n",
    "    Downloads image data from a URL and returns it as bytes.\n",
    "\n",
    "    Args:\n",
    "        image_url: The URL of the image.\n",
    "\n",
    "    Returns:\n",
    "        The image data as a bytes object.\n",
    "    \"\"\"\n",
    "    # Open the URL using urllib.request.urlopen().\n",
    "    with urllib.request.urlopen(image_url) as response:\n",
    "        # Cast the response to an http.client.HTTPResponse object for clarity.\n",
    "        response = typing.cast(http.client.HTTPResponse, response)\n",
    "        # Read the image data (bytes) from the response.\n",
    "        image_bytes = response.read()\n",
    "    # Return the image data.\n",
    "    return image_bytes\n",
    "\n",
    "\n",
    "def load_image_from_url(image_url: str) -> Image:\n",
    "    \"\"\"\n",
    "    Loads an image from a URL and returns it as a Vertex AI Image object.\n",
    "\n",
    "    Args:\n",
    "        image_url: The URL of the image to load.\n",
    "\n",
    "    Returns:\n",
    "        A Vertex AI Image object representing the loaded image.\n",
    "    \"\"\"\n",
    "    # Get the image data (bytes) from the URL using the function we defined earlier.\n",
    "    image_bytes = get_image_bytes_from_url(image_url)\n",
    "    # Create a Vertex AI Image object from the image bytes.\n",
    "    return Image.from_bytes(image_bytes)\n",
    "\n",
    "\n",
    "def display_content_as_image(content: str | Image | Part) -> bool:\n",
    "    \"\"\"\n",
    "    Checks if the given content is a Vertex AI Image and displays it if it is.\n",
    "\n",
    "    Args:\n",
    "        content: The content to check and potentially display.\n",
    "                It can be either a string, a Vertex AI Image, or a Vertex AI Part.\n",
    "\n",
    "    Returns:\n",
    "        True if the content was a Vertex AI Image and was displayed, False otherwise.\n",
    "    \"\"\"\n",
    "    # Check if the content is an instance of the Vertex AI Image class.\n",
    "    if not isinstance(content, Image):\n",
    "        # If it's not an Image, return False (we didn't display anything).\n",
    "        return False\n",
    "    # If it is an Image, display it using the display_images function.\n",
    "    display_images([content])\n",
    "    # Return True (we displayed the image).\n",
    "    return True\n",
    "\n",
    "\n",
    "def display_content_as_video(content: str | Image | Part) -> bool:\n",
    "    \"\"\"\n",
    "    Checks if the given content is a Vertex AI Part representing a video and displays it if it is.\n",
    "\n",
    "    Args:\n",
    "        content: The content to check and potentially display.\n",
    "                It can be either a string, a Vertex AI Image, or a Vertex AI Part.\n",
    "\n",
    "    Returns:\n",
    "        True if the content was a Vertex AI Part representing a video and was displayed, False otherwise.\n",
    "    \"\"\"\n",
    "    # Check if the content is an instance of the Vertex AI Part class.\n",
    "    if not isinstance(content, Part):\n",
    "        # If it's not a Part, return False.\n",
    "        return False\n",
    "    # Cast the content to a Part object to access its attributes.\n",
    "    part = typing.cast(Part, content)\n",
    "    # Extract the Google Cloud Storage file path from the Part.\n",
    "    # The file_uri is expected to be in the format \"gs://bucket_name/path/to/video.mp4\"\n",
    "    file_path = part.file_data.file_uri.removeprefix(\"gs://\")\n",
    "    # Construct the public URL for the video stored in Google Cloud Storage.\n",
    "    video_url = f\"https://storage.googleapis.com/{file_path}\"\n",
    "    # Display the video using IPython.display.Video.\n",
    "    IPython.display.display(IPython.display.Video(video_url, width=600))\n",
    "    # Return True because we displayed the video.\n",
    "    return True\n",
    "\n",
    "\n",
    "def print_multimodal_prompt(contents: list[str | Image | Part]):\n",
    "    \"\"\"\n",
    "    Displays a multimodal prompt in a user-friendly way, handling text, images, and videos.\n",
    "\n",
    "    Args:\n",
    "        contents: A list of content items to be sent to the Gemini model.\n",
    "                Each item can be a string (text), a Vertex AI Image, or a Vertex AI Part.\n",
    "    \"\"\"\n",
    "    for content in contents:\n",
    "        # Try to display the content as an image.\n",
    "        if display_content_as_image(content):\n",
    "            # If it was displayed as an image, move to the next item.\n",
    "            continue\n",
    "        # Try to display the content as a video.\n",
    "        if display_content_as_video(content):\n",
    "            # If it was displayed as a video, move to the next item.\n",
    "            continue\n",
    "        # If it's not an image or video, assume it's text and print it.\n",
    "        print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5OWurhO4mu4J"
   },
   "source": [
    "## Image understanding across multiple images\n",
    "\n",
    "One of the capabilities of Gemini is being able to reason across multiple images.\n",
    "\n",
    "This is an example of using Gemini to calculate the total cost of groceries using an image of fruits and a price list:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UyRoVquPmy9H"
   },
   "outputs": [],
   "source": [
    "# Define the URLs of the images we'll be using.\n",
    "image_grocery_url = \"https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/banana-apple.jpg\"\n",
    "image_prices_url = \"https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/pricelist.jpg\"\n",
    "\n",
    "# Load the images from the URLs using the load_image_from_url function we defined earlier.\n",
    "image_grocery = load_image_from_url(image_grocery_url)\n",
    "image_prices = load_image_from_url(image_prices_url)\n",
    "\n",
    "# Define the text prompts that will be part of our multimodal input.\n",
    "instructions = \"Instructions: Consider the following image that contains fruits:\"\n",
    "prompt1 = \"How much should I pay for the fruits given the following price list?\"\n",
    "prompt2 = \"\"\"\n",
    "Answer the question through these steps:\n",
    "Step 1: Identify what kind of fruits there are in the first image.\n",
    "Step 2: Count the quantity of each fruit.\n",
    "Step 3: For each grocery in first image, check the price of the grocery in the price list.\n",
    "Step 4: Calculate the subtotal price for each type of fruit.\n",
    "Step 5: Calculate the total price of fruits using the subtotals.\n",
    "\n",
    "Answer and describe the steps taken:\n",
    "\"\"\"\n",
    "\n",
    "# Assemble the multimodal prompt content.\n",
    "# This list will be sent to the Gemini model. It includes text and image data.\n",
    "contents = [\n",
    "    instructions,\n",
    "    image_grocery,  # The image of the fruits\n",
    "    prompt1,\n",
    "    image_prices,  # The image of the price list\n",
    "    prompt2,  # Detailed instructions for the model\n",
    "]\n",
    "\n",
    "# Generate content using the Gemini model.\n",
    "# We use stream=True to get the response as it's being generated (streaming).\n",
    "responses = multimodal_model.generate_content(contents, stream=True)\n",
    "\n",
    "# Print the prompt that was sent to the model.\n",
    "print(\"-------Prompt--------\")\n",
    "print_multimodal_prompt(contents)\n",
    "\n",
    "# Print the model's generated response.\n",
    "print(\"\\n-------Response--------\")\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zy-me3PdgMUH"
   },
   "source": [
    "## Understanding Screens and Interfaces\n",
    "\n",
    "Gemini can also extract information from appliance screens, UIs, screenshots, icons, and layouts.\n",
    "\n",
    "For example, if you input an image of a stove, you can ask Gemini to provide instructions to help a user navigate the UI and respond in different languages:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yDjN4thV8orx"
   },
   "outputs": [],
   "source": [
    "# Define the URL of the image of the stove.\n",
    "image_stove_url = \"https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/stove.jpg\"\n",
    "\n",
    "# Load the image of the stove from the URL.\n",
    "image_stove = load_image_from_url(image_stove_url)\n",
    "\n",
    "# Define the prompt for the model.\n",
    "# We're asking for instructions to reset the clock in English and French,\n",
    "# and also to describe the location of buttons.\n",
    "prompt = \"\"\"Help me to reset the clock on this appliance?\n",
    "Provide the instructions in English and French.\n",
    "If instructions include buttons, also explain where those buttons are physically located.\n",
    "\"\"\"\n",
    "\n",
    "# Create the content list for the multimodal model.\n",
    "# It includes the image of the stove and the text prompt.\n",
    "contents = [image_stove, prompt]\n",
    "\n",
    "# Call the Gemini model to generate content based on the prompt.\n",
    "# stream=True enables streaming output.\n",
    "responses = multimodal_model.generate_content(contents, stream=True)\n",
    "\n",
    "# Display the prompt that was sent to the model.\n",
    "print(\"-------Prompt--------\")\n",
    "print_multimodal_prompt(contents)\n",
    "\n",
    "# Print the model's response as it's generated.\n",
    "print(\"\\n-------Response--------\")\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bbhfMO478orx"
   },
   "source": [
    "Note: The response may not be completely accurate, as the model may hallucinate; however, the model is able to identify the location of buttons and translate in a single query. To mitigate hallucinations, one approach is to ground the LLM with retrieval-augmented generation, which is outside the scope of this notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4437b7608c8e"
   },
   "source": [
    "## Understanding entity relationships in technical diagrams\n",
    "\n",
    "Gemini has multimodal capabilities that enable it to understand diagrams and take actionable steps, such as optimization or code generation. This example demonstrates how Gemini can decipher an entity relationship (ER) diagram, understand the relationships between tables, identify requirements for optimization in a specific environment like BigQuery, and even generate corresponding code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "klY4yBEiKmET"
   },
   "outputs": [],
   "source": [
    "# Define the URL of the image containing the ER diagram.\n",
    "image_er_url = \"https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/er.png\"\n",
    "\n",
    "# Load the ER diagram image from the URL.\n",
    "image_er = load_image_from_url(image_er_url)\n",
    "\n",
    "# Define the prompt for the model, asking it to document the ER diagram.\n",
    "prompt = \"Document the entities and relationships in this ER diagram.\"\n",
    "\n",
    "# Create the content list for the multimodal model.\n",
    "# It includes the prompt and the ER diagram image.\n",
    "contents = [prompt, image_er]\n",
    "\n",
    "# Define a GenerationConfig for more deterministic output.\n",
    "# We are setting a low temperature to reduce randomness,\n",
    "# and also setting top_p and top_k for sampling from the most likely tokens.\n",
    "generation_config = GenerationConfig(\n",
    "    temperature=0.1,  # Controls the randomness of the output. Lower values make the output more deterministic.\n",
    "    top_p=0.8,  # Limits the tokens considered to those with a cumulative probability up to this value (e.g., 0.8 means the top 80%).\n",
    "    top_k=40,  # Considers only the top 'k' tokens with the highest probabilities.\n",
    "    candidate_count=1,  # Specifies the number of response candidates to generate (we only want one here).\n",
    "    max_output_tokens=2048,  # Sets the maximum length of the generated response.\n",
    ")\n",
    "\n",
    "# Call the Gemini model to generate content based on the prompt and configuration.\n",
    "responses = multimodal_model.generate_content(\n",
    "    contents,\n",
    "    generation_config=generation_config,\n",
    "    stream=True,  # Use streaming output.\n",
    ")\n",
    "\n",
    "# Print the prompt that was sent to the model.\n",
    "print(\"-------Prompt--------\")\n",
    "print_multimodal_prompt(contents)\n",
    "\n",
    "# Print the model's response as it's being generated.\n",
    "print(\"\\n-------Response--------\")\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uXrvQxpiQKp6"
   },
   "source": [
    "## Recommendations based on multiple images\n",
    "\n",
    "Gemini is capable of image comparison and providing recommendations. This may be useful in industries like e-commerce and retail.\n",
    "\n",
    "Below is an example of choosing which pair of glasses would be better suited to an oval-shaped face:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s7M7B9q7L1X7"
   },
   "outputs": [],
   "source": [
    "# Define the URLs for the two images of glasses.\n",
    "image_glasses1_url = \"https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/glasses1.jpg\"\n",
    "image_glasses2_url = \"https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/glasses2.jpg\"\n",
    "\n",
    "# Load the images of the glasses from their URLs.\n",
    "image_glasses1 = load_image_from_url(image_glasses1_url)\n",
    "image_glasses2 = load_image_from_url(image_glasses2_url)\n",
    "\n",
    "# Define the text prompts for the model.\n",
    "# We're asking for a recommendation based on face shape, providing images of two glasses,\n",
    "# and requesting the reasoning in JSON format.\n",
    "prompt1 = \"\"\"\n",
    "Which of these glasses you recommend for me based on the shape of my face?\n",
    "I have an oval shape face.\n",
    "----\n",
    "Glasses 1:\n",
    "\"\"\"\n",
    "prompt2 = \"\"\"\n",
    "----\n",
    "Glasses 2:\n",
    "\"\"\"\n",
    "prompt3 = \"\"\"\n",
    "----\n",
    "Explain how you reach out to this decision.\n",
    "Provide your recommendation based on my face shape, and reasoning for each in JSON format.\n",
    "\"\"\"\n",
    "\n",
    "# Create the content list for the multimodal model.\n",
    "# The order matters here: prompt1, image1, prompt2, image2, prompt3.\n",
    "contents = [prompt1, image_glasses1, prompt2, image_glasses2, prompt3]\n",
    "\n",
    "# Generate content using the Gemini model with streaming output.\n",
    "responses = multimodal_model.generate_content(contents, stream=True)\n",
    "\n",
    "# Print the prompt that was sent to the model.\n",
    "print(\"-------Prompt--------\")\n",
    "print_multimodal_prompt(contents)\n",
    "\n",
    "# Print the model's response as it's generated.\n",
    "print(\"\\n-------Response--------\")\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZBrdsvIU7Zkf"
   },
   "source": [
    "## Similarity/Differences\n",
    "\n",
    "Gemini can compare images and identify similarities or differences between objects.\n",
    "\n",
    "The following example shows two scenes from Marienplatz in Munich, Germany that are slightly different. Gemini can compare between the images and find similarities/differences:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JUSJduLh8457"
   },
   "outputs": [],
   "source": [
    "# Define the URLs for the two landmark images.\n",
    "image_landmark1_url = \"https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/landmark1.jpg\"\n",
    "image_landmark2_url = \"https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/landmark2.jpg\"\n",
    "\n",
    "# Load the landmark images from their URLs.\n",
    "image_landmark1 = load_image_from_url(image_landmark1_url)\n",
    "image_landmark2 = load_image_from_url(image_landmark2_url)\n",
    "\n",
    "# Define the text prompts for the model.\n",
    "# We're asking the model to identify the landmark in Image 1,\n",
    "# and then compare the two images, noting similarities and differences.\n",
    "prompt1 = \"\"\"\n",
    "Consider the following two images:\n",
    "Image 1:\n",
    "\"\"\"\n",
    "prompt2 = \"\"\"\n",
    "Image 2:\n",
    "\"\"\"\n",
    "prompt3 = \"\"\"\n",
    "1. What is shown in Image 1? Where is it?\n",
    "2. What is similar between the two images?\n",
    "3. What is difference between Image 1 and Image 2 in terms of the contents or people shown?\n",
    "\"\"\"\n",
    "\n",
    "# Create the content list for the multimodal model,\n",
    "# including the prompts and images in the desired order.\n",
    "contents = [prompt1, image_landmark1, prompt2, image_landmark2, prompt3]\n",
    "\n",
    "# Define a GenerationConfig for a more deterministic and focused response.\n",
    "# We use a temperature of 0.0 for maximum determinism.\n",
    "generation_config = GenerationConfig(\n",
    "    temperature=0.0,  # Very low temperature for a very deterministic output (less creative, more factual).\n",
    "    top_p=0.8,  # Limits the considered tokens to those with a cumulative probability up to this value.\n",
    "    top_k=40,  # Considers only the top 'k' tokens with the highest probabilities.\n",
    "    candidate_count=1,  # Specifies the number of response candidates to generate (we want only one).\n",
    "    max_output_tokens=2048,  # Sets the maximum length of the generated response.\n",
    ")\n",
    "\n",
    "# Call the Gemini model to generate content based on the prompt and configuration.\n",
    "responses = multimodal_model.generate_content(\n",
    "    contents,\n",
    "    generation_config=generation_config,\n",
    "    stream=True,  # Use streaming output.\n",
    ")\n",
    "\n",
    "# Print the prompt that was sent to the model.\n",
    "print(\"-------Prompt--------\")\n",
    "print_multimodal_prompt(contents)\n",
    "\n",
    "# Print the model's response as it's being generated.\n",
    "print(\"\\n-------Response--------\")\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dN8nVlITK5kz"
   },
   "source": [
    "## Generating a video description\n",
    "\n",
    "Gemini can also extract tags throughout a video:\n",
    "\n",
    "> Video: https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/mediterraneansea.mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tT2nArvxZv-P"
   },
   "outputs": [],
   "source": [
    "# Define the prompt for the model.\n",
    "# We're asking what's in the video, where to see it,\n",
    "# and for similar-looking places.\n",
    "prompt = \"\"\"\n",
    "What is shown in this video?\n",
    "Where should I go to see it?\n",
    "What are the top 5 places in the world that look like this?\n",
    "\"\"\"\n",
    "\n",
    "# Create a Part object representing the video.\n",
    "# This is used to include video data in the prompt.\n",
    "video = Part.from_uri(\n",
    "    uri=\"gs://github-repo/img/gemini/multimodality_usecases_overview/mediterraneansea.mp4\",  # The Google Cloud Storage URI of the video.\n",
    "    mime_type=\"video/mp4\",  # The MIME type of the video.\n",
    ")\n",
    "\n",
    "# Create the content list for the multimodal model.\n",
    "# It includes the text prompt and the video Part.\n",
    "contents = [prompt, video]\n",
    "\n",
    "# Generate content using the Gemini model with streaming output.\n",
    "responses = multimodal_model.generate_content(contents, stream=True)\n",
    "\n",
    "# Print the prompt that was sent to the model.\n",
    "print(\"-------Prompt--------\")\n",
    "print_multimodal_prompt(contents)\n",
    "\n",
    "# Print the model's response as it's being generated.\n",
    "print(\"\\n-------Response--------\")\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u9RdLpH128Ao"
   },
   "source": [
    "> You can confirm that the location is indeed Antalya, Turkey by visiting the Wikipedia page: https://en.wikipedia.org/wiki/Antalya\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ksjZiIfnO0zQ"
   },
   "source": [
    "## Extracting tags of objects throughout the video\n",
    "\n",
    "Gemini can also extract tags throughout a video.\n",
    "\n",
    "> Video: https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/photography.mp4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b9qE2GGIA975"
   },
   "outputs": [],
   "source": [
    "# Define the prompt for the model, asking for video analysis and tagging.\n",
    "prompt = \"\"\"\n",
    "Answer the following questions using the video only:\n",
    "- What is in the video?\n",
    "- What is the action in the video?\n",
    "- Provide 10 best tags for this video?\n",
    "\"\"\"\n",
    "\n",
    "# Create a Part object to represent the video data from a Google Cloud Storage URI.\n",
    "video = Part.from_uri(\n",
    "    uri=\"gs://github-repo/img/gemini/multimodality_usecases_overview/photography.mp4\",  # The GCS URI of the video.\n",
    "    mime_type=\"video/mp4\",  # The MIME type of the video.\n",
    ")\n",
    "\n",
    "# Create the content list for the multimodal model, containing the prompt and video Part.\n",
    "contents = [prompt, video]\n",
    "\n",
    "# Call the Gemini model to generate content based on the prompt and video.\n",
    "responses = multimodal_model.generate_content(contents, stream=True)\n",
    "\n",
    "# Print the prompt that was sent to the model (including a visual representation of the video).\n",
    "print(\"-------Prompt--------\")\n",
    "print_multimodal_prompt(contents)\n",
    "\n",
    "# Print the model's response as it is generated.\n",
    "print(\"\\n-------Response--------\")\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wiQzvIJfQxbQ"
   },
   "source": [
    "## Asking more questions about a video\n",
    "\n",
    "Below is another example of using Gemini to ask questions the video and return a JSON response.\n",
    "\n",
    "> Video: https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/pixel8.mp4  \n",
    "> _Note: Although this video contains audio, Gemini does not currently support audio input and will only answer based on the video._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oQIV9SwCz5WM"
   },
   "outputs": [],
   "source": [
    "# Define the prompt, asking specific questions about the video content and requesting JSON output.\n",
    "prompt = \"\"\"\n",
    "Answer the following questions using the video only:\n",
    "What is the profession of the main person?\n",
    "What are the main features of the phone highlighted?\n",
    "Which city was this recorded in?\n",
    "Provide the answer JSON.\n",
    "\"\"\"\n",
    "\n",
    "# Create a Part object representing the video from its Google Cloud Storage URI.\n",
    "video = Part.from_uri(\n",
    "    uri=\"gs://github-repo/img/gemini/multimodality_usecases_overview/pixel8.mp4\",  # The GCS URI of the video.\n",
    "    mime_type=\"video/mp4\",  # The MIME type of the video.\n",
    ")\n",
    "\n",
    "# Create the content list for the multimodal model, including the prompt and the video Part.\n",
    "contents = [prompt, video]\n",
    "\n",
    "# Call the Gemini model to generate content based on the prompt and video, using streaming output.\n",
    "responses = multimodal_model.generate_content(contents, stream=True)\n",
    "\n",
    "# Print the prompt that was sent to the model (including a visual representation of the video).\n",
    "print(\"-------Prompt--------\")\n",
    "print_multimodal_prompt(contents)\n",
    "\n",
    "# Print the model's response as it is generated.\n",
    "print(\"\\n-------Response--------\")\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9LurOmNuRpDr"
   },
   "source": [
    "## Retrieving extra information beyond the video\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "17A1dteeJut-"
   },
   "source": [
    "> Video: https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/ottawatrain3.mp4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-CY-zlixU87O"
   },
   "outputs": [],
   "source": [
    "# Define the prompt, asking questions about the train line shown in the video.\n",
    "prompt = \"\"\"\n",
    "Which line is this?\n",
    "where does it go?\n",
    "What are the stations/stops of this line?\n",
    "\"\"\"\n",
    "\n",
    "# Create a Part object representing the video from its Google Cloud Storage URI.\n",
    "video = Part.from_uri(\n",
    "    uri=\"gs://github-repo/img/gemini/multimodality_usecases_overview/ottawatrain3.mp4\",  # The GCS URI of the video.\n",
    "    mime_type=\"video/mp4\",  # The MIME type of the video.\n",
    ")\n",
    "\n",
    "# Create the content list for the multimodal model, including the prompt and the video Part.\n",
    "contents = [prompt, video]\n",
    "\n",
    "# Call the Gemini model to generate content based on the prompt and video, using streaming output.\n",
    "responses = multimodal_model.generate_content(contents, stream=True)\n",
    "\n",
    "# Print the prompt that was sent to the model (including a visual representation of the video).\n",
    "print(\"-------Prompt--------\")\n",
    "print_multimodal_prompt(contents)\n",
    "\n",
    "# Print the model's response as it is generated.\n",
    "print(\"\\n-------Response--------\")\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WZrxMm_83Vps"
   },
   "source": [
    "> You can confirm that this is indeed the Confederation Line on Wikipedia here: https://en.wikipedia.org/wiki/Confederation_Line\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "intro_multimodal_use_cases.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m126",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m126"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
