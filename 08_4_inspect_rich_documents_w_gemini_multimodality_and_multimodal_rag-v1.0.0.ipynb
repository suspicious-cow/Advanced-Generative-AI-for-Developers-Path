{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1803c39-71f2-4564-bd69-3df66a1cb947",
   "metadata": {},
   "source": [
    "# Inspect Rich Documents with Gemini Multimodality and Multimodal RAG Challenge Lab\n",
    "\n",
    "__Note__: if you encounter an authentication error when running the cells in the notebook, go to __Vertex AI__ > __Dashboard__, and click on __Enable All Recommended APIs__. Then, re-run the failed cell, and continue the lab.  \n",
    "\n",
    "## Setup and requirements\n",
    "\n",
    "### Install Vertex AI SDK for Python and other dependencies\n",
    "\n",
    "Run the following four cells below before you get to Task 1. Be sure to add your current project ID to the cell titled __Define Google Cloud project information__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219e959e-a819-4af5-8ad2-db5cf77f969e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"RUN THIS CELL AS IS\"\n",
    "\n",
    "# Install required python packages and other dependencies\n",
    "!pip3 install --upgrade --user google-cloud-aiplatform\n",
    "\n",
    "!pip3 install --upgrade --user google-cloud-aiplatform pymupdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245e8b96-1d74-4fee-903d-66e10ec82d46",
   "metadata": {},
   "source": [
    "### Restart current runtime\n",
    "\n",
    "You must restart the runtime in order to use the newly installed packages in this Jupyter runtime. You can do this by running the cell below, which will restart the current kernel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864cef62-bfea-4843-b2a3-0b19bc00a0df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"RUN THIS CELL AS IS\"\n",
    "\n",
    "import IPython\n",
    "\n",
    "# Restart the kernet after libraries are loaded.\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ea66ce-c9a5-478b-b95d-7900488d1960",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ The kernel is going to restart. Please wait until it is finished before continuing to the next step. ⚠️</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e078e2-2891-42ed-95f7-ce31795045d0",
   "metadata": {},
   "source": [
    "### Define Google Cloud project information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81ca78d-1320-49dd-8c43-3d5608f47c61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We import the sys module to check our environment and possibly load additional modules.\n",
    "import sys\n",
    "\n",
    "# Explanation for Beginners:\n",
    "# 1) PROJECT_ID: The ID of your Google Cloud project. If you are using Vertex AI, \n",
    "#    all your resources (models, endpoints, datasets) will be billed to this project.\n",
    "# 2) LOCATION: The region where you plan to run Vertex AI. \n",
    "#    Common choices are \"us-central1\" or \"us-east1\" (among others).\n",
    "# 3) We try to detect if we’re NOT running in Google Colab. If so, we attempt \n",
    "#    to retrieve PROJECT_ID and LOCATION from the local gcloud configuration.\n",
    "\n",
    "# Define project information and update the location if it differs from the one \n",
    "# specified in the lab instructions.\n",
    "# Replace \"[YOUR_PROJECT_ID]\" with your actual Google Cloud project ID if you wish.\n",
    "PROJECT_ID = \"[your project]\"  \n",
    "LOCATION = \"[your location]\"          \n",
    "\n",
    "# Try to get the PROJECT_ID and LOCATION automatically (if not running in Colab).\n",
    "if \"google.colab\" not in sys.modules:\n",
    "    import subprocess\n",
    "\n",
    "    PROJECT_ID = subprocess.check_output(\n",
    "        [\"gcloud\", \"config\", \"get-value\", \"project\"], text=True\n",
    "    ).strip()\n",
    "\n",
    "    LOCATION = subprocess.check_output(\n",
    "        [\"gcloud\", \"config\", \"get-value\", \"compute/region\"], text=True\n",
    "    ).strip()\n",
    "\n",
    "print(f\"Your project ID is: {PROJECT_ID}\")\n",
    "print(f\"Your location is: {LOCATION}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48216434",
   "metadata": {},
   "source": [
    "### Initialize Vertex AI\n",
    "\n",
    "Initialize the Vertex AI SDK for Python for your project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f5ded7d-13b9-42ff-87dc-0b5c998f884f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"RUN THIS CELL AS IS\"\n",
    "\n",
    "# We initialize the Vertex AI library for the specified Google Cloud project and region.\n",
    "# \n",
    "# Explanation for Beginners:\n",
    "# 1) `vertexai`: This library allows interaction with Vertex AI services, such as \n",
    "#    text, image, and multimodal models.\n",
    "# 2) `vertexai.init(...)`: We specify which GCP project and region we’re connecting to.\n",
    "#    This means any calls to Vertex AI (e.g., generating text, training a model) will be \n",
    "#    billed to the indicated project and will operate in the specified region.\n",
    "# 3) `PROJECT_ID` and `LOCATION`: These should be set in the previous cell. \n",
    "#    If you haven’t replaced them with your own values, the code may rely on \n",
    "#    auto-detection via gcloud configuration (depending on your environment).\n",
    "\n",
    "import vertexai\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb712712-c7fb-43ef-be19-2497d610111c",
   "metadata": {},
   "source": [
    "## Task 1. Generating Multimodal Insights with the Gemini Pro Vision model\n",
    "\n",
    "Gemini 1.0 Pro Vision (gemini-1.0-pro-vision) is a multimodal model that supports multimodal prompts. You can include text, image(s), and video in your prompt requests and get text or code responses.\n",
    "\n",
    "To complete Task 1, follow the instructions at the top of each notebook cell:\n",
    "* Run the cells with the comment \"RUN THIS CELL AS IS\".\n",
    "* Complete and run the cells with the comment \"COMPLETE THE MISSING PART AND RUN THIS CELL\".\n",
    "\n",
    "__Note__: Ensure you can see the weather related data in the response that is printed.\n",
    "\n",
    "\n",
    "### Setup and requirements for Task 1\n",
    "\n",
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85d32222-695d-4993-bdc4-fde7bc7948f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"RUN THIS CELL AS IS\"\n",
    "\n",
    "# We import several classes from the Vertex AI generative models library:\n",
    "# 1) GenerationConfig: Allows us to configure parameters for generative operations,\n",
    "#    such as temperature, max_output_tokens, etc.\n",
    "# 2) GenerativeModel: The main class for calling text, image, or multimodal generative models.\n",
    "# 3) Image: A specialized class for handling image-based inputs/outputs in generative tasks.\n",
    "# 4) Part: Useful for structuring multi-part content (e.g., video files, audio files).\n",
    "\n",
    "from vertexai.generative_models import (\n",
    "    GenerationConfig,\n",
    "    GenerativeModel,\n",
    "    Image,\n",
    "    Part,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435fad59-d08b-4aef-8230-7f567643ce14",
   "metadata": {},
   "source": [
    "#### Load Gemini 1.0 Pro Vision model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd63f204-6acc-41aa-ac25-45544d5c3888",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"RUN THIS CELL AS IS\"\n",
    "\n",
    "# We create an instance of a specific generative model called \"gemini-1.0-pro-vision.\"\n",
    "# This model can handle multimodal inputs (e.g., text and images).\n",
    "#\n",
    "# Explanation for Beginners:\n",
    "# 1) The string \"gemini-1.0-pro-vision\" specifies which Gemini model version \n",
    "#    we're loading from Vertex AI. It's designed for vision tasks in addition to text.\n",
    "# 2) By assigning this model to `multimodal_model`, we can later call methods \n",
    "#    like `generate_content` to process and generate text or image outputs \n",
    "#    in a multimodal context (text + images).\n",
    "\n",
    "multimodal_model = GenerativeModel(\"gemini-1.0-pro-vision\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c3a051-d567-4146-b428-fcb285c6485d",
   "metadata": {},
   "source": [
    "#### Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8de36ca-ded6-46e4-99e9-4ed2fc88ee8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"RUN THIS CELL AS IS\"\n",
    "# \n",
    "# Explanation for Beginners:\n",
    "# 1) display_images: Displays Vertex AI Image objects in a Jupyter environment, converting \n",
    "#    them to RGB mode and optionally resizing them if they exceed certain dimensions.\n",
    "# 2) get_image_bytes_from_url: Fetches the raw bytes of an image directly from a web URL.\n",
    "# 3) load_image_from_url: Uses the bytes fetched from a URL to create a Vertex AI Image object.\n",
    "# 4) display_content_as_image: Checks if content is an image and, if so, displays it inline.\n",
    "# 5) display_content_as_video: Checks if content is a \"Part\" representing a video file, \n",
    "#    builds the correct URL, and displays it inline.\n",
    "# 6) print_multimodal_prompt: Iterates through a list of textual or image/video content, \n",
    "#    printing text or displaying media inline to show what is being sent to the model.\n",
    "\n",
    "import http.client\n",
    "import typing\n",
    "import urllib.request\n",
    "\n",
    "import IPython.display\n",
    "from PIL import Image as PIL_Image\n",
    "from PIL import ImageOps as PIL_ImageOps\n",
    "\n",
    "\n",
    "def display_images(\n",
    "    images: typing.Iterable[Image],\n",
    "    max_width: int = 600,\n",
    "    max_height: int = 350,\n",
    ") -> None:\n",
    "    for image in images:\n",
    "        pil_image = typing.cast(PIL_Image.Image, image._pil_image)\n",
    "        if pil_image.mode != \"RGB\":\n",
    "            # RGB is supported by all Jupyter environments (e.g. RGBA is not yet)\n",
    "            pil_image = pil_image.convert(\"RGB\")\n",
    "        image_width, image_height = pil_image.size\n",
    "        if max_width < image_width or max_height < image_height:\n",
    "            # Resize to display a smaller notebook image\n",
    "            pil_image = PIL_ImageOps.contain(pil_image, (max_width, max_height))\n",
    "        IPython.display.display(pil_image)\n",
    "\n",
    "\n",
    "def get_image_bytes_from_url(image_url: str) -> bytes:\n",
    "    with urllib.request.urlopen(image_url) as response:\n",
    "        response = typing.cast(http.client.HTTPResponse, response)\n",
    "        image_bytes = response.read()\n",
    "    return image_bytes\n",
    "\n",
    "\n",
    "def load_image_from_url(image_url: str) -> Image:\n",
    "    image_bytes = get_image_bytes_from_url(image_url)\n",
    "    return Image.from_bytes(image_bytes)\n",
    "\n",
    "\n",
    "def display_content_as_image(content: str | Image | Part) -> bool:\n",
    "    if not isinstance(content, Image):\n",
    "        return False\n",
    "    display_images([content])\n",
    "    return True\n",
    "\n",
    "\n",
    "def display_content_as_video(content: str | Image | Part) -> bool:\n",
    "    if not isinstance(content, Part):\n",
    "        return False\n",
    "    part = typing.cast(Part, content)\n",
    "    file_path = part.file_data.file_uri.removeprefix(\"gs://\")\n",
    "    video_url = f\"https://storage.googleapis.com/{file_path}\"\n",
    "    IPython.display.display(IPython.display.Video(video_url, width=600))\n",
    "    return True\n",
    "\n",
    "\n",
    "def print_multimodal_prompt(contents: list[str | Image | Part]):\n",
    "    \"\"\"\n",
    "    Given contents that would be sent to Gemini,\n",
    "    output the full multimodal prompt for ease of readability.\n",
    "    \"\"\"\n",
    "    for content in contents:\n",
    "        if display_content_as_image(content):\n",
    "            continue\n",
    "        if display_content_as_video(content):\n",
    "            continue\n",
    "        print(content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea68bb17-db90-437a-89b1-d1fe1d2e715f",
   "metadata": {},
   "source": [
    "### Task 1.1. Image understanding across multiple images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bcae938-8299-4641-87d9-d78c3dc88ac5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"RUN THIS CELL AS IS\"\n",
    "\n",
    "# You're going to work with provided variables in this task. \n",
    "# First, review and describe the content/purpose of each variable below. \n",
    "\n",
    "# We define two URLs pointing to images named \"Ask_first_1.png\" and \"Dont_do_this_1.png\".\n",
    "# Explanation for Beginners:\n",
    "# 1) These images may contain some textual or graphical information \n",
    "#    that we want to process or analyze.\n",
    "# 2) We use our utility function `load_image_from_url` to convert each URL into a\n",
    "#    Vertex AI Image object that can be passed to the model.\n",
    "\n",
    "image_ask_first_1_url = \"https://storage.googleapis.com/spls/gsp520/Google_Branding/Ask_first_1.png\"\n",
    "image_dont_do_this_1_url = \"https://storage.googleapis.com/spls/gsp520/Google_Branding/Dont_do_this_1.png\"\n",
    "\n",
    "# We load each image from its respective URL using our helper function.\n",
    "image_ask_first_1 = load_image_from_url(image_ask_first_1_url)\n",
    "image_dont_do_this_1 = load_image_from_url(image_dont_do_this_1_url)\n",
    "\n",
    "# We define a simple set of instructions and prompts that will guide our model.\n",
    "# Explanation for Beginners:\n",
    "# 1) instructions: A simple string to provide context for the images.\n",
    "# 2) prompt1: A short prompt that asks, \"What is the title of this image?\"\n",
    "# 3) prompt2: A more detailed list of steps we want the model to follow, such as\n",
    "#    identifying the title, describing the image, extracting text, and describing sentiment.\n",
    "\n",
    "instructions = \"Instructions: Consider the following image that contains text:\"\n",
    "prompt1 = \"What is the title of this image\"\n",
    "prompt2 = \"\"\"\n",
    "Answer the question through these steps:\n",
    "Step 1: Identify the title of each image by using the filename of each image.\n",
    "Step 2: Describe the image.\n",
    "Step 3: For each image, describe the actions that a user is expected to take.\n",
    "Step 4: Extract the text from each image as a full sentence.\n",
    "Step 5: Describe the sentiment for each image with an explanation.\n",
    "\n",
    "Answer and describe the steps taken:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867dc55c",
   "metadata": {},
   "source": [
    "#### Create an input for the multimodal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e390c4b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"COMPLETE THE MISSING PART AND RUN THIS CELL\"\n",
    "\n",
    "# Now, you're going to create an input for your multimodal model. Create your contents list using the variables above. Ensure the structure matches the format expected by the multimodal model.\n",
    "\n",
    "# We define a list called 'contents' that combines both text and images in the order\n",
    "# we want the model to process them. This list will be passed to our multimodal model,\n",
    "# allowing it to read the text prompts as well as analyze the images.\n",
    "\n",
    "# Explanation for Beginners:\n",
    "# 1) instructions: A short prompt indicating that these images contain text.\n",
    "# 2) image_ask_first_1: The first image (a Vertex AI Image object).\n",
    "# 3) prompt1: A short textual query about the image's title.\n",
    "# 4) image_dont_do_this_1: The second image (another Vertex AI Image object).\n",
    "# 5) prompt2: A more detailed prompt describing the steps the model should take,\n",
    "#    such as identifying the title from the filename, describing the image,\n",
    "#    listing user actions, extracting text, and describing sentiment.\n",
    "\n",
    "contents = [\n",
    "    instructions,\n",
    "    image_ask_first_1,   # The first image\n",
    "    prompt1,\n",
    "    image_dont_do_this_1,  # The second image\n",
    "    prompt2\n",
    "]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4aad87",
   "metadata": {},
   "source": [
    "#### Generate responses from the multimodal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7156c265",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"COMPLETE THE MISSING PART AND RUN THIS CELL\"\n",
    "\n",
    "# In the next part of this task, you're going to generate responses from the multimodal model. Capture the output of the model in the \"responses\" variable by using your \"contents\" list.\n",
    "\n",
    "\n",
    "# We use the `generate_content` method of our multimodal_model (Gemini 1.0 Pro Vision)\n",
    "# to generate a response from the combined text + image content list.\n",
    "# \n",
    "# Explanation for Beginners:\n",
    "# 1) contents: A list of text strings and images we defined earlier, \n",
    "#    containing instructions, a short prompt, two images, and a final prompt. \n",
    "# 2) stream=True: The model will return partial outputs as they’re generated, \n",
    "#    allowing us to display them in real time or collect them incrementally.\n",
    "\n",
    "responses = multimodal_model.generate_content(contents, stream=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28025b0",
   "metadata": {},
   "source": [
    "#### Display the prompt and responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc92127e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"COMPLETE THE MISSING PART AND RUN THIS CELL\"\n",
    "\n",
    "# In the last part of this task, you're going to print your contents and responses with the prompt and responses title provided. Use descriptive titles to help organize the output (e.g., \"Prompts\", \"Model Responses\") and then display the prompt and responses by using the print() function. \n",
    "\n",
    "# Hint: \"\\n\" inserts a newline character for clearer separation between the sections.\n",
    "\n",
    "\n",
    "# We print the prompts (including text and images) that we've passed to the model,\n",
    "# then display the model responses as they stream back from Gemini.\n",
    "#\n",
    "# Explanation for Beginners:\n",
    "# 1) print_multimodal_prompt(contents): This function attempts to display images\n",
    "#    if it encounters an Image object and simply prints out text otherwise.\n",
    "# 2) We iterate over the responses (which arrive as a stream if stream=True)\n",
    "#    and print out each chunk of text. By using 'end=\"\"', we avoid extra newlines\n",
    "#    between each chunk, producing a smoother concatenated output.\n",
    "\n",
    "print(\"Prompts\\n\")\n",
    "print_multimodal_prompt(contents)\n",
    "\n",
    "print(\"\\nModel Responses\\n\")\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0527c725-e400-4838-991b-33cd0772e450",
   "metadata": {
    "tags": []
   },
   "source": [
    "### To verify your work for Task 1.1, click __Check my progress__ in the lab instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5b2269-4982-4c77-85b7-b6413b17f293",
   "metadata": {},
   "source": [
    "### Task 1.2. Similarity/Differences between images\n",
    "\n",
    "#### Explore the variables of the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "323aa40f-18dd-4454-a9f1-474d4c4c721b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"RUN THIS CELL AS IS\"\n",
    "\n",
    "# You're going to work with provided variables in this task. First, review and describe the content/purpose of each variable below. \n",
    "\n",
    "\n",
    "# We define URLs for two new images, \"Ask_first_3.png\" and \"Dont_do_this_3.png.\"\n",
    "# We then load these URLs into Vertex AI Image objects using our helper function \n",
    "# `load_image_from_url`. We'll use these images for additional prompts later.\n",
    "\n",
    "# Explanation for Beginners:\n",
    "# 1) image_ask_first_3_url, image_dont_do_this_3_url:\n",
    "#    These are direct links to images stored in a Google Cloud Storage bucket.\n",
    "# 2) load_image_from_url(...):\n",
    "#    Converts each image URL into a Vertex AI Image object. We'll pass these \n",
    "#    Image objects to our model for multimodal analysis.\n",
    "\n",
    "image_ask_first_3_url = \"https://storage.googleapis.com/spls/gsp520/Google_Branding/Ask_first_3.png\"\n",
    "image_dont_do_this_3_url = \"https://storage.googleapis.com/spls/gsp520/Google_Branding/Dont_do_this_3.png\"\n",
    "\n",
    "image_ask_first_3 = load_image_from_url(image_ask_first_3_url)\n",
    "image_dont_do_this_3 = load_image_from_url(image_dont_do_this_3_url)\n",
    "\n",
    "# Next, we define three text prompts. The first two are simple markers \n",
    "# indicating \"Image 1\" and \"Image 2,\" while the third prompt asks specific \n",
    "# questions comparing these two images.\n",
    "\n",
    "# Explanation for Beginners:\n",
    "# 1) prompt1: Introduces Image 1 context.\n",
    "# 2) prompt2: Introduces Image 2 context.\n",
    "# 3) prompt3: Requests details about what's shown in each image, \n",
    "#    how they compare, and how they differ in terms of text.\n",
    "\n",
    "prompt1 = \"\"\"\n",
    "Consider the following two images:\n",
    "Image 1:\n",
    "\"\"\"\n",
    "prompt2 = \"\"\"\n",
    "Image 2:\n",
    "\"\"\"\n",
    "prompt3 = \"\"\"\n",
    "1. What is shown in Image 1 and Image 2?\n",
    "2. What is similar between the two images?\n",
    "3. What is difference between Image 1 and Image 2 in terms of the text ?\n",
    "\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bb6c29",
   "metadata": {},
   "source": [
    "#### Create an input for the multimodal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "420a9d34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"COMPLETE THE MISSING PART AND RUN THIS CELL\"\n",
    "\n",
    "# Now, you're going to create an input for your multimodal model. Create your contents list using the variables above. Ensure the structure matches the format expected by the multimodal model.\n",
    "\n",
    "# We assemble a 'contents' list to combine both text prompts and images in the order\n",
    "# we want the model to see them. The model will process prompt1 (introducing Image 1),\n",
    "# then see Image 1, then process prompt2 (introducing Image 2), then see Image 2,\n",
    "# and finally read prompt3 (our detailed questions).\n",
    "\n",
    "# Explanation for Beginners:\n",
    "# 1) prompt1: Introduces the concept of Image 1.\n",
    "# 2) image_ask_first_3: The first Vertex AI Image object.\n",
    "# 3) prompt2: Introduces Image 2.\n",
    "# 4) image_dont_do_this_3: The second Vertex AI Image object.\n",
    "# 5) prompt3: A set of questions prompting the model to compare and contrast the images.\n",
    "\n",
    "contents = [\n",
    "    prompt1,            # Text that sets context for Image 1\n",
    "    image_ask_first_3,  # The first image object\n",
    "    prompt2,            # Text that sets context for Image 2\n",
    "    image_dont_do_this_3,  # The second image object\n",
    "    prompt3             # The final prompt with comparison questions\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68eaa88b",
   "metadata": {},
   "source": [
    "#### Set configuration parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3bdc9ef8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"COMPLETE THE MISSING PART AND RUN THIS CELL\"\n",
    "\n",
    "# Now, you're going to set configuration parameters that will influence how the multimodal model generates text. These settings control aspects like the creativity and focus of the responses. Here's how:\n",
    "# Temperature: Controls randomness. Lower values mean more predictable results, higher values mean more surprising and creative outpu\n",
    "# Top p / Top k: Affects how the model chooses words. Explore different values to see how they change the results.\n",
    "# Other parameters: Check the model's documentation for additional options you might want to adjust.\n",
    "\n",
    "# Store your configuration parameters in a generation_config variable. This improves reusability, allowing you to easily apply the same settings across tasks and make adjustments as needed.\n",
    "\n",
    "from vertexai.generative_models import GenerationConfig\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    temperature=0.0,    # Lower temperature => more deterministic responses\n",
    "    top_p=0.8,          # Restrict to cumulative probability p for next word selection\n",
    "    top_k=40,           # Restrict to top k likely tokens\n",
    "    candidate_count=1,  # Number of candidate responses to generate\n",
    "    max_output_tokens=2048  # Maximum tokens in the generated response\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78eecbb",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Generate responses from the multimodal model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74696b35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"COMPLETE THE MISSING PART AND RUN THIS CELL\"\n",
    "\n",
    "# In the next part of this task, you're going to generate responses from a multimodal model. capture the output of the model in the \"responses\" variable by using your \"contents\" list and the confiuguration settings.\n",
    "\n",
    "\n",
    "responses = multimodal_model.generate_content(\n",
    "    contents,\n",
    "    generation_config=generation_config,\n",
    "    stream=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16501a73",
   "metadata": {},
   "source": [
    "#### Display the prompt and responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28236144",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"COMPLETE THE MISSING PART AND RUN THIS CELL\"\n",
    "\n",
    "# In the last part of this task, you're going to print your contents and responses with the prompt and responses title provided. Use descriptive titles to help organize the output (e.g., \"Prompts\", \"Model Responses\") and then display the prompt and responses by using the print() function. \n",
    "\n",
    "# Hint: \"\\n\" inserts a newline character for clearer separation between the sections.\n",
    "\n",
    "print(\"Prompts\\n\")\n",
    "print_multimodal_prompt(contents)\n",
    "\n",
    "print(\"\\nModel Responses\\n\")\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ab2ba5-7789-43ff-aaea-88cfcb75a4f9",
   "metadata": {},
   "source": [
    "### To verify your work for Task 1.2, click __Check my progress__ in the lab instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f815778c-aee0-4c3d-9d98-445ec693b18a",
   "metadata": {},
   "source": [
    "### Task 1.3. Generate a video description\n",
    "\n",
    "#### Explore the variables of the task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90071041-d4c0-4bf2-a783-853dadaa3dca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"RUN THIS CELL AS IS\"\n",
    "\n",
    "# You're going to work with provided variables in this task. \n",
    "# First, review and describe the content/purpose of each variable below. \n",
    "\n",
    "\n",
    "prompt = \"\"\"\n",
    "What is the product shown in this video?\n",
    "What specific product features are highlighted in the video?\n",
    "Where was this video filmed? Which cities around the world could potentially serve as the background in the video?\n",
    "What is the sentiment of the video?\n",
    "\"\"\"\n",
    "video = Part.from_uri(\n",
    "    uri=\"gs://spls/gsp520/google-pixel-8-pro.mp4\",\n",
    "    mime_type=\"video/mp4\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19c1b8b",
   "metadata": {},
   "source": [
    "#### Create an input for the multimodal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b12f7b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"COMPLETE THE MISSING PART AND RUN THIS CELL\"\n",
    "\n",
    "# Now, you're going to create an input for your multimodal model. Create your contents list using the variables above. Ensure the structure matches the format expected by the multimodal model.\n",
    "\n",
    "contents = [\n",
    "    prompt,\n",
    "    video\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f4971b",
   "metadata": {},
   "source": [
    "#### Generate responses from the multimodal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "17e1a6e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"COMPLETE THE MISSING PART AND RUN THIS CELL\"\n",
    "\n",
    "# In the next part of this task, you're going to generate responses from a multimodal model. Capture the output of the model in the \"responses\" variable by using your \"contents\" list.\n",
    "\n",
    "\n",
    "responses = multimodal_model.generate_content(contents, stream=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa2d51f",
   "metadata": {},
   "source": [
    "#### Display the prompt and responses\n",
    "\n",
    "**Note:** If you encounter any authentication error below cell run, go to the **Navigation menu**, click **Vertex AI > Dashboard**, then click **\"Enable all Recommended APIs\"** Now, go back to cell 16, and run cells 16, 17 and below cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef32b772",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"COMPLETE THE MISSING PART AND RUN THIS CELL\"\n",
    "\n",
    "# In the last part of this task, you're going to print your contents and responses with the prompt and responses title provided. Use descriptive titles to help organize the output (e.g., \"Prompts\", \"Model Responses\") and then display the prompt and responses by using the print() function. \n",
    "\n",
    "# Hint: \"\\n\" inserts a newline character for clearer separation between the sections.\n",
    "\n",
    "print(\"Prompts\\n\")\n",
    "print_multimodal_prompt(contents)\n",
    "\n",
    "print(\"\\nModel Responses\\n\")\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4574b5-8ae4-4212-85d7-badc66e9d3df",
   "metadata": {},
   "source": [
    "Proceed to Task 1.4 below (no progress check for Task 1.3 in lab instructions). \n",
    "\n",
    "### Task 1.4. Extract tags of objects throughout the video\n",
    "\n",
    "#### Explore the variables of the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "69b54344-7190-46a4-877f-c9c9f7daa7a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"COMPLETE THE MISSING PART AND RUN THIS CELL\"\n",
    "\n",
    "# You're going to work with provided variables in this task. First, review and describe the content/purpose of each variable below. \n",
    "\n",
    "prompt = \"\"\"\n",
    "Answer the following questions using the video only:\n",
    "\n",
    "Which particular sport is highlighted in the video?\n",
    "What values or beliefs does the advertisement communicate about the brand?\n",
    "What emotions or feelings does the advertisement evoke in the audience?\n",
    "Which tags associated with objects featured throughout the video could be extracted?\n",
    "\"\"\"\n",
    "video = Part.from_uri(\n",
    "    uri=\"gs://spls/gsp520/google-pixel-8-pro.mp4\",\n",
    "    mime_type=\"video/mp4\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2885e5bb",
   "metadata": {},
   "source": [
    "#### Create an input for the multimodal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ab39bda2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"COMPLETE THE MISSING PART AND RUN THIS CELL\"\n",
    "\n",
    "# Now, you're going to create an input for your multimodal model. Create your contents list using the variables above. Ensure the structure matches the format expected by the multimodal model.\n",
    "\n",
    "contents = [\n",
    "    prompt,\n",
    "    video\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce38fe28",
   "metadata": {},
   "source": [
    "#### Generate responses from the multimodal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "442a83c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"COMPLETE THE MISSING PART AND RUN THIS CELL\"\n",
    "\n",
    "# In the next part of this task, you're going to generate responses from a multimodal model. capture the output of the model in the \"responses\" variable by using your \"contents\" list and video input.\n",
    "\n",
    "\n",
    "responses = multimodal_model.generate_content(contents, stream=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b090def3",
   "metadata": {},
   "source": [
    "#### Display the prompt and responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cba353",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"COMPLETE THE MISSING PART AND RUN THIS CELL\"\n",
    "\n",
    "# In the last part of this task, you're going to print your contents and responses with the prompt and responses title provided. Use descriptive titles to help organize the output (e.g., \"Prompts\", \"Model Responses\") and then display the prompt and responses by using the print() function. \n",
    "\n",
    "# Hint: \"\\n\" inserts a newline character for clearer separation between the sections.\n",
    "\n",
    "print(\"Prompts\\n\")\n",
    "print_multimodal_prompt(contents)\n",
    "\n",
    "print(\"\\nModel Responses\\n\")\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0932722c-fd8e-411a-a09e-ff1e530ba531",
   "metadata": {},
   "source": [
    "Proceed to Task 1.5 below (no progress check for Task 1.4 in lab instructions). \n",
    "\n",
    "### Task 1.5. Ask more questions about a video\n",
    "\n",
    "**Note:** Although this video contains audio, Gemini does not currently support audio input and will only answer based on the video.\n",
    "\n",
    "#### Explore the variables of the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c7d9b2e-f167-473e-8fce-65d677ec5ff9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"COMPLETE THE MISSING PART AND RUN THIS CELL\"\n",
    "\n",
    "# You're going to work with provided variables in this task. \n",
    "# First, review and describe the content/purpose of each variable below. \n",
    "\n",
    "\n",
    "prompt = \"\"\"\n",
    "Answer the following questions using the video only:\n",
    "\n",
    "How does the advertisement portray the use of technology, specifically AI, in capturing and preserving memories?\n",
    "What visual cues or storytelling elements contribute to the nostalgic atmosphere of the advertisement?\n",
    "How does the advertisement depict the role of friendship and social connections in enhancing experiences and creating memories?\n",
    "Are there any specific features or functionalities of the phone highlighted in the advertisement, besides its AI capabilities?\n",
    "\n",
    "Provide the answer JSON.\n",
    "\"\"\"\n",
    "video = Part.from_uri(\n",
    "    uri=\"gs://spls/gsp520/google-pixel-8-pro.mp4\",\n",
    "    mime_type=\"video/mp4\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae45d655",
   "metadata": {},
   "source": [
    "#### Create an input for the multimodal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f7cb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"COMPLETE THE MISSING PART AND RUN THIS CELL\"\n",
    "\n",
    "# Now, you're going to create an input for your multimodal model. Create your contents list using the variables above. Ensure the structure matches the format expected by the multimodal model.\n",
    "\n",
    "contents = [\n",
    "    prompt,\n",
    "    video\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61307a2",
   "metadata": {},
   "source": [
    "#### Generate responses from the multimodal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6dea2a83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"COMPLETE THE MISSING PART AND RUN THIS CELL\"\n",
    "\n",
    "# In the next part of this task, you're going to generate responses from a multimodal model. capture the output of the model in the \"responses\" variable by using your \"contents\" list and video input.\n",
    "\n",
    "\n",
    "responses = multimodal_model.generate_content(contents, stream=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987f77f7",
   "metadata": {},
   "source": [
    "#### Display the prompt and responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d993e0c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"COMPLETE THE MISSING PART AND RUN THIS CELL\"\n",
    "\n",
    "# In the last part of this task, you're going to print your contents and responses with the prompt and responses title provided. Use descriptive titles to help organize the output (e.g., \"Prompts\", \"Model Responses\") and then display the prompt and responses by using the print() function. \n",
    "\n",
    "# Hint: \"\\n\" inserts a newline character for clearer separation between the sections.\n",
    "\n",
    "print(\"Prompts\\n\")\n",
    "print_multimodal_prompt(contents)\n",
    "\n",
    "print(\"\\nModel Responses\\n\")\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b238a1a4-7b7c-463f-af54-d853f1786b73",
   "metadata": {},
   "source": [
    "Proceed to Task 1.6 below (no progress check for Task 1.5 in lab instructions). \n",
    "\n",
    "### Task 1.6. Retrieve extra information beyond the video\n",
    "\n",
    "#### Explore the variables of the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e1668408-6c55-460f-b687-5916b417ed85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"COMPLETE THE MISSING PART AND RUN THIS CELL\"\n",
    "\n",
    "# You're going to work with provided variables in this task. \n",
    "# First, review and describe the content/purpose of each variable below. \n",
    "\n",
    "\n",
    "prompt = \"\"\"\n",
    "Answer the following questions using the video only:\n",
    "\n",
    "How does the advertisement appeal to its target audience through its messaging and imagery?\n",
    "What overall message or takeaway does the advertisement convey about the brand and its products?\n",
    "Are there any symbolic elements or motifs used throughout the advertisement to reinforce its central themes?\n",
    "What is the best hashtag for this video based on the description ?\n",
    "\n",
    "\"\"\"\n",
    "video = Part.from_uri(\n",
    "    uri=\"gs://spls/gsp520/google-pixel-8-pro.mp4\",\n",
    "    mime_type=\"video/mp4\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868429f1",
   "metadata": {},
   "source": [
    "#### Create an input for the multimodal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3ec4cd5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"COMPLETE THE MISSING PART AND RUN THIS CELL\"\n",
    "\n",
    "# Now, you're going to create an input for your multimodal model. Create your contents list using the variables above. Ensure the structure matches the format expected by the multimodal model.\n",
    "\n",
    "contents = [\n",
    "    prompt,\n",
    "    video\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b46261e",
   "metadata": {},
   "source": [
    "#### Generate responses from the multimodal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "65bb93f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"COMPLETE THE MISSING PART AND RUN THIS CELL\"\n",
    "\n",
    "# In the next part of this task, you're going to generate responses from a multimodal model. capture the output of the model in the \"responses\" variable by using your \"contents\" list and video input.\n",
    "\n",
    "\n",
    "responses = multimodal_model.generate_content(contents, stream=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa426629",
   "metadata": {},
   "source": [
    "#### Display the prompt and responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c492f1f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"COMPLETE THE MISSING PART AND RUN THIS CELL\"\n",
    "\n",
    "# In the last part of this task, you're going to print your contents and responses with the prompt and responses title provided. Use descriptive titles to help organize the output (e.g., \"Prompts\", \"Model Responses\") and then display the prompt and responses by using the print() function. \n",
    "\n",
    "# Hint: \"\\n\" inserts a newline character for clearer separation between the sections.\n",
    "\n",
    "print(\"Prompts\\n\")\n",
    "print_multimodal_prompt(contents)\n",
    "\n",
    "print(\"\\nModel Responses\\n\")\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f908d1-b54f-4d8d-993e-faec763d7a37",
   "metadata": {},
   "source": [
    "### To verify your work for Task 1.6, click __Check my progress__ in the lab instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42950158-1e1f-40bd-97b4-d18474dd86fe",
   "metadata": {},
   "source": [
    "## Task 2. Retrieving and integrating knowledge with multimodal retrieval augmented generation (RAG)\n",
    "\n",
    "To complete Task 2, follow the instructions at the top of each notebook cell:\n",
    "* Run the cells with the comment \"RUN THIS CELL AS IS\".\n",
    "* Complete and run the cells with the comment \"COMPLETE THE MISSING PART AND RUN THIS CELL\".\n",
    "\n",
    "For additional information about the available data and helper functions for Task 2, review the section named __Available data and helper functions for Task 2__ in the lab instructions.\n",
    "\n",
    "### Setup and requirements for Task 2\n",
    "\n",
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c106e655-0da7-4eba-8595-706622ab7ef3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"RUN THIS CELL AS IS\"\n",
    "\n",
    "# Import necessary packages and libraries.\n",
    "\n",
    "# We import various classes from the Vertex AI generative models library, as well as\n",
    "# Markdown and display from IPython, which allows us to render text and objects\n",
    "# nicely within a notebook environment.\n",
    "#\n",
    "# Explanation for Beginners (inline comments follow):\n",
    "# 1) Markdown, display: Functions from IPython.display that help format and show\n",
    "#    text output in a more readable way (e.g., as Markdown).\n",
    "# 2) Content, GenerationConfig, GenerationResponse, GenerativeModel: Classes and\n",
    "#    tools for configuring and handling responses from Vertex AI generative models.\n",
    "# 3) HarmCategory, HarmBlockThreshold: Used to configure or interpret the model’s\n",
    "#    safety checks, deciding whether certain content gets blocked or allowed.\n",
    "# 4) Image, Part: Classes for handling images or multi-part content in a multimodal context.\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "from vertexai.generative_models import (\n",
    "    Content,\n",
    "    GenerationConfig,\n",
    "    GenerationResponse,\n",
    "    GenerativeModel,\n",
    "    HarmCategory,\n",
    "    HarmBlockThreshold,\n",
    "    Image,\n",
    "    Part,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860ce78c",
   "metadata": {},
   "source": [
    "#### Load the Gemini 1.0 Pro and Gemini 1.0 Pro Vision model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "be19c04e-b7d2-4439-a0bd-58ffa0c1f2dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"RUN THIS CELL AS IS\"\n",
    "\n",
    "# Load the Gemini 1.0 Pro and Gemini 1.0 Pro Vision model.\n",
    "\n",
    "# We create two GenerativeModel instances from Vertex AI:\n",
    "# 1) text_model: A text-focused Gemini model named \"gemini-1.0-pro,\" primarily used for\n",
    "#    text generation tasks (e.g., code, dialogue).\n",
    "# 2) multimodal_model: A vision-capable Gemini model named \"gemini-1.0-pro-vision,\" which\n",
    "#    can handle text + image inputs for multimodal tasks.\n",
    "\n",
    "# Explanation for Beginners:\n",
    "# 1) \"gemini-1.0-pro\": This version is optimized for text generation (like Q&A or code).\n",
    "# 2) \"gemini-1.0-pro-vision\": Extends capabilities to also process or generate image data,\n",
    "#    allowing you to pass images in the prompt.\n",
    "# 3) By creating two separate models, you can choose the one that best fits\n",
    "#    your current task (pure text vs. multimodal).\n",
    "# 4) These models can still share parameters or underlying technology but are\n",
    "#    configured for different types of input/output.\n",
    "\n",
    "text_model = GenerativeModel(\"gemini-1.0-pro\")\n",
    "multimodal_model = GenerativeModel(\"gemini-1.0-pro-vision\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb962cea-0b31-43f6-b757-90bc1ff1cd5b",
   "metadata": {},
   "source": [
    "#### Download custom Python modules and utilities \n",
    "\n",
    "The cell below will download some helper functions needed for this notebook, to improve readability. You can also view the code (`intro_multimodal_rag_utils.py`) directly on [Github](https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/use-cases/retrieval-augmented-generation/utils/intro_multimodal_rag_utils.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "12aa75ac-3512-4ee2-a4b6-e8e926d19587",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"RUN THIS CELL AS IS\"\n",
    "\n",
    "# We import modules needed for file and URL handling, as well as checking the Python environment.\n",
    "# Explanation for Beginners:\n",
    "# 1) os, sys: Python modules for interacting with the operating system and environment.\n",
    "#    - os lets us work with paths, files, and directories.\n",
    "#    - sys can provide info about the runtime environment (e.g., whether we’re in Colab).\n",
    "# 2) urllib.request: A module that helps us download files from the internet.\n",
    "#\n",
    "# The code below:\n",
    "# 1) Checks if a folder called \"utils\" exists. If not, it creates the folder.\n",
    "# 2) Defines a base URL (url_prefix) pointing to a repository with our helper scripts.\n",
    "# 3) Loops through a list of file names (\"intro_multimodal_rag_utils.py\") to download them\n",
    "#    from the URL and save them locally in the \"utils\" folder.\n",
    "\n",
    "import os\n",
    "import urllib.request\n",
    "import sys\n",
    "\n",
    "# If the \"utils\" folder doesn't exist, we create it.\n",
    "if not os.path.exists(\"utils\"):\n",
    "    os.makedirs(\"utils\")\n",
    "\n",
    "# This is the base URL from which we'll download helper scripts.\n",
    "url_prefix = (\n",
    "    \"https://raw.githubusercontent.com/\"\n",
    "    \"GoogleCloudPlatform/generative-ai/main/\"\n",
    "    \"gemini/use-cases/retrieval-augmented-generation/utils/\"\n",
    ")\n",
    "\n",
    "# List of filenames to fetch from the specified URL.\n",
    "files = [\"intro_multimodal_rag_utils.py\"]\n",
    "\n",
    "# We iterate over each file and download it to our \"utils\" directory.\n",
    "for fname in files:\n",
    "    urllib.request.urlretrieve(\n",
    "        f\"{url_prefix}/{fname}\",  # Full URL of the file to download\n",
    "        filename=f\"utils/{fname}\" # Local path where the file will be saved\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a4c367-585d-4e15-883d-ed0a674475f1",
   "metadata": {},
   "source": [
    "#### Get documents and images from Cloud Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83f06b4-4e9c-449b-94f3-4175b37777c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"RUN THIS CELL AS IS\"\n",
    "\n",
    "# Download documents and images used in this notebook.\n",
    "\n",
    "!gsutil -m rsync -r gs://spls/gsp520 .\n",
    "print(\"Download completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b72d16-d530-424b-934a-ebf091bc254c",
   "metadata": {},
   "source": [
    "### Task 2.1. Build metadata of documents containing text and images\n",
    "\n",
    "__Note__: These steps may take a few minutes to complete.\n",
    "\n",
    "#### Import helper functions to build metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "442e1759-0c37-44ff-9f2b-85ee427e7f9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"RUN THIS CELL AS IS\"\n",
    "\n",
    "# Import helper functions from utils.\n",
    "from utils.intro_multimodal_rag_utils import get_document_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcfd03c-9713-4621-b5e5-fee645d338e9",
   "metadata": {},
   "source": [
    "#### Explore the variables of the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8c2b6d28-4d64-4e03-b395-23bd2c8b6bbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"RUN THIS CELL AS IS\"\n",
    "\n",
    "# You're going to work with provided variables in this task. \n",
    "# First, review and describe the content/purpose of each variable below. \n",
    "\n",
    "\n",
    "# Specify the \"PDF folder path\" with single PDF and \"PDF folder\" with multiple PDF.\n",
    "\n",
    "pdf_folder_path = \"Google_Branding/\"  # if running in Vertex AI Workbench.\n",
    "\n",
    "# Specify the image description prompt. Change it\n",
    "image_description_prompt = \"\"\"Explain what is going on in the image.\n",
    "If it's a table, extract all elements of the table.\n",
    "If it's a graph, explain the findings in the graph.\n",
    "Do not include any numbers that are not mentioned in the image.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60611e48",
   "metadata": {},
   "source": [
    "\n",
    "#### Extract and store metadata of text and images from a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9d2331",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"COMPLETE THE MISSING PART AND RUN THIS CELL\"\n",
    "\n",
    "# Call the \"get_document_metadata\" function from the utils file to extract text and image metadata from the PDF document. Store the results in two different DataFrames: \"text_metadata_df\" and \"image_metadata_df\".  \n",
    "# text_metadata_df: This will contain extracted text snippets, their corresponding page numbers, and potentially other relevant information.\n",
    "# image_metadata_df: This will contain descriptions of the images found in the PDF (if any), along with their location within the document.\n",
    "\n",
    "# We call the get_document_metadata function to process PDFs, extracting text and images, \n",
    "# then generating embeddings for both. The function returns two DataFrames: \n",
    "# 1) text_metadata_df: Contains page-by-page and chunk-level text info and embeddings.\n",
    "# 2) image_metadata_df: Contains extracted image info and embeddings.\n",
    "\n",
    "# Explanation for Beginners:\n",
    "# 1) get_document_metadata: A function (likely from our \"intro_multimodal_rag_utils.py\") \n",
    "#    that handles reading PDF documents, extracting text chunks, images, \n",
    "#    generating textual and image embeddings, and storing all this in DataFrames.\n",
    "# 2) multimodal_model: This is an instance of a Vertex AI GenerativeModel \n",
    "#    that can handle text + image data. We're using the \"gemini-1.5-pro\" model name here \n",
    "#    for advanced reasoning and multimodal tasks.\n",
    "# 3) pdf_folder_path: The local folder path containing our PDFs to process.\n",
    "# 4) image_save_dir=\"images\": Extracted images will be saved in a local \"images\" folder.\n",
    "# 5) image_description_prompt: A text prompt that the model uses to describe each extracted image.\n",
    "# 6) embedding_size=1408: The dimension of embeddings for text and image data. \n",
    "#    Higher dimensional embeddings can capture more nuanced information.\n",
    "# 7) add_sleep_after_page=True: Tells the script to pause after processing each page \n",
    "#    to avoid hitting API rate limits or quotas.\n",
    "# 8) sleep_time_after_page=5: Defines how many seconds to wait after each page \n",
    "#    before proceeding to the next one.\n",
    "\n",
    "text_metadata_df, image_metadata_df = get_document_metadata(\n",
    "    multimodal_model,             # Using the Gemini 1.5 Pro model for analysis\n",
    "    pdf_folder_path,\n",
    "    image_save_dir=\"images\",\n",
    "    image_description_prompt=image_description_prompt,\n",
    "    embedding_size=1408,\n",
    "    add_sleep_after_page=True,    # Pause after each page to manage quotas\n",
    "    sleep_time_after_page=5,      # Number of seconds to pause after each page\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\n\\n --- Completed processing. ---\")\n",
    "\n",
    "# NOTE: This can take a few minutes to complete\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96cd147-4ce1-4110-9c5b-d755e7a07457",
   "metadata": {},
   "source": [
    "#### Inspect the processed text metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f471508-1a38-4e3b-a4be-47e82acaf54b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"COMPLETE THE MISSING PART AND RUN THIS CELL\"\n",
    "\n",
    "# Explore the text_metadata_df dataframe by displaying the first few rows of the dataframe.\n",
    "\n",
    "text_metadata_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ed0c6a-3d40-497b-b2c5-4a86c3b65c8d",
   "metadata": {},
   "source": [
    "#### Import the helper functions to implement RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c4580d12-94a1-4e1b-8ff6-587916c85241",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"COMPLETE THE MISSING PART AND RUN THIS CELL\"\n",
    "\n",
    "# Import helper functions from utils.\n",
    "\n",
    "from utils.intro_multimodal_rag_utils import (\n",
    "    get_similar_text_from_query,\n",
    "    print_text_to_text_citation,\n",
    "    get_similar_image_from_query,\n",
    "    print_text_to_image_citation,\n",
    "    get_gemini_response,\n",
    "    display_images,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f110046a-0615-4836-8776-113bba1b4088",
   "metadata": {},
   "source": [
    "Proceed to Task 2.2 below (no progress check for Task 2.1 in lab instructions). \n",
    "\n",
    "### Task 2.2. Create a user query\n",
    "\n",
    "#### Explore the variables of the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9ad2d8f5-f937-44fc-9c2d-2a1426134bb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"RUN THIS CELL AS IS\"\n",
    "\n",
    "# You're going to work with provided variables in this task. \n",
    "# First, review and describe the content/purpose of each variable below. \n",
    "\n",
    "query = \"\"\"Questions:\n",
    " - What are the key expectations that users can have from Google regarding the provision and development of its services?\n",
    "- What specific rules and guidelines are established for users when using Google services?\n",
    "- How does Google handle intellectual property rights related to the content found within its services, including content owned by users, Google, and third parties? \n",
    "- What legal rights and remedies are available to users in case of problems or disagreements with Google?\n",
    "- How do the service-specific additional terms interact with these general Terms of Service, and which terms take precedence in case of any conflicts?\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d1fa0d-3b6f-44ff-bcbf-8c8b184f9ba7",
   "metadata": {},
   "source": [
    "Proceed to Task 2.3 below (no progress check for Task 2.2 in lab instructions).\n",
    "\n",
    "### Task 2.3. Get all relevant text chunks\n",
    "\n",
    "#### Retrieve relevant chunks of text based on the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cfbc25d9-2d7e-433e-a5df-8d38eae9e75f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"COMPLETE THE MISSING PART AND RUN THIS CELL\"\n",
    "\n",
    "# Call the \"get_similar_text_from_query\" function from the utils file to retrieve relevant chunks of text based on the query. Store the results in a dictionart called \"matching_results_chunks_data\".  \n",
    "# matching_results_chunks_data: This dictionary will contain file_name, page_num, cosine_score, chunk_number and chunk_socre. The dictionary represents a search result for a query related to the text_metadata_df.\n",
    "\n",
    "# We retrieve the top 3 most relevant text chunks from the text_metadata_df DataFrame \n",
    "# by matching them against our query. We use the \"text_embedding_chunk\" column, which\n",
    "# stores the embeddings for each text chunk.\n",
    "\n",
    "# Explanation for Beginners:\n",
    "# 1) get_similar_text_from_query: A function that takes in a query string, a text DataFrame,\n",
    "#    and compares the embeddings of the query with the embeddings of each text chunk.\n",
    "# 2) column_name=\"text_embedding_chunk\": Points to the column in text_metadata_df that holds\n",
    "#    the chunk-level text embeddings.\n",
    "# 3) top_n=3: Only returns the top 3 matches to our query for easier inspection.\n",
    "# 4) chunk_text=True: Tells the function to return the actual chunk of text so we can see \n",
    "#    what's in it, instead of returning just page-level text or metadata.\n",
    "\n",
    "matching_results_chunks_data = get_similar_text_from_query(\n",
    "    query,\n",
    "    text_metadata_df,\n",
    "    column_name=\"text_embedding_chunk\",\n",
    "    top_n=3,\n",
    "    chunk_text=True,\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4ec297-ca76-4cee-a008-c22002a0f7d3",
   "metadata": {},
   "source": [
    "#### Display the first item of the text chunk dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d72676-9721-4ade-9342-5ffa4dec45ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"COMPLETE THE MISSING PART AND RUN THIS CELL\"\n",
    "\n",
    "# Explore the first item in your matching_results_chunks_data dictionary by displaying the first item.\n",
    "\n",
    "matching_results_chunks_data[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f86a088-41c6-4bd2-8fc7-45f0a7e8a2e4",
   "metadata": {},
   "source": [
    "Proceed to Task 2.4 below (no progress check for Task 2.3 in lab instructions).\n",
    "\n",
    "### Task 2.4. Create context_text\n",
    "\n",
    "#### Create a list to store the combined chunks of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e1595d30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"COMPLETE THE MISSING PART AND RUN THIS CELL\"\n",
    "\n",
    "\n",
    "# Create an empty list named \"context_text\". This list will be used to store the combined chunks of text.\n",
    "context_text = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95503358",
   "metadata": {},
   "source": [
    "#### Iterate through each item in the text chunks dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9aeddeea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"COMPLETE THE MISSING PART AND RUN THIS CELL\"\n",
    "\n",
    "# Create a for loop to iterate through each item in the matching_results_chunks_data dictionary in order to combine all the selected relevant text chunks\n",
    "\n",
    "# We iterate over the matching_results_chunks_data dictionary, which stores \n",
    "# information about each text chunk that was found relevant to our query.\n",
    "#\n",
    "# Explanation for Beginners:\n",
    "# 1) matching_results_chunks_data is a dictionary where each key is an index\n",
    "#    for a matched chunk, and each value is another dictionary containing metadata\n",
    "#    like file_name, page_num, and chunk_text.\n",
    "# 2) context_text is a list that we'll later combine or pass to a model \n",
    "#    so it can reference all the relevant chunks when generating an answer.\n",
    "# 3) value[\"chunk_text\"] is the actual text content of the chunk. We add it \n",
    "#    to the context_text list so we have a consolidated set of relevant text excerpts.\n",
    "\n",
    "for key, value in matching_results_chunks_data.items():\n",
    "    context_text.append(value[\"chunk_text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b14e4b",
   "metadata": {},
   "source": [
    "#### Join all the text chunks and store in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dc954cd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"COMPLETE THE MISSING PART AND RUN THIS CELL\"\n",
    "\n",
    "# Take all of the individual text chunks stored in the context_text list and join them together into a single string named final_context_text. Use \"\\n\" part inserts a newline character between each chunk, effectively creating separate lines or paragraphs.\n",
    "\n",
    "final_context_text = \"\\n\".join(context_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cba9b5",
   "metadata": {},
   "source": [
    "Proceed to Task 2.5 below (no progress check for Task 2.4 in lab instructions).\n",
    "\n",
    "### Task 2.5. Pass context to Gemini\n",
    "\n",
    "#### Explore the variables of the task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bf8241d2-7b7e-48c8-a7bd-1db5bcdb115f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"RUN THIS CELL AS IS\"\n",
    "\n",
    "# You're going to work with provided variables in this task. First, review and describe the content/purpose of each variable below. \n",
    "\n",
    "prompt = f\"\"\" Instructions: Compare the images and the text provided as Context: to answer multiple Question:\n",
    "Make sure to think thoroughly before answering the question and put the necessary steps to arrive at the answer in bullet points for easy explainability.\n",
    "If unsure, respond, \"Not enough context to answer\".\n",
    "\n",
    "Context:\n",
    " - Text Context:\n",
    " {final_context_text}\n",
    "\n",
    "\n",
    "{query}\n",
    "\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46376c65",
   "metadata": {},
   "source": [
    "#### Generate Gemini response with streaming output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebe99d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"COMPLETE THE MISSING PART AND RUN THIS CELL\"\n",
    "\n",
    "# Call \"get_gemini_response\" function from utils module in order to generate Gemini response with streaming output. This function uses a multimodal Gemini model, a text prompt, and configuration parameters and instructs the Gemini model to generate a response using the provided prompt. As Gemini model enables streaming, you will receive chunks of the response as they were produced. \n",
    "# Format the streamed output using Markdown syntax for easy readability and conversion to HTML.\n",
    "\n",
    "# We import Markdown from IPython.display to format our model’s response in a more readable way.\n",
    "# We also import display so we can display rich outputs inline.\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Explanation for Beginners:\n",
    "# 1) Markdown: This class renders a string of text as Markdown, \n",
    "#    enabling headings, bullet points, bold/italics, etc.\n",
    "# 2) display(...): Allows us to show rendered objects (like Markdown) inline in Jupyter.\n",
    "# 3) get_gemini_response: A custom function (likely from your utils) that calls \n",
    "#    the Gemini model, passing along a prompt plus optional config parameters.\n",
    "\n",
    "# Below, we actually call get_gemini_response on our multimodal_model:\n",
    "# 1) model_input=[prompt] sends a single string prompt (could be text or text+images).\n",
    "# 2) stream=True returns the model’s response in chunks (streaming).\n",
    "# 3) generation_config=GenerationConfig(temperature=1) sets the model’s “creativity” level;\n",
    "#    a higher temperature allows for more varied and exploratory answers.\n",
    "\n",
    "display(\n",
    "    Markdown(\n",
    "        get_gemini_response(\n",
    "            multimodal_model,                   # Our Gemini 1.0 Pro Vision (multimodal) model\n",
    "            model_input=[prompt],               # The prompt we want to generate a response for\n",
    "            stream=True,                        # Stream partial outputs in real time\n",
    "            generation_config=GenerationConfig( # Configuration for how the model generates text\n",
    "                temperature=1                    # Higher temperature => more creative output\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f691d3-2468-4efd-9040-6b2736ab85df",
   "metadata": {},
   "source": [
    "### To verify your work for Task 2.5, click __Check my progress__ in the lab instructions."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m126",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m126"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
