{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "mHF9VCProKJN",
    "tags": []
   },
   "source": [
    "# AI Explanations: Deploying an Explainable Image Model with Vertex AI "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "hZzRVxNtH-zG"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This lab shows how to train a classification model on image data and deploy it to Vertex AI to serve predictions with explanations (feature attributions). In this lab you will:\n",
    "* Explore the dataset\n",
    "* Build and train a custom image classification model with Vertex AI\n",
    "* Deploy the model to an endpoint\n",
    "* Serve predictions with explanations\n",
    "* Visualize feature attributions from Integrated Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "rgLXkyHEvTVD",
    "tags": []
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PyxoF-iqqD1t"
   },
   "source": [
    "### Import libraries\n",
    "\n",
    "Import the libraries for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell imports various libraries for handling images,\n",
    "generating random values, creating and saving plots, and\n",
    "interacting with Google Cloud AI Platform.\n",
    "\n",
    "EXTRA NOTES:\n",
    "- base64: Handy for encoding and decoding data into ASCII text, \n",
    "    especially when transmitting images or other binary data.\n",
    "- os: Helps with file paths, environment variables, and general OS-level tasks.\n",
    "- random: Offers random number generation capabilities (e.g., \n",
    "    for shuffling data or creating random IDs).\n",
    "- datetime: Assists with creating and manipulating date-time objects \n",
    "            (useful for logging, timestamps, etc.).\n",
    "- io.BytesIO: Treats bytes as if they were in a file, enabling in-memory \n",
    "        file operations without writing to disk.\n",
    "- matplotlib.image (mpimg): Specific module in matplotlib for reading \n",
    "                            and displaying images.\n",
    "- numpy (np): Fundamental package for array computing and numerical operations.\n",
    "- tensorflow (tf): Core library for neural networks and deep learning workflows.\n",
    "- tensorflow_hub (hub): Repository of pre-trained TensorFlow models, \n",
    "                        often used for transfer learning.\n",
    "- google.cloud.aiplatform: Python client library for Google Cloud AI Platform, \n",
    "                        allowing deployment and management of models.\n",
    "- matplotlib.pyplot (plt): State-based interface to matplotlib for creating figures and plots.\n",
    "\"\"\"\n",
    "\n",
    "import base64  # Provides functions for encoding/decoding data in Base64 format\n",
    "import os      # Offers a portable way of using OS-level functionality\n",
    "import random  # Contains functions related to random number generation\n",
    "from datetime import datetime  # Supplies classes for manipulating dates and times\n",
    "from io import BytesIO         # Used for handling in-memory binary streams (like image data)\n",
    "\n",
    "import matplotlib.image as mpimg  # Allows for loading and processing images in matplotlib\n",
    "import numpy as np               # Fundamental package for scientific computing with arrays\n",
    "import tensorflow as tf          # Library for building and training deep learning models\n",
    "import tensorflow_hub as hub     # Enables use of pre-trained TensorFlow models from TF Hub\n",
    "from google.cloud import aiplatform  # Tools for interacting with Google Cloud AI Platform\n",
    "from matplotlib import pyplot as plt  # Visualization library for creating plots and figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In a Jupyter notebook, the exclamation mark calls a shell command.\n",
    "# Here we retrieve the current Google Cloud Platform (GCP) project\n",
    "# configured in gcloud, which returns a list containing the project name.\n",
    "PROJECT = !(gcloud config get-value core/project)\n",
    "PROJECT = PROJECT[0]  # The returned list has one item, so we extract the string.\n",
    "\n",
    "BUCKET = PROJECT  # By default, we set the bucket name to the project name.\n",
    "REGION = \"us-central1\"  # Define the region (adjust as needed).\n",
    "\n",
    "# Generate a timestamp for naming outputs uniquely (format: YYYYMMDDHHMMSS).\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "# For clarity, reassign these values (redundant if not changed, but shown for clarity).\n",
    "BUCKET = PROJECT\n",
    "REGION = \"us-central1\"\n",
    "\n",
    "# Define a Google Cloud Storage (GCS) pattern (location) for input TFRecord files.\n",
    "GCS_PATTERN = \"gs://flowers-public/tfrecords-jpeg-192x192-2/*.tfrec\"\n",
    "\n",
    "# Construct paths for data input and model output in GCS.\n",
    "DATA_PATH = f\"gs://{BUCKET}/flowers/data\"\n",
    "OUTDIR = f\"gs://{BUCKET}/flowers/model_{TIMESTAMP}\"\n",
    "\n",
    "# Set environment variables so they can be accessed by other commands or processes.\n",
    "os.environ[\"BUCKET\"] = BUCKET\n",
    "os.environ[\"REGION\"] = REGION\n",
    "os.environ[\"DATA_PATH\"] = DATA_PATH\n",
    "os.environ[\"OUTDIR\"] = OUTDIR\n",
    "os.environ[\"TIMESTAMP\"] = TIMESTAMP\n",
    "\n",
    "# Print the current project for confirmation\n",
    "print(f\"Project: {PROJECT}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "editable": true,
    "id": "fsmCk2dwJnLZ"
   },
   "source": [
    "Run the following cell to create your Cloud Storage bucket if it does not already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "editable": true,
    "id": "160PRO3aJqLD"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# This cell checks if a GCS bucket with the name stored in $BUCKET\n",
    "# already exists. If it doesn't exist, the script creates the bucket\n",
    "# in the specified region, and then lists all available buckets.\n",
    "\n",
    "# Store the output of listing GCS directories in a variable\n",
    "exists=$(gsutil ls -d | grep -w gs://${BUCKET}/)\n",
    "\n",
    "# Check if the bucket name appears in the list of GCS directories\n",
    "if [ -n \"$exists\" ]; then\n",
    "   # If exists is not empty, the bucket already exists\n",
    "   echo -e \"Bucket gs://${BUCKET} already exists.\"\n",
    "else\n",
    "   # If exists is empty, create a new bucket in the specified region\n",
    "   echo \"Creating a new GCS bucket.\"\n",
    "   gsutil mb -l ${REGION} gs://${BUCKET}\n",
    "\n",
    "   # Print all current buckets in the project for confirmation\n",
    "   echo -e \"\\nHere are your current buckets:\"\n",
    "   gsutil ls\n",
    "fi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Dataset\n",
    "The dataset used for this tutorial is the [flowers dataset](https://www.tensorflow.org/datasets/catalog/tf_flowers) from [TensorFlow Datasets](https://www.tensorflow.org/datasets/catalog/overview). This section shows how to shuffle, split, and copy the files to your GCS bucket. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load, split, and copy the dataset to your GCS bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths for training and validation data within the DATA_PATH directory\n",
    "TRAINING_DATA_PATH = DATA_PATH + \"/training\"\n",
    "EVAL_DATA_PATH = DATA_PATH + \"/validation\"\n",
    "\n",
    "# Define the fraction of data files to be used for validation\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "# Gather all file paths that match our GCS pattern of TFRecords\n",
    "filenames = tf.io.gfile.glob(GCS_PATTERN)\n",
    "\n",
    "# Shuffle the list of filenames to ensure random distribution\n",
    "random.shuffle(filenames)\n",
    "\n",
    "# Calculate the split index for separating training and validation files\n",
    "split = int(len(filenames) * VALIDATION_SPLIT)\n",
    "\n",
    "# Slice the list into two parts: training files and validation files\n",
    "training_filenames = filenames[split:]\n",
    "validation_filenames = filenames[:split]\n",
    "\n",
    "# Copy training files to the designated GCS directory using the gsutil command\n",
    "for file in training_filenames:\n",
    "    !gsutil -m cp $file $TRAINING_DATA_PATH/\n",
    "\n",
    "# Copy validation files to the designated GCS directory using the gsutil command\n",
    "for file in validation_filenames:\n",
    "    !gsutil -m cp $file $EVAL_DATA_PATH/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following commands. You should see a number of .tfrec files in your GCS bucket at both gs://{BUCKET}/flowers/data/training and gs://{BUCKET}/flowers/data/validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the contents (with file sizes) of the TRAINING_DATA_PATH directory in GCS\n",
    "!gsutil ls -l $TRAINING_DATA_PATH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the contents (with file sizes) of the EVAL_DATA_PATH directory in GCS\n",
    "!gsutil ls -l $EVAL_DATA_PATH\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create ingest functions and visualize some of the examples\n",
    "Define and execute helper functions to plot the images and corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In this cell, we define constants and functions to load image data from TFRecords,\n",
    "convert it to a usable format, and visualize samples from the dataset. \n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Image dimensions expected by the model or visualization\n",
    "IMAGE_SIZE = [192, 192]\n",
    "\n",
    "# Number of samples in each training batch\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Do not change; these correspond to the labels in the dataset\n",
    "CLASSES = [\n",
    "    \"daisy\",\n",
    "    \"dandelion\",\n",
    "    \"roses\",\n",
    "    \"sunflowers\",\n",
    "    \"tulips\",\n",
    "]\n",
    "\n",
    "\n",
    "def read_tfrecord(example):\n",
    "    \"\"\"\n",
    "    Parses a single TFRecord example into an image and a one-hot label.\n",
    "    \"\"\"\n",
    "    # Define the expected data structure in each TFRecord\n",
    "    features = {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string),    # The image is stored as a string (bytes)\n",
    "        \"class\": tf.io.FixedLenFeature([], tf.int64),     # An integer class index (not used directly here)\n",
    "        \"one_hot_class\": tf.io.VarLenFeature(tf.float32), # A sparse representation of the one-hot label\n",
    "    }\n",
    "\n",
    "    # Parse the input 'example' based on the specified features\n",
    "    example = tf.io.parse_single_example(example, features)\n",
    "\n",
    "    # Decode the JPEG-encoded image into a tensor with shape [height, width, channels]\n",
    "    image = tf.image.decode_jpeg(example[\"image\"], channels=3)\n",
    "\n",
    "    # Convert the image data to float32 in the [0,1] range\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "\n",
    "    # Reshape the image to the specified IMAGE_SIZE plus 3 channels\n",
    "    image = tf.reshape(image, [*IMAGE_SIZE, 3])\n",
    "\n",
    "    # Convert the sparse representation of the one-hot class label to a dense tensor\n",
    "    one_hot_class = tf.sparse.to_dense(example[\"one_hot_class\"])\n",
    "\n",
    "    # Reshape the one-hot label to have size 5, corresponding to the five flower classes\n",
    "    one_hot_class = tf.reshape(one_hot_class, [5])\n",
    "\n",
    "    return image, one_hot_class\n",
    "\n",
    "\n",
    "def load_dataset(gcs_pattern):\n",
    "    \"\"\"\n",
    "    Loads TFRecord files from the given GCS pattern into a tf.data.Dataset,\n",
    "    applying 'read_tfrecord' to each example.\n",
    "    \"\"\"\n",
    "    # Gather all files matching the provided pattern\n",
    "    filenames = tf.io.gfile.glob(gcs_pattern + \"/*\")\n",
    "\n",
    "    # Create a TFRecordDataset and map each record through 'read_tfrecord'\n",
    "    ds = tf.data.TFRecordDataset(filenames).map(read_tfrecord)\n",
    "    return ds\n",
    "\n",
    "\n",
    "def dataset_to_numpy(dataset, num_samples):\n",
    "    \"\"\"\n",
    "    Converts the first 'num_samples' examples of a tf.data.Dataset \n",
    "    into NumPy arrays for both images and labels.\n",
    "    \"\"\"\n",
    "    numpy_images = []\n",
    "    numpy_labels = []\n",
    "\n",
    "    # Take 'num_samples' examples from the dataset\n",
    "    for images, labels in dataset.take(num_samples):\n",
    "        # Convert each tensor to a NumPy array and store it\n",
    "        numpy_images.append(images.numpy())\n",
    "        numpy_labels.append(labels.numpy())\n",
    "\n",
    "    return numpy_images, numpy_labels\n",
    "\n",
    "\n",
    "def display_one_image(image, title, subplot):\n",
    "    \"\"\"\n",
    "    Displays a single image within a particular subplot index.\n",
    "    \"\"\"\n",
    "    plt.subplot(subplot)\n",
    "    plt.axis(\"off\")  # Hide the axis lines and labels\n",
    "    plt.imshow(image)  # Render the image\n",
    "    plt.title(title, fontsize=16)\n",
    "    return subplot + 1\n",
    "\n",
    "\n",
    "def display_9_images_from_dataset(dataset):\n",
    "    \"\"\"\n",
    "    Retrieves and displays 9 images from the dataset along with their class labels.\n",
    "    \"\"\"\n",
    "    subplot = 331  # This corresponds to a 3x3 grid starting index\n",
    "    plt.figure(figsize=(13, 13))\n",
    "\n",
    "    # Convert 9 examples from the dataset into NumPy arrays\n",
    "    images, labels = dataset_to_numpy(dataset, 9)\n",
    "\n",
    "    # Iterate over the retrieved images and labels\n",
    "    for i, image in enumerate(images):\n",
    "        # Identify the class by finding the index of the max value in the one-hot vector\n",
    "        title = CLASSES[np.argmax(labels[i], axis=-1)]\n",
    "        subplot = display_one_image(image, title, subplot)\n",
    "\n",
    "        # Stop after 9 images\n",
    "        if i >= 8:\n",
    "            break\n",
    "\n",
    "    # Adjust layout to prevent overlapping titles or images\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Create a dataset from the specified training GCS path\n",
    "ds = load_dataset(gcs_pattern=TRAINING_DATA_PATH)\n",
    "\n",
    "# Display 9 images from the dataset along with their labels\n",
    "display_9_images_from_dataset(ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build training pipeline\n",
    "In this section you will build an application with keras to train an image classification model on Vertex AI Custom Training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a directory for the training application and an __ init __.py file (this is required for a Python application but it can be empty)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# In a Jupyter notebook cell, this script creates the directory structure\n",
    "# flowers/trainer (including intermediate directories if necessary),\n",
    "# and then creates an empty __init__.py file to mark it as a package.\n",
    "\n",
    "mkdir -p flowers/trainer\n",
    "touch flowers/trainer/__init__.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "iN69d4D9Flrh"
   },
   "source": [
    "### Create training application in train.py\n",
    "\n",
    "This code contains the training logic. Here you build an application to ingest data from GCS and train an image classification model using [mobileNet](https://tfhub.dev/google/imagenet/mobilenet_v2_100_192/feature_vector/5) as a feature extractor, then sending it's output feature vector through a tf.keras.dense layer with 5 units and softmax activation (because there are 5 possible labels). Also, use the `fire` library which enables arguments to `train_and_evaluate` to be passed via the command line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile flowers/trainer/train.py\n",
    "\"\"\"\n",
    "This module defines a training script for a flower classification model using \n",
    "TensorFlow and TensorFlow Hub. It reads TFRecord files, builds a model \n",
    "(with MobileNet v2 as a feature extractor), trains the model, \n",
    "and saves the trained model to the specified output directory.\n",
    "\n",
    "To run this script from the command line:\n",
    "    python train.py \\\n",
    "        --train_data_path=gs://.../training/*.tfrec \\\n",
    "        --eval_data_path=gs://.../validation/*.tfrec \\\n",
    "        --output_dir=gs://.../model_output \\\n",
    "        --batch_size=32 \\\n",
    "        --num_epochs=1 \\\n",
    "        --train_examples=1000\n",
    "\"\"\"\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "# 'fire' lets us create a CLI easily from this Python script,\n",
    "# so we can pass arguments from the command line.\n",
    "import fire  \n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# Define the target image size for the model\n",
    "IMAGE_SIZE = [192, 192]\n",
    "\n",
    "\n",
    "def read_tfrecord(example):\n",
    "    \"\"\"\n",
    "    Parses a single TFRecord example into an (image, label) pair.\n",
    "    The label is a one-hot encoded vector of length 5, corresponding\n",
    "    to the 5 flower classes.\n",
    "\n",
    "    Args:\n",
    "        example (tf.Tensor): A serialized TFRecord example.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (image, one_hot_class)\n",
    "            image (tf.Tensor): A float32 tensor of shape [192, 192, 3]\n",
    "            one_hot_class (tf.Tensor): A float32 tensor of shape [5]\n",
    "    \"\"\"\n",
    "    # Define the features to extract from the TFRecord\n",
    "    features = {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string),  # JPEG bytes\n",
    "        \"class\": tf.io.FixedLenFeature([], tf.int64),   # Not used directly here\n",
    "        \"one_hot_class\": tf.io.VarLenFeature(tf.float32),  # Sparse representation of one-hot label\n",
    "    }\n",
    "\n",
    "    # Parse the example to a dictionary\n",
    "    example = tf.io.parse_single_example(example, features)\n",
    "\n",
    "    # Decode the JPEG bytes into an RGB image\n",
    "    image = tf.image.decode_jpeg(example[\"image\"], channels=3)\n",
    "    # Normalize pixel values to [0, 1] range\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    # Reshape to the specified image size\n",
    "    image = tf.reshape(image, [*IMAGE_SIZE, 3])\n",
    "\n",
    "    # Convert the sparse label representation to dense\n",
    "    one_hot_class = tf.sparse.to_dense(example[\"one_hot_class\"])\n",
    "    # Ensure the label is of shape [5]\n",
    "    one_hot_class = tf.reshape(one_hot_class, [5])\n",
    "\n",
    "    return image, one_hot_class\n",
    "\n",
    "\n",
    "def load_dataset(gcs_pattern, batch_size=32, training=True):\n",
    "    \"\"\"\n",
    "    Creates a tf.data.Dataset from TFRecords stored at gcs_pattern.\n",
    "    Applies the read_tfrecord function to each record and batches the data.\n",
    "\n",
    "    Args:\n",
    "        gcs_pattern (str): The GCS path/pattern to the TFRecord files (e.g., 'gs://bucket/training/*.tfrec').\n",
    "        batch_size (int): Number of samples per batch.\n",
    "        training (bool): Whether this dataset is for training or evaluation (affects repetition).\n",
    "\n",
    "    Returns:\n",
    "        tf.data.Dataset: The dataset that yields (images, labels) in batches.\n",
    "    \"\"\"\n",
    "    # Collect all TFRecord files matching the pattern\n",
    "    filenames = tf.io.gfile.glob(gcs_pattern)\n",
    "\n",
    "    # Create a Dataset from the TFRecord files\n",
    "    ds = tf.data.TFRecordDataset(filenames).map(read_tfrecord).batch(batch_size)\n",
    "\n",
    "    # For training, repeat indefinitely; for evaluation, do not repeat\n",
    "    if training:\n",
    "        return ds.repeat()\n",
    "    else:\n",
    "        return ds\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    \"\"\"\n",
    "    Builds a Keras Sequential model using MobileNet v2 from TensorFlow Hub \n",
    "    as a feature extractor. The top layer is replaced with a Dense layer \n",
    "    for classifying 5 flower types.\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.Model: A compiled Keras model ready for training.\n",
    "    \"\"\"\n",
    "    # URL for a pre-trained MobileNet v2 feature extractor\n",
    "    mobilenet_v2 = (\n",
    "        \"https://tfhub.dev/google/imagenet/\"\n",
    "        \"mobilenet_v2_100_192/feature_vector/5\"\n",
    "    )\n",
    "\n",
    "    # Create a KerasLayer that wraps the pretrained model from TF Hub\n",
    "    feature_extractor_layer = hub.KerasLayer(\n",
    "        mobilenet_v2,\n",
    "        input_shape=[*IMAGE_SIZE, 3],  # Expected input shape\n",
    "        trainable=False               # Do not train the weights of MobileNet v2\n",
    "    )\n",
    "\n",
    "    # Build a Sequential model with the feature extractor plus a final Dense layer\n",
    "    model = tf.keras.Sequential([\n",
    "        feature_extractor_layer,\n",
    "        tf.keras.layers.Dense(5, activation=\"softmax\")  # 5 flower classes\n",
    "    ])\n",
    "\n",
    "    # Compile the model with Adam optimizer and categorical cross-entropy loss\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_and_evaluate(\n",
    "    train_data_path,\n",
    "    eval_data_path,\n",
    "    output_dir,\n",
    "    batch_size,\n",
    "    num_epochs,\n",
    "    train_examples\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains the model and evaluates it, then saves the trained model to GCS.\n",
    "\n",
    "    Args:\n",
    "        train_data_path (str): GCS path pattern for training TFRecord files (e.g., 'gs://bucket/training/*.tfrec').\n",
    "        eval_data_path (str): GCS path pattern for evaluation TFRecord files (e.g., 'gs://bucket/validation/*.tfrec').\n",
    "        output_dir (str): Directory path (GCS or local) for saving the trained model.\n",
    "        batch_size (int): Number of samples per batch.\n",
    "        num_epochs (int): Number of epochs to train.\n",
    "        train_examples (int): Total number of training examples (used to calculate steps_per_epoch).\n",
    "    \"\"\"\n",
    "    # Build the model (MobileNet v2 feature extractor + Dense output layer)\n",
    "    model = build_model()\n",
    "\n",
    "    # Prepare the training dataset (repeats indefinitely)\n",
    "    train_ds = load_dataset(\n",
    "        gcs_pattern=train_data_path,\n",
    "        batch_size=batch_size,\n",
    "        training=True\n",
    "    )\n",
    "\n",
    "    # Prepare the evaluation dataset (no repetition)\n",
    "    eval_ds = load_dataset(\n",
    "        gcs_pattern=eval_data_path,\n",
    "        batch_size=batch_size,\n",
    "        training=False\n",
    "    )\n",
    "\n",
    "    # Calculate steps_per_epoch to tell the model how many batches to consume per epoch\n",
    "    # Here, we ensure that every example in 'train_examples' is covered each epoch.\n",
    "    # The total number of steps is (train_examples / batch_size).\n",
    "    # But we also must ensure each step processes a full batch, so we compute as integer.\n",
    "    steps_per_epoch = train_examples // batch_size\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=eval_ds,\n",
    "        epochs=num_epochs,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        verbose=2  # 0=silent, 1=progress bar, 2=one line per epoch\n",
    "    )\n",
    "\n",
    "    # Save the model to the specified output directory (e.g., GCS path)\n",
    "    model.save(output_dir)\n",
    "\n",
    "    print(f\"Exported trained model to {output_dir}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Use the Fire library to create a simple CLI for train_and_evaluate\n",
    "    fire.Fire(train_and_evaluate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test training application locally \n",
    "It's always a good idea to test out a training application locally (with only a few training steps) to make sure the code runs as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Define a local directory for storing the trained model (and remove it if it exists)\n",
    "OUTDIR_LOCAL=local_test_training\n",
    "rm -rf \"${OUTDIR_LOCAL}\"\n",
    "\n",
    "# Update PYTHONPATH so that Python can find the 'trainer' module under 'flowers/trainer'\n",
    "export PYTHONPATH=\"${PYTHONPATH}:${PWD}/flowers\"\n",
    "\n",
    "# Run the training script as a module. Provide paths for training and evaluation,\n",
    "# specify where to save outputs, and set training parameters:\n",
    "# --batch_size=1  (for demonstration),\n",
    "# --num_epochs=1  (only one epoch),\n",
    "# --train_examples=10 (use a small subset of examples).\n",
    "python3 -m trainer.train \\\n",
    "    --train_data_path=\"gs://${BUCKET}/flowers/data/training/*.tfrec\" \\\n",
    "    --eval_data_path=\"gs://${BUCKET}/flowers/data/validation/*.tfrec\" \\\n",
    "    --output_dir=\"${OUTDIR_LOCAL}\" \\\n",
    "    --batch_size=1 \\\n",
    "    --num_epochs=1 \\\n",
    "    --train_examples=10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package code as source distribution\n",
    "Now that you have validated your model training code, we need to package our code as a source distribution in order to submit a custom training job to Vertex AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile flowers/setup.py\n",
    "\"\"\"\n",
    "This setup script defines how to package the 'flowers_trainer' application for \n",
    "installation or distribution. It uses setuptools to identify all included packages \n",
    "and specify package metadata and dependencies.\n",
    "\"\"\"\n",
    "\n",
    "from setuptools import find_packages\n",
    "from setuptools import setup\n",
    "\n",
    "# Use the setup() function to define package distribution parameters\n",
    "setup(\n",
    "    name='flowers_trainer',         # Package name\n",
    "    version='0.1',                  # Version number\n",
    "    packages=find_packages(),       # Automatically find all packages in the current directory\n",
    "    include_package_data=True,      # Include any package data files specified in MANIFEST.in\n",
    "    install_requires=[              # List of dependencies to install along with this package\n",
    "        'fire==0.4.0', \n",
    "        'tensorflow-hub==0.12.0'\n",
    "    ],\n",
    "    description='Flowers image classifier training application.'  # Short description of the package\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# This cell changes the current directory to 'flowers', where 'setup.py' resides,\n",
    "# creates a source distribution (in .tar.gz format), and then returns to the \n",
    "# previous directory.\n",
    "cd flowers\n",
    "python ./setup.py sdist --formats=gztar\n",
    "cd ..\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the package in GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Copy the distribution package (flowers_trainer-0.1.tar.gz)\n",
    "# from the local 'flowers/dist' directory to the specified Google Cloud Storage bucket.\n",
    "gsutil cp flowers/dist/flowers_trainer-0.1.tar.gz gs://${BUCKET}/flowers/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To submit to the Cloud we use [`gcloud custom-jobs create`](https://cloud.google.com/sdk/gcloud/reference/ai/custom-jobs/create) and simply specify some additional parameters for the Vertex AI Training Service:\n",
    "- display-name: A unique identifier for the Cloud job. We usually append system time to ensure uniqueness\n",
    "- region: Cloud region to train in. See [here](https://cloud.google.com/vertex-ai/docs/general/locations) for supported Vertex AI Training Service regions\n",
    "\n",
    "You might have earlier seen `gcloud ai custom-jobs create` executed with the `worker pool spec` and pass-through Python arguments specified directly in the command call, here we will use a YAML file, this will make it easier to transition to hyperparameter tuning.\n",
    "\n",
    "Through the `args:` argument we add in the passed-through arguments for our `task.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# In this cell, we define a custom training job configuration for Google Cloud AI Platform (Vertex AI).\n",
    "# We specify the Docker image for the training runtime, the Python package that contains our training code,\n",
    "# and various hyperparameters such as the number of epochs, the batch size, etc. \n",
    "# Then, we submit the job to Vertex AI using the 'gcloud ai custom-jobs create' command.\n",
    "\n",
    "JOB_NAME=flowers_${TIMESTAMP}\n",
    "\n",
    "# The URI where the Python package (containing our training code) is stored in GCS\n",
    "PYTHON_PACKAGE_URI=gs://${BUCKET}/flowers/flowers_trainer-0.1.tar.gz\n",
    "\n",
    "# The container image to use for executing the Python package\n",
    "PYTHON_PACKAGE_EXECUTOR_IMAGE_URI=\"us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-11.py310:latest\"\n",
    "\n",
    "# The entry point module in our package that starts the training\n",
    "PYTHON_MODULE=trainer.train\n",
    "\n",
    "# Create a YAML file that defines the worker pool specification \n",
    "# (including machine type, Python package spec, and command-line arguments for training).\n",
    "echo > ./config.yaml \\\n",
    "\"workerPoolSpecs:\n",
    "  machineSpec:\n",
    "    machineType: n1-standard-8\n",
    "  replicaCount: 1\n",
    "  pythonPackageSpec:\n",
    "    executorImageUri: $PYTHON_PACKAGE_EXECUTOR_IMAGE_URI\n",
    "    packageUris: $PYTHON_PACKAGE_URI\n",
    "    pythonModule: $PYTHON_MODULE\n",
    "    args:\n",
    "    - --train_data_path=gs://${BUCKET}/flowers/data/training/*.tfrec\n",
    "    - --eval_data_path=gs://${BUCKET}/flowers/data/validation/*.tfrec\n",
    "    - --output_dir=$OUTDIR\n",
    "    - --num_epochs=15\n",
    "    - --train_examples=15000\n",
    "    - --batch_size=32\n",
    "\"\n",
    "\n",
    "# Submit the custom training job to Vertex AI, specifying the region and the config.yaml file.\n",
    "gcloud ai custom-jobs create \\\n",
    "  --region=${REGION} \\\n",
    "  --display-name=$JOB_NAME \\\n",
    "  --config=config.yaml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE** Model training will take 5 minutes or so. You have to wait for training to finish before moving forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "Su2qu-4CW-YH"
   },
   "source": [
    "### Serving function for image data\n",
    "\n",
    "To pass images to the prediction service, you encode the compressed (e.g., JPEG) image bytes into base 64 -- which makes the content safe from modification while transmitting binary data over the network. Since this deployed model expects input data as raw (uncompressed) bytes, you need to ensure that the base 64 encoded data gets converted back to raw bytes before it is passed as input to the deployed model.\n",
    "\n",
    "To resolve this, define a serving function (`serving_fn`) and attach it to the model as a preprocessing step. Add a `@tf.function` decorator so the serving function is fused to the underlying model (instead of upstream on a CPU).\n",
    "\n",
    "When you send a prediction or explanation request, the content of the request is base 64 decoded into a Tensorflow string (`tf.string`), which is passed to the serving function (`serving_fn`). The serving function preprocesses the `tf.string` into raw (uncompressed) numpy bytes (`preprocess_fn`) to match the input requirements of the model:\n",
    "- `io.decode_jpeg`- Decompresses the JPG image which is returned as a Tensorflow tensor with three channels (RGB).\n",
    "- `image.convert_image_dtype` - Changes integer pixel values to float 32.\n",
    "- `image.resize` - Resizes the image to match the input shape for the model.\n",
    "- `resized / 255.0` - Rescales (normalization) the pixel data between 0 and 1.\n",
    "\n",
    "At this point, the data can be passed to the model (`m_call`).\n",
    "\n",
    "#### XAI Signatures\n",
    "\n",
    "When the serving function is saved back with the underlying model (`tf.saved_model.save`), you specify the input layer of the serving function as the signature `serving_default`.\n",
    "\n",
    "For XAI image models, you need to save two additional signatures from the serving function:\n",
    "\n",
    "- `xai_preprocess`: The preprocessing function in the serving function.\n",
    "- `xai_model`: The concrete function for calling the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model into memory. **NOTE** This directory will not exist if your model has not finished training. Please wait for training to complete before moving forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained Keras model from the specified output directory (OUTDIR).\n",
    "# This could be a local path or a GCS path, depending on how the model was saved.\n",
    "local_model = tf.keras.models.load_model(OUTDIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a summary of the loaded model, showing each layer’s name, \n",
    "# output shape, and number of parameters.\n",
    "local_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This constant defines the name of the tensor input key expected by the model.\n",
    "CONCRETE_INPUT = \"numpy_inputs\"\n",
    "\n",
    "def _preprocess(bytes_input):\n",
    "    \"\"\"\n",
    "    Decodes a single JPEG image, converts it to float32 in [0,1],\n",
    "    and resizes it to 192x192.\n",
    "    \"\"\"\n",
    "    # Decode the JPEG bytes into an RGB image\n",
    "    decoded = tf.io.decode_jpeg(bytes_input, channels=3)\n",
    "    # Convert the pixel values from int range [0,255] to float range [0,1]\n",
    "    decoded = tf.image.convert_image_dtype(decoded, tf.float32)\n",
    "    # Resize to the model's required dimensions (192 x 192)\n",
    "    resized = tf.image.resize(decoded, size=(192, 192))\n",
    "    return resized\n",
    "\n",
    "\n",
    "@tf.function(input_signature=[tf.TensorSpec([None], tf.string)])\n",
    "def preprocess_fn(bytes_inputs):\n",
    "    \"\"\"\n",
    "    Applies _preprocess to each image in the input batch.\n",
    "    Returns a dictionary where the key matches the model's input name.\n",
    "    \n",
    "    Args:\n",
    "        bytes_inputs (tf.Tensor): A batch of encoded images as byte strings.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Contains a single key/value pair:\n",
    "              { CONCRETE_INPUT: resized_images }.\n",
    "    \"\"\"\n",
    "    # Map over the batch of byte-string inputs, applying _preprocess to each\n",
    "    decoded_images = tf.map_fn(\n",
    "        _preprocess, \n",
    "        bytes_inputs, \n",
    "        dtype=tf.float32, \n",
    "        back_prop=False\n",
    "    )\n",
    "    # Return a dict whose key matches the model's expected input name\n",
    "    return {CONCRETE_INPUT: decoded_images}\n",
    "\n",
    "\n",
    "@tf.function(input_signature=[tf.TensorSpec([None], tf.string)])\n",
    "def serving_fn(bytes_inputs):\n",
    "    \"\"\"\n",
    "    The main serving function that decodes and preprocesses inputs,\n",
    "    then runs them through the model to obtain output probabilities.\n",
    "    \n",
    "    Args:\n",
    "        bytes_inputs (tf.Tensor): A batch of encoded images as byte strings.\n",
    "    \n",
    "    Returns:\n",
    "        tf.Tensor: Model predictions (probabilities) for each image.\n",
    "    \"\"\"\n",
    "    # Preprocess the raw byte inputs to the format expected by the model\n",
    "    images = preprocess_fn(bytes_inputs)\n",
    "    # Call the model, which returns probabilities\n",
    "    prob = m_call(**images)\n",
    "    return prob\n",
    "\n",
    "\n",
    "# Convert the model's call function to a ConcreteFunction, specifying the expected input shape.\n",
    "m_call = tf.function(local_model.call).get_concrete_function(\n",
    "    [\n",
    "        tf.TensorSpec(\n",
    "            shape=[None, 192, 192, 3], \n",
    "            dtype=tf.float32, \n",
    "            name=CONCRETE_INPUT\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Save the model in the TensorFlow SavedModel format, including the preprocessing\n",
    "# and serving signatures. \"xai_preprocess\" and \"xai_model\" are included for\n",
    "# Explainable AI (XAI) integration.\n",
    "tf.saved_model.save(\n",
    "    local_model,\n",
    "    OUTDIR,\n",
    "    signatures={\n",
    "        \"serving_default\": serving_fn,  # Default inference entry point\n",
    "        \"xai_preprocess\": preprocess_fn,  # Preprocessing function for XAI\n",
    "        \"xai_model\": m_call,            # Model function for XAI\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the serving function signature\n",
    "\n",
    "You can get the signatures of your model's input and output layers by reloading the model into memory, and querying it for the signatures corresponding to each layer.\n",
    "\n",
    "When making a prediction request, you need to route the request to the serving function instead of the model, so you need to know the input layer name of the serving function -- which you will use later when you make a prediction request.\n",
    "\n",
    "You also need to know the name of the serving function's input and output layer for constructing the explanation metadata -- which is discussed subsequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model from the specified directory (OUTDIR).\n",
    "loaded = tf.saved_model.load(OUTDIR)\n",
    "\n",
    "# Obtain the name of the serving function's input by looking at\n",
    "# the signatures stored in the loaded model. \n",
    "# 'structured_input_signature[1]' is the argument specification, \n",
    "# and 'keys()' returns the dictionary keys for the input placeholders.\n",
    "serving_input = list(\n",
    "    loaded.signatures[\"serving_default\"].structured_input_signature[1].keys()\n",
    ")[0]\n",
    "print(\"Serving function input:\", serving_input)\n",
    "\n",
    "# Similarly, find the serving function's output name from 'structured_outputs'\n",
    "serving_output = list(\n",
    "    loaded.signatures[\"serving_default\"].structured_outputs.keys()\n",
    ")[0]\n",
    "print(\"Serving function output:\", serving_output)\n",
    "\n",
    "# Retrieve the input name for our locally loaded model \n",
    "# (the name of the first layer or tensor in the model graph).\n",
    "input_name = local_model.input.name\n",
    "print(\"Model input name:\", input_name)\n",
    "\n",
    "# Retrieve the output name for our locally loaded model\n",
    "# (the name of the final layer in the model graph).\n",
    "output_name = local_model.output.name\n",
    "print(\"Model output name:\", output_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create explanation parameters for a Vertex AI model deployment.\n",
    "# Here, we're configuring the explanation method to be \"integrated gradients\"\n",
    "# and specifying that it should use 50 steps in the integration process.\n",
    "parameters = aiplatform.explain.ExplanationParameters(\n",
    "    {\n",
    "        \"integrated_gradients_attribution\": {\n",
    "            \"step_count\": 50\n",
    "        }\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the model\n",
    "\n",
    "Next, upload your model to a `Model` resource using `Model.upload()` method, with the following parameters:\n",
    "\n",
    "- `display_name`: The human readable name for the `Model` resource.\n",
    "- `artifact`: The Cloud Storage location of the trained model artifacts.\n",
    "- `serving_container_image_uri`: The serving container image.\n",
    "- `sync`: Whether to execute the upload asynchronously or synchronously.\n",
    "- `explanation_parameters`: Parameters to configure explaining for `Model`'s predictions.\n",
    "- `explanation_metadata`: Metadata describing the `Model`'s input and output for explanation.\n",
    "\n",
    "If the `upload()` method is run asynchronously, you can subsequently block until completion with the `wait()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell sets up configuration for a model name and metadata describing how the\n",
    "# model expects input and output. Then it defines metadata objects for explanation\n",
    "# inputs and outputs, which Vertex AI can use to apply integrated gradients or other\n",
    "# explanation methods to the model.\n",
    "\n",
    "MODEL_NAME = \"flower_classifier_v1\"\n",
    "\n",
    "# The input metadata dictionary specifies the model's input tensor name\n",
    "# and the data modality (in this case, an image).\n",
    "INPUT_METADATA = {\n",
    "    \"input_tensor_name\": CONCRETE_INPUT,\n",
    "    \"modality\": \"image\"\n",
    "}\n",
    "\n",
    "# The output metadata dictionary specifies the name of the model's output tensor.\n",
    "OUTPUT_METADATA = {\n",
    "    \"output_tensor_name\": serving_output\n",
    "}\n",
    "\n",
    "# ExplanationMetadata.InputMetadata and OutputMetadata help the Vertex AI\n",
    "# explanation service understand how to pass inputs to the model and interpret\n",
    "# its outputs for providing explanations.\n",
    "input_metadata = aiplatform.explain.ExplanationMetadata.InputMetadata(INPUT_METADATA)\n",
    "output_metadata = aiplatform.explain.ExplanationMetadata.OutputMetadata(OUTPUT_METADATA)\n",
    "\n",
    "# Create the ExplanationMetadata object with the defined input and output metadata.\n",
    "metadata = aiplatform.explain.ExplanationMetadata(\n",
    "    inputs={\"image\": input_metadata},\n",
    "    outputs={\"class\": output_metadata}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE** This can take a few minutes to run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Vertex AI with the given project and staging bucket.\n",
    "aiplatform.init(project=PROJECT, staging_bucket=BUCKET)\n",
    "\n",
    "# Upload a model to Vertex AI using the provided model artifacts and configuration.\n",
    "#  - display_name: Name to display in the Vertex AI console.\n",
    "#  - artifact_uri: Location of the SavedModel directory in GCS.\n",
    "#  - serving_container_image_uri: Docker image to use for model serving.\n",
    "#  - explanation_parameters & explanation_metadata: Configuration for AI Explanations.\n",
    "#  - sync=False: Makes the upload asynchronous, allowing the code to continue running while\n",
    "#    the upload operation proceeds in the background.\n",
    "\n",
    "model = aiplatform.Model.upload(\n",
    "    display_name=MODEL_NAME,\n",
    "    artifact_uri=OUTDIR,\n",
    "    serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-11:latest\",\n",
    "    explanation_parameters=parameters,\n",
    "    explanation_metadata=metadata,\n",
    "    sync=False,\n",
    ")\n",
    "\n",
    "# Wait for the asynchronous upload operation to complete.\n",
    "model.wait()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the model\n",
    "\n",
    "Next, deploy your model for online prediction. To deploy the model, you invoke the `deploy` method, with the following parameters:\n",
    "\n",
    "- `deployed_model_display_name`: A human readable name for the deployed model.\n",
    "- `traffic_split`: Percent of traffic at the endpoint that goes to this model, which is specified as a dictionary of one or more key/value pairs.\n",
    "If only one model, then specify as { \"0\": 100 }, where \"0\" refers to this model being uploaded and 100 means 100% of the traffic.\n",
    "If there are existing models on the endpoint, for which the traffic will be split, then use model_id to specify as { \"0\": percent, model_id: percent, ... }, where model_id is the model id of an existing model to the deployed endpoint. The percents must add up to 100.\n",
    "- `machine_type`: The type of machine to use for training.\n",
    "- `max_replica_count`: The maximum number of compute instances to scale to. In this tutorial, only one instance is provisioned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE** This can take a few minutes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the uploaded model to an endpoint in Vertex AI. This will create a serving\n",
    "# endpoint that you can send prediction requests to.\n",
    "endpoint = model.deploy(\n",
    "    deployed_model_display_name=MODEL_NAME,  # Name used to identify the deployed model\n",
    "    traffic_split={\"0\": 100},               # Allocate 100% of the traffic to this model\n",
    "    machine_type=\"n1-standard-4\",           # VM configuration for serving\n",
    "    min_replica_count=1,                    # Minimum number of model server replicas\n",
    "    max_replica_count=1,                    # Maximum number of model server replicas\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the request content\n",
    "You are going to send the flower image as compressed JPG image, instead of the raw uncompressed bytes:\n",
    "\n",
    "- `mpimg.imsave`: Write the uncompressed image to disk as a compressed JPEG image.\n",
    "- `tf.io.read_file`: Read the compressed JPG images back into memory as raw bytes.\n",
    "- `base64.b64encode`: Encode the raw bytes into a base 64 encoded string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the evaluation dataset from the EVAL_DATA_PATH directory\n",
    "eval_ds = load_dataset(EVAL_DATA_PATH)\n",
    "\n",
    "# Extract 5 samples from the evaluation dataset into NumPy arrays for easy inspection\n",
    "x_test, y_test = dataset_to_numpy(eval_ds, 5)\n",
    "\n",
    "# Pick one image from the eval dataset\n",
    "test_image = x_test[0]\n",
    "\n",
    "# Save this image as a JPEG file on the local disk for reference\n",
    "mpimg.imsave(\"tmp.jpg\", test_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the JPG image and encode it with base64 to send to the model endpoint. Send the encoded image to the endpoint with `endpoint.explain`. Then you can parse the response for the prediction and explanation. Full documentation on endpoint.explain can be found [here](https://cloud.google.com/vertex-ai/docs/reference/rest/v1beta1/projects.locations.endpoints/explain)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the local JPEG file (tmp.jpg) and convert it into a bytes tensor\n",
    "bytes_data = tf.io.read_file(\"tmp.jpg\")\n",
    "\n",
    "# Convert the tensor of bytes into a Python bytes object, then encode it in Base64.\n",
    "# Decode it again to a string so that it can be JSON-serializable.\n",
    "b64str = base64.b64encode(bytes_data.numpy()).decode(\"utf-8\")\n",
    "\n",
    "# According to the Vertex AI prediction/explanation API, instances are passed\n",
    "# as a list of dictionaries. Each dictionary has the model input name set to\n",
    "# another dictionary containing the Base64-encoded string with the key \"b64\".\n",
    "instances_list = [{serving_input: {\"b64\": b64str}}]\n",
    "\n",
    "# Send the instances list to the deployed endpoint and request an explanation.\n",
    "# This method returns both the prediction results and explanations.\n",
    "response = endpoint.explain(instances_list)\n",
    "\n",
    "# Print the full response, including predictions and explanation metadata.\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize feature attributions from Integrated Gradients. \n",
    "Query the response to get predictions and feature attributions. Use Matplotlib to visualize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell demonstrates how to parse the prediction and explanation\n",
    "(response) returned by the Vertex AI endpoint, then visualize both\n",
    "the original image and its explanation mask (feature attributions).\n",
    "\n",
    "We do the following:\n",
    "1. Identify the predicted class by finding the maximum confidence.\n",
    "2. Decode the original image from Base64.\n",
    "3. Decode the explanation mask from Base64.\n",
    "4. Plot the original image, the explanation mask, and an overlay of the mask\n",
    "    on the original image.\n",
    "\"\"\"\n",
    "\n",
    "import io\n",
    "from io import BytesIO\n",
    "\n",
    "import base64  # Needed for decoding the explanation mask and original image\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Classes correspond to: daisy, dandelion, roses, sunflowers, tulips\n",
    "CLASSES = [\n",
    "    \"daisy\",\n",
    "    \"dandelion\",\n",
    "    \"roses\",\n",
    "    \"sunflowers\",\n",
    "    \"tulips\",\n",
    "]\n",
    "\n",
    "# Loop through each prediction in the endpoint's response\n",
    "for prediction in response.predictions:\n",
    "    # Identify the predicted class by picking the index with the highest confidence score\n",
    "    label_index = np.argmax(prediction)\n",
    "    class_name = CLASSES[label_index]\n",
    "    confidence_score = prediction[label_index]\n",
    "\n",
    "    # Print out the predicted class and confidence\n",
    "    print(f\"Predicted class: {class_name}\")\n",
    "    print(f\"Confidence score: {confidence_score}\")\n",
    "\n",
    "    # Decode the original image (already stored as a Base64 string in b64str)\n",
    "    image_bytes = base64.b64decode(b64str)\n",
    "    image_file = BytesIO(image_bytes)\n",
    "    img = mpimg.imread(image_file, format=\"JPG\")  # Load as an RGB array\n",
    "\n",
    "# Loop through each explanation in the endpoint's response\n",
    "for explanation in response.explanations:\n",
    "    # Feature attributions are stored in explanation.attributions[0].feature_attributions\n",
    "    attributions = dict(explanation.attributions[0].feature_attributions)\n",
    "\n",
    "    # Identify which class this explanation refers to\n",
    "    xai_label_index = explanation.attributions[0].output_index[0]\n",
    "    xai_class_name = CLASSES[xai_label_index]\n",
    "\n",
    "    # The explanation mask is stored under \"b64_jpeg\" in the \"attributions\" dictionary\n",
    "    xai_b64str = attributions[\"image\"][\"b64_jpeg\"]\n",
    "\n",
    "    # Decode the explanation mask into a viewable image\n",
    "    xai_image_bytes = base64.b64decode(xai_b64str)\n",
    "    xai_image_file = io.BytesIO(xai_image_bytes)\n",
    "    xai_img = mpimg.imread(xai_image_file, format=\"JPG\")\n",
    "\n",
    "# Plot the original image, the feature attribution mask, and an overlay\n",
    "fig = plt.figure(figsize=(13, 18))\n",
    "\n",
    "# Plot the original image\n",
    "fig.add_subplot(1, 3, 1)\n",
    "plt.title(\"Input Image\")\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Plot the feature attribution mask\n",
    "fig.add_subplot(1, 3, 2)\n",
    "plt.title(\"Feature Attribution Mask\")\n",
    "plt.imshow(xai_img)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Plot the overlay of mask on the original image\n",
    "fig.add_subplot(1, 3, 3)\n",
    "plt.title(\"Overlayed Attribution Mask\")\n",
    "plt.imshow(img)\n",
    "plt.imshow(xai_img, alpha=0.6)  # Semi-transparent overlay\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's next?\n",
    "\n",
    "To learn more about AI Explanations, check out the resources here.\n",
    "\n",
    "* [Vertex AI Explanations documentation](https://cloud.google.com/vertex-ai/docs/explainable-ai)\n",
    "* [Integrated gradients paper](https://arxiv.org/abs/1703.01365)\n",
    "* [XRAI paper](https://arxiv.org/abs/1906.02825)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ai-explanations-image.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m121",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m121"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
