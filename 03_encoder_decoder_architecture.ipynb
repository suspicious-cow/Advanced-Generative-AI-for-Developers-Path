{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovpZyIhNIgoq"
   },
   "source": [
    "# Text generation with an RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Objectives\n",
    "\n",
    "- Learn how to generate text using a RNN\n",
    "- Create training examples and targets for text generation\n",
    "- Build a RNN model for sequence generation using Keras Subclassing\n",
    "- Create a text generator and evaluate the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BwpJ5IffzRG6"
   },
   "source": [
    "This tutorial demonstrates how to generate text using a character-based RNN. You will work with a dataset of Shakespeare's writing from Andrej Karpathy's [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). Given a sequence of characters from this data (\"Shakespear\"), train a model to predict the next character in the sequence (\"e\"). Longer sequences of text can be generated by calling the model repeatedly.\n",
    "\n",
    "Below is the sample output when the model in this tutorial trained for 30 epochs, and started with the prompt \"Q\":"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HcygKkEVZBaa"
   },
   "source": [
    "<pre>\n",
    "QUEENE:\n",
    "I had thought thou hadst a Roman; for the oracle,\n",
    "Thus by All bids the man against the word,\n",
    "Which are so weak of care, by old care done;\n",
    "Your children were in your holy love,\n",
    "And the precipitation through the bleeding throne.\n",
    "\n",
    "BISHOP OF ELY:\n",
    "Marry, and will, my lord, to weep in such a one were prettiest;\n",
    "Yet now I was adopted heir\n",
    "Of the world's lamentable day,\n",
    "To watch the next way with his father with his face?\n",
    "\n",
    "ESCALUS:\n",
    "The cause why then we are all resolved more sons.\n",
    "\n",
    "VOLUMNIA:\n",
    "O, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, it is no sin it should be dead,\n",
    "And love and pale as any will to that word.\n",
    "\n",
    "QUEEN ELIZABETH:\n",
    "But how long have I heard the soul for this world,\n",
    "And show his hands of life be proved to stand.\n",
    "\n",
    "PETRUCHIO:\n",
    "I say he look'd on, if I must be content\n",
    "To stay him from the fatal of our country's bliss.\n",
    "His lordship pluck'd from this sentence then for prey,\n",
    "And then let us twain, being the moon,\n",
    "were she such a case as fills m\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_bGsCP9DZFQ5"
   },
   "source": [
    "While some of the sentences are grammatical, most do not make sense. The model has not learned the meaning of words, but here are some things to consider:\n",
    "\n",
    "* The model is character-based. When training started, the model did not know how to spell an English word, or that words were even a unit of text.\n",
    "\n",
    "* The structure of the output resembles a play—blocks of text generally begin with a speaker name, in all capital letters similar to the dataset.\n",
    "\n",
    "* As demonstrated below, the model is trained on small batches of text (100 characters each), and is still able to generate a longer sequence of text with coherent structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Tensorflow with GPU Support for Fast Training\n",
    "\n",
    "To install TensorFlow with GPU support in an Anaconda environment in 2024, follow these steps:\n",
    "\n",
    "### Create and Activate Anaconda Environment\n",
    "\n",
    "First, create a new Anaconda environment and activate it by going to the Anaconda Prompt and running the following commands:\n",
    "\n",
    "```bash\n",
    "conda create --name tf_gpu python=3.9\n",
    "conda activate tf_gpu\n",
    "```\n",
    "\n",
    "### Install CUDA and cuDNN\n",
    "\n",
    "Install the CUDA Toolkit and cuDNN library using Conda:\n",
    "\n",
    "```bash\n",
    "conda install -c conda-forge cudatoolkit=11.2 cudnn=8.1.0\n",
    "```\n",
    "\n",
    "### Install TensorFlow\n",
    "\n",
    "Upgrade pip and install TensorFlow then downgrade the Numpy version to be compatible:\n",
    "\n",
    "```bash\n",
    "pip install --upgrade pip\n",
    "pip install \"tensorflow<2.11\"\n",
    "pip install \"numpy<2\"\n",
    "```\n",
    "\n",
    "*Note: As of 2024, versions above 2.10 are not officially supported for GPU on Windows native installations.*\n",
    "\n",
    "### Verify the Installation\n",
    "\n",
    "To verify that TensorFlow can access your GPU:\n",
    "\n",
    "```bash\n",
    "python -c \"import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))\"\n",
    "```\n",
    "\n",
    "If a list of GPU devices is returned, you've successfully installed TensorFlow with GPU support.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Is built with CUDA:\", tf.test.is_built_with_cuda())\n",
    "print(\"GPUs visible to TensorFlow:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "srXC6pLGLwS6"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WGyKZj3bzf9p"
   },
   "source": [
    "### Import TensorFlow and other libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # Importing the `os` module to interact with the operating system\n",
    "import warnings  # Importing the `warnings` module to manage warning messages\n",
    "import time  # Importing the `time` module to work with time-related functions\n",
    "\n",
    "import numpy as np  # Importing `numpy`, a package for numerical computations\n",
    "import tensorflow as tf  # Importing `tensorflow`, a library for machine learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yG_n40gFzf9s",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Suppress TensorFlow-specific logs to minimize console output\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EHDoRoc5PKWz"
   },
   "source": [
    "### Download the Shakespeare dataset\n",
    "\n",
    "Change the following line to run this code on your own data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pD_55cOxLkAb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Using TensorFlow's utility function to download a file and return the path to the file\n",
    "path_to_file = tf.keras.utils.get_file(\n",
    "    \"shakespeare.txt\",  # The name to save the file as locally\n",
    "    \"https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\"  # The URL from which to download the file\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UHjdCjDuSvX_"
   },
   "source": [
    "### Read the data\n",
    "\n",
    "First, we'll download the file and then decode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aavnuByVymwK",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Open the file in binary mode, read its contents, and decode it to a UTF-8 string\n",
    "text = open(path_to_file, \"rb\").read().decode(encoding=\"utf-8\")\n",
    "\n",
    "# Print the length of the text in characters\n",
    "print(f\"Length of text: {len(text)} characters\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the first 250 characters in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Duhg9NrUymwO",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print the first 250 characters of the text to provide a sample of the content\n",
    "print(text[:250])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check to see how many unique characters are in our corpus/document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IlCgQBRVymwR",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a sorted list of unique characters found in the text\n",
    "vocab = sorted(set(text))\n",
    "\n",
    "# Print the number of unique characters in the text\n",
    "print(f\"{len(vocab)} unique characters\")\n",
    "\n",
    "# Show all the characters in the vocabulary\n",
    "print(vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rNnrKn_lL-IJ"
   },
   "source": [
    "## Process the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LFjSVAlWzf-N"
   },
   "source": [
    "### Vectorize the text\n",
    "\n",
    "Before training, you need to convert the strings to a numerical representation. \n",
    "\n",
    "Using `tf.keras.layers.StringLookup` layer can convert each character into a numeric ID. It just needs the text to be split into tokens first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a86OoYtO01go",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the text into individual characters, considering UTF-8 encoding\n",
    "chars = tf.strings.unicode_split(text, input_encoding=\"UTF-8\")\n",
    "\n",
    "# Display the resulting tensor of characters\n",
    "chars\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1s4f1q3iqY8f"
   },
   "source": [
    "Now create the `tf.keras.layers.StringLookup` layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6GMlCe3qzaL9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a layer that maps characters to an integer based on the vocabulary\n",
    "ids_from_chars = tf.keras.layers.StringLookup(\n",
    "    vocabulary=list(vocab),  # The list of unique characters in the vocabulary\n",
    "    mask_token=None  # No mask token is used, meaning all characters are treated as valid\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZmX_jbgQqfOi"
   },
   "source": [
    "It converts from tokens to character IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WLv5Q_2TC2pc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert the characters to their corresponding integer IDs using the StringLookup layer\n",
    "ids = ids_from_chars(chars)\n",
    "\n",
    "# Display the resulting tensor of integer IDs\n",
    "ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tZfqhkYCymwX"
   },
   "source": [
    "Since the goal of this tutorial is to generate text, it will also be important to invert this representation and recover human-readable strings from it. For this you can use `tf.keras.layers.StringLookup(..., invert=True)`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uenivzwqsDhp"
   },
   "source": [
    "Note: Here instead of passing the original vocabulary generated with `sorted(set(text))` use the `get_vocabulary()` method of the `tf.keras.layers.StringLookup` layer so that the `[UNK]` tokens is set the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wd2m3mqkDjRj",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a layer that maps integer IDs back to their corresponding characters\n",
    "chars_from_ids = tf.keras.layers.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(),  # Get the vocabulary from the existing StringLookup layer\n",
    "    invert=True,  # Invert the mapping to go from IDs back to characters\n",
    "    mask_token=None  # No mask token is used, meaning all IDs are treated as valid\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pqTDDxS-s-H8"
   },
   "source": [
    "This layer recovers the characters from the vectors of IDs, and returns them as a `tf.RaggedTensor` of characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c2GCh0ySD44s",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert the integer IDs back to their corresponding characters using the StringLookup layer\n",
    "chars = chars_from_ids(ids)\n",
    "\n",
    "# Display the resulting tensor of characters\n",
    "chars\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-FeW5gqutT3o"
   },
   "source": [
    "You can `tf.strings.reduce_join` to join the characters back into strings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zxYI-PeltqKP",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Join the characters along the last axis (-1) to form complete strings, and convert the result to a NumPy array\n",
    "result = tf.strings.reduce_join(chars, axis=-1).numpy()\n",
    "\n",
    "# Display the resulting NumPy array of joined strings\n",
    "result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make a function to encapsulate this functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w5apvBDn9Ind",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def text_from_ids(ids):\n",
    "    \"\"\"\n",
    "    Convert a sequence of integer IDs back into a string.\n",
    "\n",
    "    Args:\n",
    "        ids: A tensor of integer IDs representing characters.\n",
    "\n",
    "    Returns:\n",
    "        A TensorFlow string tensor where the IDs have been converted back to a string.\n",
    "    \"\"\"\n",
    "    # Join the characters along the last axis to form a complete string\n",
    "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bbmsf23Bymwe"
   },
   "source": [
    "### The prediction task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wssHQ1oGymwe"
   },
   "source": [
    "Given a character, or a sequence of characters, what is the most probable next character? This is the task you're training the model to perform. The input to the model will be a sequence of characters, and you train the model to predict the output—the following character at each time step.\n",
    "\n",
    "Since RNNs maintain an internal state that depends on the previously seen elements, given all the characters computed until this moment, what is the next character?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hgsVvVxnymwf"
   },
   "source": [
    "### Create Training Examples and Targets\n",
    "\n",
    "In this step, we'll break down our text data into sequences that can be used to train the model.\n",
    "\n",
    "#### Understanding Input and Target Sequences\n",
    "\n",
    "We'll start by dividing the text into smaller sequences, where each sequence has a fixed number of characters defined by `seq_length`. These sequences are called **input sequences**.\n",
    "\n",
    "For each input sequence, we need a corresponding **target sequence**. The target sequence is just the input sequence shifted by one character to the right. This means the model will learn to predict the next character in the sequence.\n",
    "\n",
    "For example, if `seq_length` is set to 4 and our text is \"Hello\":\n",
    "- The **input sequence** would be \"Hell\"\n",
    "- The **target sequence** would be \"ello\"\n",
    "\n",
    "#### Why Add 1 to `seq_length`?\n",
    "\n",
    "To generate both the input and target sequences from the text, we'll actually create sequences of length `seq_length + 1`. This gives us one extra character at the end of each sequence that becomes the target character when the sequence is split.\n",
    "\n",
    "So, if `seq_length` is 4:\n",
    "- The sequence generated from \"Hello\" will be \"Hello\" (5 characters).\n",
    "- We'll split this sequence into:\n",
    "  - **Input:** \"Hell\"\n",
    "  - **Target:** \"ello\"\n",
    "\n",
    "#### Converting Text to Character Indices\n",
    "\n",
    "Before we can create these sequences, we need to convert our text into numerical data that TensorFlow can work with. Each character in the text will be mapped to a unique integer ID (this mapping is typically based on a predefined vocabulary).\n",
    "\n",
    "We use the `tf.data.Dataset.from_tensor_slices` function to transform the entire text into a stream of these integer IDs. This function takes a list (or array) of data (in this case, the character indices) and creates a TensorFlow dataset where each element corresponds to a single character's ID.\n",
    "\n",
    "This stream of character IDs will then be divided into sequences of `seq_length + 1`, from which we'll extract our input and target sequences for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UopbsKi88tm5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert the entire Shakespeare text into a sequence of integer IDs by first splitting it into characters\n",
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, \"UTF-8\"))\n",
    "\n",
    "# Display the resulting tensor of integer IDs representing the entire text\n",
    "all_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qmxrYDCTy-eL",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a TensorFlow dataset from the tensor of integer IDs\n",
    "# The resulting dataset `ids_dataset` now contains each integer ID as a separate element\n",
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cjH5v45-yqqH",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Iterate over the first 5 elements of the `ids_dataset`\n",
    "for ids in ids_dataset.take(5):\n",
    "    # Convert each integer ID back to its corresponding character, then decode it to a UTF-8 string\n",
    "    print(chars_from_ids(ids).numpy().decode(\"utf-8\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Define the Length of Each Sequence of Characters\n",
    "```python\n",
    "seq_length = 100\n",
    "```\n",
    "- In this line, we define `seq_length`, which determines the number of characters in each input sequence. \n",
    "- This value controls how much context the model will use when learning to predict the next character.\n",
    "- For example, if `seq_length` is 100, each input sequence fed into the model will contain 100 characters from the text.\n",
    "\n",
    "#### Calculate the Number of Training Examples per Epoch\n",
    "```python\n",
    "examples_per_epoch = len(text) // (seq_length + 1)\n",
    "```\n",
    "- Here, we calculate the number of training examples that can be generated from the entire text for one epoch of training.\n",
    "- The total number of examples is found by dividing the length of the text (`len(text)`) by `seq_length + 1`. \n",
    "- The `+1` accounts for the fact that each sequence of `seq_length` characters needs one additional character to create the corresponding target sequence.\n",
    "- The result, `examples_per_epoch`, tells us how many input-target pairs can be created from the text data. This value helps in understanding the training process and setting up the training loop correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C-G2oaTxy6km",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the length of each sequence of characters\n",
    "seq_length = 100\n",
    "\n",
    "# Calculate the number of training examples per epoch\n",
    "# The total number of examples is determined by dividing the length of the text by the sequence length plus one\n",
    "examples_per_epoch = len(text) // (seq_length + 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ZSYAcQV8OGP"
   },
   "source": [
    "The `batch` method lets you easily convert these individual characters to sequences of the desired size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BpdjRO2CzOfZ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Batch the dataset into sequences of `seq_length + 1` characters\n",
    "# The `drop_remainder=True` ensures that only full batches are kept\n",
    "sequences = ids_dataset.batch(seq_length + 1, drop_remainder=True)\n",
    "\n",
    "# Iterate over the first batch of sequences\n",
    "for seq in sequences.take(1):\n",
    "    # Convert the sequence of IDs back into characters and print the result\n",
    "    print(chars_from_ids(seq))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5PHW902-4oZt"
   },
   "source": [
    "It's easier to see what this is doing if you join the tokens back into strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QO32cMWu4a06",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Iterate over the first 5 batches of sequences\n",
    "for seq in sequences.take(5):\n",
    "    # Convert each sequence of IDs back into text and print the resulting string\n",
    "    print(text_from_ids(seq).numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UbLcIPBj_mWZ"
   },
   "source": [
    "For training you'll need a dataset of `(input, label)` pairs. Where `input` and \n",
    "`label` are sequences. At each time step the input is the current character and the label is the next character. \n",
    "\n",
    "Here's a function that takes a sequence as input, duplicates, and shifts it to align the input and label for each timestep:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9NGu-FkO_kYU",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_input_target(sequence):\n",
    "    \"\"\"\n",
    "    Split a sequence into input and target sequences for training.\n",
    "\n",
    "    Args:\n",
    "        sequence: A sequence of text (or IDs) to be split.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "            - input_text: The input sequence, which is the original sequence minus the last character.\n",
    "            - target_text: The target sequence, which is the original sequence minus the first character.\n",
    "    \"\"\"\n",
    "    input_text = sequence[:-1]  # All characters except the last one\n",
    "    target_text = sequence[1:]  # All characters except the first one\n",
    "    return input_text, target_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WxbDTJTw5u_P",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example usage of the split_input_target function\n",
    "input_text, target_text = split_input_target(list(\"Tensorflow\"))\n",
    "\n",
    "# Display the input and target sequences\n",
    "print(\"Input:\", input_text)\n",
    "print(\"Target:\", target_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B9iKPXkw5xwa",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply the split_input_target function to each sequence in the dataset\n",
    "# The `dataset` now contains pairs of input and target sequences for each original sequence\n",
    "dataset = sequences.map(split_input_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GNbw-iR0ymwj",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Iterate over the first pair of input and target sequences in the dataset\n",
    "for input_example, target_example in dataset.take(1):\n",
    "    # Convert the input sequence of IDs back into text and print it\n",
    "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
    "    # Convert the target sequence of IDs back into text and print it\n",
    "    print(\"Target:\", text_from_ids(target_example).numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJdfPmdqzf-R"
   },
   "source": [
    "### Create training batches\n",
    "\n",
    "You used `tf.data` to split the text into manageable sequences. But before feeding this data into the model, you need to shuffle the data and pack it into batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p2pGotuNzf-S",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Batch size: the number of sequences per batch\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size for shuffling the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "# Shuffle the dataset with the specified buffer size,\n",
    "# then batch the dataset and prefetch to optimize the data pipeline\n",
    "dataset = (\n",
    "    dataset.shuffle(BUFFER_SIZE)  # Shuffle the dataset with the buffer size\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)  # Batch the dataset and drop the last batch if it's incomplete\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)  # Prefetch to allow the data pipeline to be optimized by TensorFlow\n",
    ")\n",
    "\n",
    "dataset  # The final prepared dataset ready for training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r6oUuElIMgVx"
   },
   "source": [
    "## Build The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m8gPwEjRzf-Z"
   },
   "source": [
    "This section defines the model as a `keras.Model` subclass (For details see [Making new Layers and Models via subclassing](https://www.tensorflow.org/guide/keras/custom_layers_and_models)). \n",
    "\n",
    "#### Build a model with the following layers\n",
    "\n",
    "* `tf.keras.layers.Embedding`: The input layer. A trainable lookup table that will map each character-ID to a vector with `embedding_dim` dimensions;\n",
    "* `tf.keras.layers.GRU`: A type of RNN with size `units=rnn_units` (You can also use an LSTM layer here.)\n",
    "* `tf.keras.layers.Dense`: The output layer, with `vocab_size` outputs. It outputs one logit for each character in the vocabulary. These are the log-likelihood of each character according to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zHT8cLh7EAsg",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Length of the vocabulary in characters\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The dimension of the embedding vector, which represents each character in a continuous vector space\n",
    "embedding_dim = 256\n",
    "\n",
    "# The number of units in the RNN layer, which controls the capacity of the model\n",
    "rnn_units = 1024\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class below does the following:\n",
    "- We derive a class from tf.keras.Model\n",
    "- The constructor is used to define the layers of the model\n",
    "- We define the pass forward using the layers defined in the constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wj8HQ2w8z4iO",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "        \"\"\"\n",
    "        Initialize the model with an embedding layer, GRU layer, and dense output layer.\n",
    "\n",
    "        Args:\n",
    "            vocab_size: The size of the vocabulary (number of unique characters).\n",
    "            embedding_dim: The dimension of the embedding vector.\n",
    "            rnn_units: The number of units in the GRU layer.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Embedding layer: maps integer inputs to dense vectors of a fixed size\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # GRU layer: a type of RNN layer that is particularly good for sequential data\n",
    "        self.gru = tf.keras.layers.GRU(\n",
    "            rnn_units,\n",
    "            return_sequences=True,  # Return the full sequence of outputs\n",
    "            return_state=True,  # Return the final state in addition to the output\n",
    "            recurrent_initializer='glorot_uniform'  # Initialize the recurrent weights\n",
    "        )\n",
    "        \n",
    "        # Dense layer: outputs predictions for the next character in the sequence\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, inputs, states=None, return_state=False, training=False):\n",
    "        \"\"\"\n",
    "        Run the forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            inputs: The input data, typically sequences of integers representing characters.\n",
    "            states: The initial state of the RNN (optional).\n",
    "            return_state: Whether to return the final state (optional).\n",
    "            training: Whether the model is in training mode (optional).\n",
    "\n",
    "        Returns:\n",
    "            If return_state is True, returns both the output and the final state.\n",
    "            Otherwise, returns just the output.\n",
    "        \"\"\"\n",
    "        # Pass inputs through the embedding layer\n",
    "        x = self.embedding(inputs, training=training)\n",
    "        \n",
    "        # Handle the initial state explicitly if not provided\n",
    "        if states is None:\n",
    "            states = tf.zeros([tf.shape(x)[0], self.gru.units])\n",
    "        \n",
    "        # Pass the embedded inputs and initial states through the GRU layer\n",
    "        x, states = self.gru(x, initial_state=states, training=training)\n",
    "        \n",
    "        # Pass the output of the GRU layer through the dense layer\n",
    "        x = self.dense(x, training=training)\n",
    "\n",
    "        # Return the output and optionally the state\n",
    "        if return_state:\n",
    "            return x, states\n",
    "        else:\n",
    "            return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IX58Xj9z47Aw",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate the model with the specified vocabulary size, embedding dimension, and number of RNN units\n",
    "model = MyModel(\n",
    "    vocab_size=len(ids_from_chars.get_vocabulary()),  # Determine the vocabulary size based on the StringLookup layer\n",
    "    embedding_dim=embedding_dim,  # The dimension of the embedding vector\n",
    "    rnn_units=rnn_units,  # The number of units in the GRU layer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with the Adam optimizer and sparse categorical cross-entropy loss function\n",
    "model.compile(\n",
    "    optimizer='adam',  # Adam optimizer is used for training the model\n",
    "    loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True)  # Loss function that handles integer labels directly\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RkA5upJIJ7W7"
   },
   "source": [
    "For each character the model looks up the embedding, runs the GRU one timestep with the embedding as input, and applies the dense layer to generate logits predicting the log-likelihood of the next character."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ubPo0_9Prjb"
   },
   "source": [
    "## Try the model\n",
    "\n",
    "Now run the model to see that it behaves as expected.\n",
    "\n",
    "First check the shape of the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C-_70kKAPrPU",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example usage of the model with a batch of data\n",
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    # Generate predictions for the input batch using the model\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    \n",
    "    # Print the shape of the predictions tensor\n",
    "    # The shape should be (batch_size, sequence_length, vocab_size)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q6NzLBi4VM4o"
   },
   "source": [
    "In the above example the sequence length of the input is `100` but the model can be run on inputs of any length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Summary\n",
    "\n",
    "The output below provides a summary of the neural network model architecture, which is generated by calling `model.summary()`. This summary includes the following key pieces of information:\n",
    "\n",
    "1. **Model Name**: \n",
    "   - The name of the model is displayed at the top. In this case, the model is named `\"my_model\"`.\n",
    "\n",
    "2. **Layer Information**:\n",
    "   - **Layer (type)**: This column lists each layer in the model, along with its type. For instance, we have an `Embedding` layer, a `GRU` (Gated Recurrent Unit) layer, and a `Dense` (fully connected) layer.\n",
    "   - **Output Shape**: This column provides the shape of the output produced by each layer. The term \"multiple\" indicates that the output shape may vary depending on the input size.\n",
    "   - **Param #**: This column shows the number of trainable parameters in each layer. These parameters are the weights that the model will learn during training.\n",
    "\n",
    "3. **Total Parameters**:\n",
    "   - **Total params**: The total number of parameters in the model, which is the sum of parameters in all layers. For this model, there are 4,022,850 parameters.\n",
    "   - **Trainable params**: The total number of parameters that will be updated during training, which is the same as the total parameters in this case, indicating that all parameters are trainable.\n",
    "   - **Non-trainable params**: The number of parameters that are not trainable. Here, it's zero, meaning there are no parameters that are frozen or fixed during training.\n",
    "\n",
    "This summary is helpful for understanding the complexity of the model, the number of parameters involved, and how the data flows through each layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vPGmAAXmVLGC",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display a summary of the model architecture\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uwv0gEkURfx1"
   },
   "source": [
    "To get actual predictions from the model you need to sample from the output distribution, to get actual character indices. This distribution is defined by the logits over the character vocabulary.\n",
    "\n",
    "Note: It is important to _sample_ from this distribution as taking the _argmax_ of the distribution can easily get the model stuck in a loop.\n",
    "\n",
    "Try it for the first example in the batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4V4MfFg0RQJg",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sample indices from the first sequence's predicted probabilities using categorical sampling\n",
    "sampled_indices = tf.random.categorical(\n",
    "    example_batch_predictions[0],  # Use the predictions from the first sequence in the batch\n",
    "    num_samples=1  # Sample one index for each time step in the sequence\n",
    ")\n",
    "\n",
    "# Remove the extra dimension from the sampled indices and convert to a NumPy array\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QM1Vbxs_URw5"
   },
   "source": [
    "This gives us, at each timestep, a prediction of the next character index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YqFMUQc_UFgM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display the sampled indices\n",
    "sampled_indices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LfLtsP3mUhCG"
   },
   "source": [
    "#### 2. Decoding the Predicted Text\n",
    "- **Input Sequence:** \n",
    "  - The first print statement converts the `input_example_batch[0]` back into readable text using the `text_from_ids` function. This shows the actual input sequence that the model was given.\n",
    "  - **Explanation:** This is the sequence of characters that the model was supposed to use to predict the next character at each timestep.\n",
    "\n",
    "- **Predicted Sequence:** \n",
    "  - The second print statement converts the `sampled_indices` into readable text, showing the sequence of characters that the model predicted.\n",
    "  - **Explanation:** This is what the model predicts will follow the input sequence. Since the model is untrained or undertrained, the predictions are mostly nonsensical, containing a mix of characters that don’t form coherent words or sentences.\n",
    "\n",
    "### Understanding the Output\n",
    "\n",
    "- **Input:** The text that was fed into the model. For example, \"`b't what you swear!\\\\nLook, as I blow this feather from my face,\\\\nAnd as the air blows it to me again,\\\\nOb'`\" is a coherent sequence, probably taken from some literary text.\n",
    "\n",
    "- **Next Char Predictions:** This is the model's attempt to predict the following characters based on the input. Since this is an untrained or poorly trained model, the predictions \"`b'!pzTD$QgfD&DFfQjDGbDuvK\\\\n3deS?HIVwrPHzwMgc?;TgPBYoVqVzaNBA\\\\n!vl?rkO!jCZU,\\\\nus:nZjReGYSKr;nKFYMmqzn3oDiWj'`\" are mostly random and don’t form meaningful text.\n",
    "\n",
    "### What This Means\n",
    "\n",
    "- **Current Model Performance:** The model is currently not performing well because it has either not been trained or is in the very early stages of training. The predictions are essentially random, showing that the model hasn’t yet learned the patterns in the text data.\n",
    "\n",
    "- **Next Steps:** As training progresses, you would expect the model's predictions to become more accurate and start forming coherent text that closely matches the patterns in the training data.\n",
    "\n",
    "This code snippet is part of the process where you can visually inspect how well your model is learning to predict the next character in a sequence. Initially, the predictions will be poor, but with training, you should see improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xWcFwPwLSo05",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print the input sequence by converting IDs back to text\n",
    "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
    "\n",
    "# Print a blank line for separation\n",
    "print()\n",
    "\n",
    "# Print the predicted next characters based on the sampled indices\n",
    "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LJL0Q0YPY6Ee"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YCbHQHiaa4Ic"
   },
   "source": [
    "At this point the problem can be treated as a standard classification problem. Given the previous RNN state, and the input this time step, predict the class of the next character."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UAjbjY03eiQ4"
   },
   "source": [
    "#### Attach an optimizer, and a loss function\n",
    "The standard `tf.keras.losses.sparse_categorical_crossentropy` loss function works in this case because it is applied across the last dimension of the predictions.\n",
    "\n",
    "Because your model returns logits, you need to set the `from_logits` flag.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZOeWdgxNFDXq",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define the loss function\n",
    "# SparseCategoricalCrossentropy is used because the labels are integer indices rather than one-hot encoded\n",
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4HrXTACTdzY-",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate the mean loss for the example batch\n",
    "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
    "\n",
    "# Print the shape of the predictions tensor\n",
    "print(\n",
    "    \"Prediction shape: \",\n",
    "    example_batch_predictions.shape,\n",
    "    \" # (batch_size, sequence_length, vocab_size)\",\n",
    ")\n",
    "\n",
    "# Print the calculated mean loss for the example batch\n",
    "print(\"Mean loss:        \", example_batch_mean_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vkvUIneTFiow"
   },
   "source": [
    "A newly initialized model shouldn't be too sure of itself, the output logits should all have similar magnitudes. To confirm this you can check that the exponential of the mean loss is approximately equal to the vocabulary size. A much higher loss means the model is sure of its wrong answers, and is badly initialized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MAJfS5YoFiHf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate the exponential of the mean loss\n",
    "# This can be interpreted as the perplexity, a measure of how well the model is predicting the next character\n",
    "perplexity = tf.exp(example_batch_mean_loss).numpy()\n",
    "\n",
    "# Display the perplexity value\n",
    "perplexity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jeOXriLcymww"
   },
   "source": [
    "### Compiling the Model\n",
    "\n",
    "In this step, we are configuring the model for training by compiling it. The `model.compile()` function is where we specify how the model will be optimized during training, and what loss function it will use to measure its performance.\n",
    "\n",
    "#### Code Breakdown\n",
    "\n",
    "- **`optimizer=\"adam\"`**:\n",
    "  - **Adam Optimizer**: \n",
    "    - The `Adam` optimizer is a popular choice for training neural networks because it combines the benefits of two other optimizers: AdaGrad (which works well with sparse gradients) and RMSProp (which works well in online and non-stationary settings).\n",
    "    - It is efficient and requires less memory, making it suitable for large datasets and models. Additionally, Adam adapts the learning rate during training, which helps in faster convergence.\n",
    "  - **Default Settings**: \n",
    "    - In this code, we're using the default settings for the Adam optimizer. These default settings include parameters like learning rate, beta1, beta2, and epsilon, which control how the optimizer updates the model's weights during training.\n",
    "\n",
    "- **`loss=loss`**:\n",
    "  - **Loss Function**:\n",
    "    - The `loss` parameter specifies the loss function that will be used to calculate how well the model's predictions match the target values. In this case, the loss function has been defined earlier in the code as `SparseCategoricalCrossentropy`.\n",
    "    - **SparseCategoricalCrossentropy**:\n",
    "      - This loss function is typically used for multi-class classification problems where each class is represented by an integer (like in character-level text generation). It computes the cross-entropy loss between the true labels and the predicted labels, but it’s optimized for the case where labels are sparse (i.e., they are integers rather than one-hot encoded vectors).\n",
    "\n",
    "#### Why This Matters\n",
    "\n",
    "- **Optimization**:\n",
    "  - Compiling the model is a crucial step because it defines how the model will learn from the data. The optimizer and loss function play a key role in determining the model's ability to converge on a solution that minimizes errors and makes accurate predictions.\n",
    "\n",
    "- **Flexibility**:\n",
    "  - By choosing Adam and SparseCategoricalCrossentropy, we are ensuring that the model has a robust and flexible training setup that is well-suited for the type of problem we are solving (e.g., text generation, language modeling).\n",
    "\n",
    "This compilation step sets the stage for training the model, where it will iteratively adjust its internal parameters (weights) to minimize the loss and improve its predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DDl1_Een6rL0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compile the model with the Adam optimizer and the previously defined loss function\n",
    "model.compile(\n",
    "    optimizer=\"adam\",  # Adam optimizer is used for its efficiency and adaptive learning rate\n",
    "    loss=loss  # The loss function is the SparseCategoricalCrossentropy defined earlier\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ieSJdchZggUj"
   },
   "source": [
    "### Configure checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6XBUUavgF56"
   },
   "source": [
    "Use a `tf.keras.callbacks.ModelCheckpoint` to ensure that checkpoints are saved during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W6fWTriUZP-n",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = \"./training_checkpoints\"\n",
    "\n",
    "# Name of the checkpoint files, including the directory and the epoch number in the filename\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}.weights.h5\")\n",
    "\n",
    "# Define a callback to save the model's weights during training\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,  # Path where the checkpoint files will be saved\n",
    "    save_weights_only=True  # Only the model's weights will be saved, not the entire model\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Ky3F_BhgkTW"
   },
   "source": [
    "### Execute the training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IxdOA-rgyGvs"
   },
   "source": [
    "To keep training time reasonable, use 10 epochs to train the model. In Colab, set the runtime to GPU for faster training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UK-hmKjYVoll",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the number of epochs for training the model\n",
    "EPOCHS = 10\n",
    "\n",
    "# Train the model using the dataset for the specified number of epochs\n",
    "# The checkpoint_callback is used to save the model's weights after each epoch\n",
    "history = model.fit(\n",
    "    dataset,  # The dataset to train on\n",
    "    epochs=EPOCHS,  # The number of complete passes through the dataset\n",
    "    callbacks=[checkpoint_callback]  # List of callbacks to apply during training, including saving checkpoints\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kKkD5M6eoSiN"
   },
   "source": [
    "#### Generate text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oIdQ8c8NvMzV"
   },
   "source": [
    "The simplest way to generate text with this model is to run it in a loop, and keep track of the model's internal state as you execute it.\n",
    "\n",
    "Each time you call the model you pass in some text and an internal state. The model returns a prediction for the next character and its new state. Pass the prediction and state back in to continue generating text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DjGz1tDkzf-u"
   },
   "source": [
    "The following makes a single step prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iSBU1tHmlUSs",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "    def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "        \"\"\"\n",
    "        Initialize the OneStep model for generating text one character at a time.\n",
    "\n",
    "        Args:\n",
    "            model: The trained model used for generating text.\n",
    "            chars_from_ids: A function that converts token IDs to characters.\n",
    "            ids_from_chars: A function that converts characters to token IDs.\n",
    "            temperature: Controls the randomness of predictions by scaling the logits.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.temperature = temperature  # Set the temperature for controlling prediction randomness\n",
    "        self.model = model  # The trained character-level model\n",
    "        self.chars_from_ids = chars_from_ids  # Function to convert IDs to characters\n",
    "        self.ids_from_chars = ids_from_chars  # Function to convert characters to IDs\n",
    "\n",
    "        # Create a mask to prevent the \"[UNK]\" token from being generated\n",
    "        skip_ids = self.ids_from_chars([\"[UNK]\"])[:, None]\n",
    "        sparse_mask = tf.SparseTensor(\n",
    "            values=[-float(\"inf\")] * len(skip_ids),  # Assign -inf to bad indices\n",
    "            indices=skip_ids,  # Indices of the tokens to be masked\n",
    "            dense_shape=[len(ids_from_chars.get_vocabulary())]  # Match the shape to the vocabulary size\n",
    "        )\n",
    "        self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "    @tf.function\n",
    "    def generate_one_step(self, inputs, states=None):\n",
    "        \"\"\"\n",
    "        Generate a single step (character) in the sequence.\n",
    "\n",
    "        Args:\n",
    "            inputs: The input characters to the model.\n",
    "            states: The initial states for the RNN.\n",
    "        \n",
    "        Returns:\n",
    "            predicted_chars: The next predicted character.\n",
    "            states: The updated states of the RNN.\n",
    "        \"\"\"\n",
    "        # Convert input strings to token IDs\n",
    "        input_chars = tf.strings.unicode_split(inputs, \"UTF-8\")\n",
    "        input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "        # Run the model to get the predicted logits and updated states\n",
    "        predicted_logits, states = self.model(\n",
    "            inputs=input_ids, states=states, return_state=True\n",
    "        )\n",
    "\n",
    "        # Use only the last prediction in the sequence\n",
    "        predicted_logits = predicted_logits[:, -1, :]\n",
    "        predicted_logits = predicted_logits / self.temperature  # Adjust logits by temperature\n",
    "\n",
    "        # Apply the prediction mask to prevent \"[UNK]\" from being generated\n",
    "        predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "        # Sample from the adjusted logits to get the next token ID\n",
    "        predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "        predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "        # Convert the token IDs back to characters\n",
    "        predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "        # Return the predicted character and the updated states\n",
    "        return predicted_chars, states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fqMOuDutnOxK",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate the OneStep model for generating text one character at a time\n",
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9yDoa0G3IgQ"
   },
   "source": [
    "Run it in a loop to generate some text. Looking at the generated text, you'll see the model knows when to capitalize, make paragraphs and imitates a Shakespeare-like writing vocabulary. With the small number of training epochs, it has not yet learned to form coherent sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ST7PSyk9t1mT",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Start the timer to measure the runtime of text generation\n",
    "start = time.time()\n",
    "\n",
    "# Initialize the RNN states to None for the first prediction\n",
    "states = None\n",
    "\n",
    "# Set the starting string for text generation\n",
    "next_char = tf.constant([\"ROMEO:\"])\n",
    "\n",
    "# Initialize a list to hold the generated characters, starting with the input string\n",
    "result = [next_char]\n",
    "\n",
    "# Generate 1000 characters one at a time\n",
    "for n in range(1000):\n",
    "    # Generate the next character and update the RNN states\n",
    "    next_char, states = one_step_model.generate_one_step(\n",
    "        next_char, states=states\n",
    "    )\n",
    "    # Append the generated character to the result list\n",
    "    result.append(next_char)\n",
    "\n",
    "# Join the list of strings into a single string\n",
    "result = tf.strings.join(result)\n",
    "\n",
    "# Stop the timer to calculate runtime\n",
    "end = time.time()\n",
    "\n",
    "# Print the generated text and the runtime\n",
    "print(result[0].numpy().decode(\"utf-8\"), \"\\n\\n\" + \"_\" * 80)\n",
    "print(\"\\nRun time:\", end - start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AM2Uma_-yVIq"
   },
   "source": [
    "The easiest thing you can do to improve the results is to train it for longer (try `EPOCHS = 30`).\n",
    "\n",
    "You can also experiment with a different start string, try adding another RNN layer to improve the model's accuracy, or adjust the temperature parameter to generate more or less random predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_OfbI4aULmuj"
   },
   "source": [
    "If you want the model to generate text *faster* the easiest thing you can do is batch the text generation. In the example below the model generates 5 outputs in about the same time it took to generate 1 above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZkLu7Y8UCMT7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Start the timer to measure the runtime of text generation\n",
    "start = time.time()\n",
    "\n",
    "# Initialize the RNN states to None for the first prediction\n",
    "states = None\n",
    "\n",
    "# Set the starting strings for text generation with multiple instances of \"ROMEO:\"\n",
    "next_char = tf.constant([\"ROMEO:\", \"ROMEO:\", \"ROMEO:\", \"ROMEO:\", \"ROMEO:\"])\n",
    "\n",
    "# Initialize a list to hold the generated characters, starting with the input strings\n",
    "result = [next_char]\n",
    "\n",
    "# Generate 1000 characters one at a time for each starting string\n",
    "for n in range(1000):\n",
    "    # Generate the next character and update the RNN states\n",
    "    next_char, states = one_step_model.generate_one_step(\n",
    "        next_char, states=states\n",
    "    )\n",
    "    # Append the generated characters to the result list\n",
    "    result.append(next_char)\n",
    "\n",
    "# Join the list of strings into a single tensor\n",
    "result = tf.strings.join(result)\n",
    "\n",
    "# Stop the timer to calculate runtime\n",
    "end = time.time()\n",
    "\n",
    "# Print the generated text for all starting strings and the runtime\n",
    "print(result, \"\\n\\n\" + \"_\" * 80)\n",
    "print(\"\\nRun time:\", end - start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "# Function to format the text for better readability\n",
    "def format_generated_text(text_tensor):\n",
    "    # Convert the tensor to string\n",
    "    text = text_tensor.numpy().decode('utf-8')\n",
    "    \n",
    "    # Split into lines based on '\\n'\n",
    "    lines = text.split('\\n')\n",
    "    \n",
    "    # Format each line\n",
    "    formatted_lines = []\n",
    "    for line in lines:\n",
    "        if line.strip():  # Only process non-empty lines\n",
    "            # Check if the line starts with a character name (simple heuristic)\n",
    "            if \":\" in line:\n",
    "                name, dialogue = line.split(\":\", 1)\n",
    "                formatted_line = f\"{name.strip()}:{textwrap.fill(dialogue.strip(), width=70, subsequent_indent=' ' * 4)}\"\n",
    "            else:\n",
    "                formatted_line = textwrap.fill(line.strip(), width=70)\n",
    "            formatted_lines.append(formatted_line)\n",
    "        else:\n",
    "            formatted_lines.append(\"\")\n",
    "    \n",
    "    # Join formatted lines back into a single string\n",
    "    formatted_text = \"\\n\\n\".join(formatted_lines)\n",
    "    \n",
    "    return formatted_text\n",
    "\n",
    "# Apply the formatting to each generated result and print it\n",
    "for i in range(result.shape[0]):\n",
    "    formatted_text = format_generated_text(result[i])\n",
    "    print(formatted_text)\n",
    "    print(\"\\n\" + \"_\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UlUQzwu6EXam"
   },
   "source": [
    "## Export the generator\n",
    "\n",
    "This single-step model can easily be [saved and restored](https://www.tensorflow.org/guide/saved_model), allowing you to use it anywhere a `tf.saved_model` is accepted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Grk32H_CzsC",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the OneStep model to the specified directory\n",
    "tf.saved_model.save(one_step_model, \"one_step\")\n",
    "\n",
    "# Reload the saved model from the specified directory\n",
    "one_step_reloaded = tf.saved_model.load(\"one_step\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Z9bb_wX6Uuu",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the RNN states to None for the first prediction\n",
    "states = None\n",
    "\n",
    "# Set the starting string for text generation\n",
    "next_char = tf.constant([\"ROMEO:\"])\n",
    "\n",
    "# Initialize a list to hold the generated characters, starting with the input string\n",
    "result = [next_char]\n",
    "\n",
    "# Generate 100 characters one at a time using the reloaded model\n",
    "for n in range(100):\n",
    "    # Generate the next character and update the RNN states\n",
    "    next_char, states = one_step_reloaded.generate_one_step(\n",
    "        next_char, states=states\n",
    "    )\n",
    "    # Append the generated character to the result list\n",
    "    result.append(next_char)\n",
    "\n",
    "# Join the list of strings into a single string\n",
    "final_result = tf.strings.join(result)\n",
    "\n",
    "# Print the generated text after decoding it from a tensor to a UTF-8 string\n",
    "print(final_result[0].numpy().decode(\"utf-8\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y4QwTjAM6A2O"
   },
   "source": [
    "## Advanced: Customized Training\n",
    "\n",
    "The above training procedure is simple, but does not give you much control.\n",
    "It uses teacher-forcing which prevents bad predictions from being fed back to the model, so the model never learns to recover from mistakes.\n",
    "\n",
    "So now that you've seen how to run the model manually next you'll implement the training loop. This gives a starting point if, for example, you want to implement _curriculum  learning_ to help stabilize the model's open-loop output.\n",
    "\n",
    "The most important part of a custom training loop is the train step function.\n",
    "\n",
    "Use `tf.GradientTape` to track the gradients. You can learn more about this approach by reading the [eager execution guide](https://www.tensorflow.org/guide/eager).\n",
    "\n",
    "The basic procedure is:\n",
    "\n",
    "1. Execute the model and calculate the loss under a `tf.GradientTape`.\n",
    "2. Calculate the updates and apply them to the model using the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x0pZ101hjwW0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomTraining(MyModel):\n",
    "    @tf.function\n",
    "    def train_step(self, inputs):\n",
    "        \"\"\"\n",
    "        Perform a single training step.\n",
    "\n",
    "        Args:\n",
    "            inputs: A tuple containing the input data and the corresponding labels.\n",
    "\n",
    "        Returns:\n",
    "            A dictionary containing the loss value for the training step.\n",
    "        \"\"\"\n",
    "        # Unpack the input data and labels\n",
    "        inputs, labels = inputs\n",
    "        \n",
    "        # Record the operations for automatic differentiation\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Make predictions using the model\n",
    "            predictions = self(inputs, training=True)\n",
    "            # Calculate the loss using the model's loss function\n",
    "            loss = self.loss(labels, predictions)\n",
    "        \n",
    "        # Compute the gradients of the loss with respect to the model's trainable variables\n",
    "        grads = tape.gradient(loss, self.trainable_variables)\n",
    "        \n",
    "        # Apply the gradients to the optimizer to update the model's weights\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "\n",
    "        # Return the loss value as a dictionary\n",
    "        return {\"loss\": loss}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Oc-eJALcK8B"
   },
   "source": [
    "The above implementation of the `train_step` method follows [Keras' `train_step` conventions](https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit). This is optional, but it allows you to change the behavior of the train step and still use keras' `Model.compile` and `Model.fit` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XKyWiZ_Lj7w5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate the CustomTraining model with the specified parameters\n",
    "model = CustomTraining(\n",
    "    vocab_size=len(ids_from_chars.get_vocabulary()),  # Determine the vocabulary size based on the StringLookup layer\n",
    "    embedding_dim=embedding_dim,  # The dimension of the embedding vector\n",
    "    rnn_units=rnn_units  # The number of units in the GRU layer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U817KUm7knlm",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compile the CustomTraining model with the Adam optimizer and sparse categorical cross-entropy loss function\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),  # Adam optimizer is used for training the model\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),  # Loss function for integer labels, expecting logits as inputs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o694aoBPnEi9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train the CustomTraining model on the dataset for 1 epoch\n",
    "model.fit(\n",
    "    dataset,  # The dataset to train on\n",
    "    epochs=1  # Number of epochs to train the model\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W8nAtKHVoInR"
   },
   "source": [
    "Or if you need more control, you can write your own complete custom training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d4tSNwymzf-q",
    "tags": []
   },
   "outputs": [],
   "source": [
    "EPOCHS = 10  # Number of epochs to train the model\n",
    "\n",
    "# Create a Mean metric to track the average loss during training\n",
    "mean = tf.metrics.Mean()\n",
    "\n",
    "# Loop over each epoch\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()  # Start timing the epoch\n",
    "\n",
    "    mean.reset_states()  # Reset the Mean metric at the start of each epoch\n",
    "    for batch_n, (inp, target) in enumerate(dataset):\n",
    "        # Perform a training step and get the logs (including the loss)\n",
    "        logs = model.train_step([inp, target])\n",
    "        # Update the Mean metric with the loss from the current batch\n",
    "        mean.update_state(logs[\"loss\"])\n",
    "\n",
    "        # Print the loss every 50 batches\n",
    "        if batch_n % 50 == 0:\n",
    "            template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
    "            print(template)\n",
    "\n",
    "    # Save the model's weights as a checkpoint every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
    "\n",
    "    # Print the average loss for the epoch and the time taken\n",
    "    print()\n",
    "    print(f\"Epoch {epoch+1} Loss: {mean.result().numpy():.4f}\")\n",
    "    print(f\"Time taken for 1 epoch {time.time() - start:.2f} sec\")\n",
    "    print(\"_\" * 80)\n",
    "\n",
    "# Save the model's weights after the final epoch\n",
    "model.save_weights(checkpoint_prefix.format(epoch=epoch))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of generating text after training\n",
    "start_string = \"JULIET:\"  # The initial string to start generating text\n",
    "num_generate = 1000  # Number of characters to generate\n",
    "\n",
    "# Convert the start string to a TensorFlow constant\n",
    "input_eval = [start_string]\n",
    "input_eval = tf.constant(input_eval)\n",
    "\n",
    "# Initialize a list to hold the generated text\n",
    "generated_text = []\n",
    "\n",
    "# Initialize RNN states to None for the first prediction\n",
    "states = None\n",
    "\n",
    "# Generate characters one by one\n",
    "for i in range(num_generate):\n",
    "    # Generate the next character and update the RNN states\n",
    "    next_char, states = one_step_model.generate_one_step(input_eval, states=states)\n",
    "    \n",
    "    # Use the generated character as the next input to the model\n",
    "    input_eval = next_char\n",
    "    \n",
    "    # Append the generated character to the list\n",
    "    generated_text.append(next_char)\n",
    "\n",
    "# Join the list of generated characters into a single string\n",
    "generated_text = tf.strings.join(generated_text).numpy()[0].decode('utf-8')\n",
    "\n",
    "# Print the generated text\n",
    "print(generated_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary and Conclusion\n",
    "In this notebook, we built and trained a character-level language model using TensorFlow. The model was trained on a dataset of text to predict the next character in a sequence, and we demonstrated its ability to generate text by simulating a play-like structure. The notebook also provided a custom training loop, model checkpointing, and text generation examples.\n",
    "\n",
    "**Next Steps:**\n",
    "- Experiment with different architectures (e.g., LSTM instead of GRU).\n",
    "- Fine-tune the model on different datasets or with more sophisticated data preprocessing.\n",
    "- Deploy the model in a web app to allow interactive text generation.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "text_generation.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m121",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m121"
  },
  "kernelspec": {
   "display_name": "general_datascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
