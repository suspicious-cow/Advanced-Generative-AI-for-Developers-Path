{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ijGzTHJJUCPY"
   },
   "outputs": [],
   "source": [
    "# Copyright 2024 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NDsTUvKjwHBW"
   },
   "source": [
    "# Multimodal Retrieval Augmented Generation (RAG) using Gemini API in Vertex AI\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/retrieval-augmented-generation/intro_multimodal_rag.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fuse-cases%2Fretrieval-augmented-generation%2Fintro_multimodal_rag.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Run in Colab Enterprise\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/retrieval-augmented-generation/intro_multimodal_rag.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://upload.wikimedia.org/wikipedia/commons/9/91/Octicons-mark-github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/use-cases/retrieval-augmented-generation/intro_multimodal_rag.ipynb\">\n",
    "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>    \n",
    "</table>\n",
    "\n",
    "<div style=\"clear: both;\"></div>\n",
    "\n",
    "<b>Share to:</b>\n",
    "\n",
    "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/retrieval-augmented-generation/intro_multimodal_rag.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/retrieval-augmented-generation/intro_multimodal_rag.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/retrieval-augmented-generation/intro_multimodal_rag.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/53/X_logo_2023_original.svg\" alt=\"X logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://reddit.com/submit?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/retrieval-augmented-generation/intro_multimodal_rag.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/retrieval-augmented-generation/intro_multimodal_rag.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
    "</a>            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BsjCZ1v9rP7s"
   },
   "source": [
    "| | |\n",
    "|-|-|\n",
    "|Author(s) | [Lavi Nigam](https://github.com/lavinigam-gcp) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7CBqVzyjHeBk"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ There is a new version of this notebook with new data and some modifications here:  ⚠️</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m0KqoJXJHzkT"
   },
   "source": [
    "[**building_DIY_multimodal_qa_system_with_mRAG.ipynb**](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/qa-ops/building_DIY_multimodal_qa_system_with_mRAG.ipynb)\n",
    "\n",
    "You can, however, still use this notebook as it is fully functional and has updated Gemini and text-embedding models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VK1Q5ZYdVL4Y"
   },
   "source": [
    "## Overview\n",
    "\n",
    "Retrieval augmented generation (RAG) has become a popular paradigm for enabling LLMs to access external data and also as a mechanism for grounding to mitigate against hallucinations.\n",
    "\n",
    "In this notebook, you will learn how to perform multimodal RAG where you will perform Q&A over a financial document filled with both text and images.\n",
    "\n",
    "### Gemini\n",
    "\n",
    "Gemini is a family of generative AI models developed by Google DeepMind that is designed for multimodal use cases. The Gemini API gives you access to the Gemini 1.0 Pro Vision and Gemini 1.0 Pro models.\n",
    "\n",
    "### Comparing text-based and multimodal RAG\n",
    "\n",
    "Multimodal RAG offers several advantages over text-based RAG:\n",
    "\n",
    "1. **Enhanced knowledge access:** Multimodal RAG can access and process both textual and visual information, providing a richer and more comprehensive knowledge base for the LLM.\n",
    "2. **Improved reasoning capabilities:** By incorporating visual cues, multimodal RAG can make better informed inferences across different types of data modalities.\n",
    "\n",
    "This notebook shows you how to use RAG with Gemini API in Vertex AI, [text embeddings](https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/text-embeddings), and [multimodal embeddings](https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/multimodal-embeddings), to build a document search engine.\n",
    "\n",
    "Through hands-on examples, you will discover how to construct a multimedia-rich metadata repository of your document sources, enabling search, comparison, and reasoning across diverse information streams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RQT500QqVPIb"
   },
   "source": [
    "### Objectives\n",
    "\n",
    "This notebook provides a guide to building a document search engine using multimodal retrieval augmented generation (RAG), step by step:\n",
    "\n",
    "1. Extract and store metadata of documents containing both text and images, and generate embeddings the documents\n",
    "2. Search the metadata with text queries to find similar text or images\n",
    "3. Search the metadata with image queries to find similar images\n",
    "4. Using a text query as input, search for contextual answers using both text and images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KnpYxfesh2rI"
   },
   "source": [
    "### Costs\n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "- Vertex AI\n",
    "\n",
    "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing) and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DXJpXzKrh2rJ"
   },
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5afkyDMSBW5"
   },
   "source": [
    "### Install Vertex AI SDK for Python and other dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kc4WxYmLSBW5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade --user google-cloud-aiplatform pymupdf rich colorama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5Xep4W9lq-Z"
   },
   "source": [
    "### Restart current runtime\n",
    "\n",
    "To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which will restart the current kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XRvKdaPDTznN"
   },
   "outputs": [],
   "source": [
    "# Restart kernel after installs so that your environment can access the new packages\n",
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SbmM4z7FOBpM"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ The kernel is going to restart. Please wait until it is finished before continuing to the next step. ⚠️</b>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FtsU9Bw9h2rL"
   },
   "source": [
    "### Authenticate your notebook environment (Colab only)\n",
    "\n",
    "If you are running this notebook on Google Colab, run the following cell to authenticate your environment. This step is not required if you are using [Vertex AI Workbench](https://cloud.google.com/vertex-ai-workbench) or [Colab Enterprise](https://cloud.google.com/colab/docs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GpYEyLsOh2rL"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Additional authentication is required for Google Colab\n",
    "if \"google.colab\" in sys.modules:\n",
    "    # Authenticate user to Google Cloud\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O1vKZZoEh2rL"
   },
   "source": [
    "### Define Google Cloud project information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gJqZ76rJh2rM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We define project-specific information and demonstrate how to retrieve the\n",
    "# project ID automatically if we are not running in Google Colab.\n",
    "#\n",
    "# Explanation for Beginners:\n",
    "# 1) PROJECT_ID: This variable holds the ID of your Google Cloud project \n",
    "#    (e.g., \"my-gcp-project\") so that all operations in Vertex AI can be \n",
    "#    correctly associated with your account.\n",
    "# 2) LOCATION: This variable represents the region where your Vertex AI resources \n",
    "#    will be created (e.g., \"us-central1\"). It's important to match this \n",
    "#    to where your project and resources are located.\n",
    "# 3) Checking \"google.colab\" in sys.modules: This helps us detect whether \n",
    "#    our code is running in Google Colab. If it's not, we assume a local \n",
    "#    environment, where we attempt to retrieve the project ID from \"gcloud\" \n",
    "#    config automatically. \n",
    "# 4) subprocess.check_output([\"gcloud\", ...]): We execute a gcloud command \n",
    "#    in the background to fetch the current gcloud-configured project ID, \n",
    "#    then strip() removes extra whitespace or newline characters.\n",
    "\n",
    "import sys\n",
    "\n",
    "PROJECT_ID = \"[your project here]\"  # @param {type:\"string\"}\n",
    "LOCATION = \"[your location here]\"   # @param {type:\"string\"}\n",
    "\n",
    "# If this notebook is not running on Google Colab, we attempt to retrieve\n",
    "# the default project ID from the user's local gcloud settings.\n",
    "if \"google.colab\" not in sys.modules:\n",
    "    import subprocess\n",
    "\n",
    "    # Run a gcloud command to get the current config's project ID.\n",
    "    PROJECT_ID = subprocess.check_output(\n",
    "        [\"gcloud\", \"config\", \"get-value\", \"project\"], text=True\n",
    "    ).strip()\n",
    "\n",
    "# Finally, we print out the project ID that we are using so we can confirm it's correct.\n",
    "print(f\"Your project ID is: {PROJECT_ID}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "D48gUW5-h2rM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We import the sys module, which can be helpful for environment checks, \n",
    "# though it is not strictly necessary for initializing Vertex AI.\n",
    "import sys\n",
    "\n",
    "# Explanation for Beginners:\n",
    "# 1) vertexai: This library allows us to interact with Google Cloud's Vertex AI.\n",
    "#    By calling vertexai.init, we specify which project and location we want \n",
    "#    to use for our AI resources (models, endpoints, etc.).\n",
    "# 2) PROJECT_ID and LOCATION: These variables should be set before running this cell,\n",
    "#    so we can properly tell Vertex AI which GCP project to bill and which region to host in.\n",
    "# 3) vertexai.init(...): This call \"logs us in\" to our project’s Vertex AI environment.\n",
    "\n",
    "# We import the vertexai library to interact with Vertex AI services.\n",
    "import vertexai\n",
    "\n",
    "# We initialize Vertex AI using the project and location defined earlier.\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BuQwwRiniVFG"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "rtMowvm-yQ97",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# I'm importing tools to display rich and Markdown-formatted text within a Jupyter environment.\n",
    "# For example, 'Markdown' and 'rich_Markdown' let me render stylized text output.\n",
    "# The 'vertexai.generative_models' library gives me access to:\n",
    "#   1) GenerationConfig, which configures generative model parameters (like temperature).\n",
    "#   2) GenerativeModel, the main class for text, code, or multimodal content generation.\n",
    "#   3) Image, a specialized class for handling and generating images.\n",
    "from IPython.display import Markdown, display  # Display objects (e.g., Markdown) in a Jupyter notebook\n",
    "from rich.markdown import Markdown as rich_Markdown  # A rich-text Markdown renderer for console output\n",
    "\n",
    "# From the Vertex AI generative models library:\n",
    "from vertexai.generative_models import (\n",
    "    GenerationConfig,  # Provides options for controlling generation (e.g., temperature, max tokens)\n",
    "    GenerativeModel,   # The foundational class for working with Vertex AI generative models\n",
    "    Image              # A class that represents or generates images via Vertex AI\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r-TX_R_xh2rM"
   },
   "source": [
    "### Load the Gemini 1.5 Pro and Gemini 1.5 Flash models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "SvMwSRJJh2rM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Here, we instantiate three Vertex AI GenerativeModel instances. Each constructor call\n",
    "# includes the name of a specific model we want to use:\n",
    "# \n",
    "# 1) text_model:      A model configured with \"gemini-1.5-pro\" for text-only tasks.\n",
    "# 2) multimodal_model:        Also \"gemini-1.5-pro,\" but we may use it for multimodal tasks\n",
    "#                             like text, image, or video.\n",
    "# 3) multimodal_model_flash:  A faster (flash) variant of the 1.5 Gemini model\n",
    "#                             for reduced latency at potential trade-off in quality.\n",
    "text_model = GenerativeModel(\"gemini-1.5-pro\")       # For text or code generation\n",
    "multimodal_model = GenerativeModel(\"gemini-1.5-pro\") # For multimodal tasks (text, images, etc.)\n",
    "multimodal_model_flash = GenerativeModel(\"gemini-1.5-flash\")  # Faster inference version\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1lCfREXK5SWD"
   },
   "source": [
    "### Download custom Python utilities & required files\n",
    "\n",
    "The cell below will download a helper functions needed for this notebook, to improve readability. It also downloads other required files. You can also view the code for the utils here: (`intro_multimodal_rag_utils.py`) directly on [GitHub](https://storage.googleapis.com/github-repo/rag/intro_multimodal_rag/intro_multimodal_rag_old_version/intro_multimodal_rag_utils.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KwbL89zcY39N",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We download documents and images from a Google Cloud Storage bucket into our local directory.\n",
    "# We use the 'gsutil -m rsync -r' command to perform a recursive synchronization, \n",
    "# ensuring that the local folder matches the contents of the specified bucket directory.\n",
    "\n",
    "# Explanation for Beginners:\n",
    "# 1) gsutil is a command-line tool for working with Google Cloud Storage (GCS).\n",
    "# 2) -m flag enables parallel (multi-threaded) transfer for faster sync.\n",
    "# 3) rsync -r recursively syncs files and subfolders between the GCS path and our local directory.\n",
    "!gsutil -m rsync -r gs://github-repo/rag/intro_multimodal_rag/intro_multimodal_rag_old_version .\n",
    "print(\"Download completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ps1G-cCfpibN"
   },
   "source": [
    "## Building metadata of documents containing text and images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jqLsy3iZ5t-R"
   },
   "source": [
    "### The data\n",
    "\n",
    "The source data that you will use in this notebook is a modified version of [Google-10K](https://abc.xyz/assets/investor/static/pdf/20220202_alphabet_10K.pdf) which provides a comprehensive overview of the company's financial performance, business operations, management, and risk factors. As the original document is rather large, you will be using a modified version with only 14 pages, split into two parts - [Part 1](https://storage.googleapis.com/github-repo/rag/intro_multimodal_rag/intro_multimodal_rag_old_version/data/google-10k-sample-part1.pdf) and [Part 2](https://storage.googleapis.com/github-repo/rag/intro_multimodal_rag/intro_multimodal_rag_old_version/data/google-10k-sample-part2.pdf) instead. Although it's truncated, the sample document still contains text along with images such as tables, charts, and graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zvt0sus5KSNX"
   },
   "source": [
    "### Import helper functions to build metadata\n",
    "\n",
    "Before building the multimodal RAG system, it's important to have metadata of all the text and images in the document. For references and citations purposes, the metadata should contain essential elements, including page number, file name, image counter, and so on. Hence, as a next step, you will generate embeddings from the metadata, which will is required to perform similarity search when querying the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "N3wo2jv2rP7v",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We import the `get_document_metadata` function from our `intro_multimodal_rag_utils` module.\n",
    "# This function processes PDFs by extracting both text and images, generating embeddings,\n",
    "# and returning two DataFrames: one for text metadata, and one for image metadata.\n",
    "#\n",
    "# Explanation for Beginners:\n",
    "# 1) `intro_multimodal_rag_utils`: This is a custom utility module that contains various\n",
    "#    helper functions for multimodal Retrieval-Augmented Generation (RAG).\n",
    "# 2) `get_document_metadata`: Specifically handles extracting text and images from PDFs,\n",
    "#    then embedding them for later similarity searches and generative tasks.\n",
    "\n",
    "from intro_multimodal_rag_utils import get_document_metadata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5BOAkYN0KlSL"
   },
   "source": [
    "### Extract and store metadata of text and images from a document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q9hBPPWs5CMd"
   },
   "source": [
    "You just imported a function called `get_document_metadata()`. This function extracts text and image metadata from a document, and returns two dataframes, namely *text_metadata* and *image_metadata*, as outputs. If you want to find out more about how `get_document_metadata()` function is implemented using Gemini and the embedding models, you can take look at the [source code](https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/use-cases/retrieval-augmented-generation/utils/intro_multimodal_rag_utils.py) directly.\n",
    "\n",
    "The reason for extraction and storing both text metadata and image metadata is that just by using either of the two alone is not sufficient to come out with a relevent answer. For example, the relevant answers could be in visual form within a document, but text-based RAG won't be able to take into consideration of the visual images. You will also be exploring this example later in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PnKru0sBh2rN"
   },
   "source": [
    "At the next step, you will use the function to extract and store metadata of text and images froma document. Please note that the following cell may take a few minutes to complete:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jFgRwzokrP7v"
   },
   "source": [
    "Note:\n",
    "\n",
    "The current implementation works best:\n",
    "\n",
    "* if your documents are a combination of text and images.\n",
    "* if the tables in your documents are available as images.\n",
    "* if the images in the document don't require too much context.\n",
    "\n",
    "Additionally,\n",
    "\n",
    "* If you want to run this on text-only documents, use normal RAG\n",
    "* If your documents contain particular domain knowledge, pass that information in the prompt below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nflT_j-9QzC_"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ Do not send more than 50 pages in the logic below, its not degined to do that and you will get into quota issue. ⚠️</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X8hE0tWD-lf8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We specify the folder containing our PDF files. You can adjust this path\n",
    "# depending on your environment:\n",
    "#  - \"/content/data/\" if you're running in Google Colab/Colab Enterprise, or\n",
    "#  - \"data/\" if running in Vertex AI Workbench.\n",
    "\n",
    "# Explanation for Beginners:\n",
    "# 1) pdf_folder_path: Holds the path to the directory where PDFs are stored.\n",
    "# 2) image_description_prompt: A text prompt used by the Gemini model to\n",
    "#    describe any images extracted from the PDFs.\n",
    "#    - If the PDF has tables, the prompt asks to extract their elements.\n",
    "#    - If it has graphs, it should explain the findings of those graphs.\n",
    "#    - The prompt also instructs not to invent or guess numbers not actually in the image.\n",
    "\n",
    "# Once we have these set, we call get_document_metadata with:\n",
    "#   - The generative model (multimodal_model, i.e., Gemini 1.5 Pro).\n",
    "#   - The folder path (pdf_folder_path).\n",
    "#   - A directory to save images (image_save_dir).\n",
    "#   - A custom prompt (image_description_prompt) that the model will use to describe images.\n",
    "#   - embedding_size=1408 for more expressive embeddings.\n",
    "#   - add_sleep_after_page=True to minimize quota issues by waiting after processing each page.\n",
    "#   - sleep_time_after_page=5 to set the wait duration (in seconds).\n",
    "\n",
    "# After processing, we get two DataFrames:\n",
    "#   1) text_metadata_df: Holds extracted text with embeddings.\n",
    "#   2) image_metadata_df: Holds extracted images with embeddings and descriptions.\n",
    "\n",
    "# pdf_folder_path = \"/content/data/\"  # If running in Google Colab/Colab Enterprise\n",
    "pdf_folder_path = \"data/\"             # If running in Vertex AI Workbench\n",
    "\n",
    "image_description_prompt = \"\"\"Explain what is going on in the image.\n",
    "If it's a table, extract all elements of the table.\n",
    "If it's a graph, explain the findings in the graph.\n",
    "Do not include any numbers that are not mentioned in the image.\n",
    "\"\"\"\n",
    "\n",
    "# We call get_document_metadata to process each PDF, embedding text and images.\n",
    "text_metadata_df, image_metadata_df = get_document_metadata(\n",
    "    multimodal_model,  # Using the Gemini 1.5 Pro model for analysis\n",
    "    pdf_folder_path,\n",
    "    image_save_dir=\"images\",\n",
    "    image_description_prompt=image_description_prompt,\n",
    "    embedding_size=1408,\n",
    "    add_sleep_after_page=True,\n",
    "    sleep_time_after_page=5,\n",
    "    # generation_config=...,  # Optionally specify custom generation settings\n",
    "    # safety_settings=...,    # Optionally specify custom safety settings\n",
    ")\n",
    "\n",
    "print(\"\\n\\n --- Completed processing. ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vQzMm5bNrP7w"
   },
   "outputs": [],
   "source": [
    "# Below are some optional parameters you can pass to the Gemini model via a GenerationConfig object \n",
    "# and safety_settings dictionary. They are currently commented out, but you can uncomment them if needed.\n",
    "\n",
    "# Explanation for Beginners (inline comments):\n",
    "# 1) generation_config: Provides fine-grained control over how the model generates responses.\n",
    "#    - temperature: Higher values produce more diverse responses.\n",
    "#    - max_output_tokens: Limits the length of the generated response.\n",
    "# 2) safety_settings: Allows you to specify thresholds for blocking or filtering certain types of content.\n",
    "#    - BLOCK_NONE means no blocking for that specific harm category (e.g., HARASSMENT, HATE_SPEECH).\n",
    "# 3) If you encounter \"Content has no parts\" or \"Exception occurred while calling gemini\" errors,\n",
    "#    you might lower or remove some of these thresholds. You can pass these parameters to \n",
    "#    the 'get_gemini_response' function or directly to your calls that generate content.\n",
    "\n",
    "# Reference:\n",
    "# - Gemini parameters: https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/gemini\n",
    "# - Safety attribute configuration: https://cloud.google.com/vertex-ai/docs/generative-ai/multimodal/configure-safety-attributes\n",
    "\n",
    "# generation_config = GenerationConfig(\n",
    "#     temperature=0.2,\n",
    "#     max_output_tokens=2048,\n",
    "# )\n",
    "\n",
    "# safety_settings = {\n",
    "#     HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "#     HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "#     HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "#     HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "# }\n",
    "\n",
    "# You can then use them like so:\n",
    "# response = get_gemini_response(\n",
    "#     generative_multimodal_model=multimodal_model,  # or your chosen model\n",
    "#     model_input=some_input_list,\n",
    "#     generation_config=generation_config,\n",
    "#     safety_settings=safety_settings,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "miBBoEXwh2rN"
   },
   "source": [
    "#### Inspect the processed text metadata\n",
    "\n",
    "\n",
    "The following cell will produce a metadata table which describes the different parts of text metadata, including:\n",
    "\n",
    "- **text**: the original text from the page\n",
    "- **text_embedding_page**: the embedding of the original text from the page\n",
    "- **chunk_text**: the original text divided into smaller chunks\n",
    "- **chunk_number**: the index of each text chunk\n",
    "- **text_embedding_chunk**: the embedding of each text chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6t3AIGFar8Mo",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We use the .head() method to display the first five rows of our text_metadata_df.\n",
    "# This DataFrame contains metadata for extracted text from the PDFs, \n",
    "# such as file name, page number, chunked text, and embeddings.\n",
    "\n",
    "text_metadata_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NjIQYI3mh2rO"
   },
   "source": [
    "#### Inspect the processed image metadata\n",
    "\n",
    "The following cell will produce a metadata table which describes the different parts of image metadata, including:\n",
    "* **img_desc**: Gemini-generated textual description of the image.\n",
    "* **mm_embedding_from_text_desc_and_img**: Combined embedding of image and its description, capturing both visual and textual information.\n",
    "* **mm_embedding_from_img_only**: Image embedding without description, for comparison with description-based analysis.\n",
    "* **text_embedding_from_image_description**: Separate text embedding of the generated description, enabling textual analysis and comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tkHtAYIK-y-q",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We use the .head() method to inspect the first five rows of our image_metadata_df.\n",
    "# This DataFrame stores metadata about extracted images from the PDFs, including:\n",
    "#   - 'file_name': Which PDF the image came from\n",
    "#   - 'page_num': The PDF page number\n",
    "#   - 'img_num': The image index on that page\n",
    "#   - 'img_path': The local file path where the extracted image was saved\n",
    "#   - 'img_desc': The description generated by Gemini\n",
    "#   - 'mm_embedding_from_img_only': The image embedding without additional text context\n",
    "#   - 'text_embedding_from_image_description': The text embedding of the generated image description\n",
    "\n",
    "image_metadata_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iBhoOkutUtPr"
   },
   "source": [
    "### Import the helper functions to implement RAG\n",
    "\n",
    "You will be importing the following functions which will be used in the remainder of this notebook to implement RAG:\n",
    "\n",
    "* **get_similar_text_from_query():** Given a text query, finds text from the document which are relevant, using cosine similarity algorithm. It uses text embeddings from the metadata to compute and the results can be filtered by top score, page/chunk number, or embedding size.\n",
    "* **print_text_to_text_citation():** Prints the source (citation) and details of the retrieved text from the `get_similar_text_from_query()` function.\n",
    "* **get_similar_image_from_query():** Given an image path or an image, finds images from the document which are relevant. It uses image embeddings from the metadata.\n",
    "* **print_text_to_image_citation():** Prints the source (citation) and the details of retrieved images from the `get_similar_image_from_query()` function.\n",
    "* **get_gemini_response():** Interacts with a Gemini model to answer questions based on a combination of text and image inputs.\n",
    "* **display_images():**  Displays a series of images provided as paths or PIL Image objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Tngn_vrIKdE1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We import several helper functions from our 'intro_multimodal_rag_utils' module. \n",
    "# Each function serves a specific purpose in our retrieval-augmented workflow:\n",
    "# 1) display_images:              Displays image files inline in a notebook.\n",
    "# 2) get_gemini_response:         Generates text outputs using the Gemini model (multimodal support).\n",
    "# 3) get_similar_image_from_query: Finds images most relevant to a given query (text or image).\n",
    "# 4) get_similar_text_from_query:  Finds text chunks most relevant to a given text query.\n",
    "# 5) print_text_to_image_citation: Prints metadata/citations for matched images (e.g., page number).\n",
    "# 6) print_text_to_text_citation:  Prints metadata/citations for matched text chunks (e.g., chunk number).\n",
    "\n",
    "from intro_multimodal_rag_utils import (\n",
    "    display_images,\n",
    "    get_gemini_response,\n",
    "    get_similar_image_from_query,\n",
    "    get_similar_text_from_query,\n",
    "    print_text_to_image_citation,\n",
    "    print_text_to_text_citation,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c9jGEj6DY1Rj"
   },
   "source": [
    "Before implementing a multimodal RAG, let's take a step back and explore what you can achieve with just text or image embeddings alone. It will help to set the foundation for implementing a multimodal RAG, which you will be doing in the later part of the notebook. You can also use these essential elements together to build applications for multimodal use cases for extracting meaningful information from the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KHuLlEvSKFWt"
   },
   "source": [
    "## Text Search\n",
    "\n",
    "Let's start the search with a simple question and see if the simple text search using text embeddings can answer it. The expected answer is to show the value of basic and diluted net income per share of Google for different share types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "5mrFVhtCut7t",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We create a query string asking for details about basic and diluted net income \n",
    "# per share for Class A, B, and C stock of Google (Alphabet).\n",
    "#\n",
    "# Explanation for Beginners:\n",
    "# 1) The variable `query` holds our user’s request in plain English.\n",
    "# 2) We can later pass this query to functions like `get_similar_text_from_query`\n",
    "#    to find the PDF pages or text chunks mentioning details about Google's\n",
    "#    Class A, B, and C shares. \n",
    "# 3) This is helpful for locating specific financial information (like net income \n",
    "#    per share) within a larger set of documents.\n",
    "\n",
    "query = \"I need details for basic and diluted net income per share of Class A, Class B, and Class C share for google?\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XWw7-AIar-S8"
   },
   "source": [
    "### Search similar text with text query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eEzP6Yyv7N-G",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We match our user query against the text embeddings stored in the \"text_embedding_chunk\"\n",
    "# column of our text_metadata_df DataFrame, and retrieve the top 3 most relevant text chunks.\n",
    "#\n",
    "# Explanation for Beginners:\n",
    "# 1) get_similar_text_from_query: This function calculates similarity scores between the query \n",
    "#    embedding and the embeddings of each text chunk in our DataFrame.\n",
    "# 2) text_metadata_df: A DataFrame that stores the text extracted from the PDFs, \n",
    "#    along with chunked embeddings for easier searching.\n",
    "# 3) column_name=\"text_embedding_chunk\": Specifies that we're comparing the query \n",
    "#    to the text embeddings at a chunk level (rather than page or entire doc).\n",
    "# 4) top_n=3: We only want the 3 best matching chunks for faster inspection.\n",
    "# 5) print_text_to_text_citation: Prints out each matched chunk's source info \n",
    "#    (like file name, page number, and the chunk text itself).\n",
    "\n",
    "matching_results_text = get_similar_text_from_query(\n",
    "    query,\n",
    "    text_metadata_df,\n",
    "    column_name=\"text_embedding_chunk\",\n",
    "    top_n=3,\n",
    "    chunk_text=True,\n",
    ")\n",
    "\n",
    "# We print citations for all matched chunks (print_top=False), \n",
    "# each chunk's text content, file name, and page number.\n",
    "print_text_to_text_citation(\n",
    "    matching_results_text, \n",
    "    print_top=False, \n",
    "    chunk_text=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O0bnOOf2rP70"
   },
   "source": [
    "You can see that the first high score match does have what we are looking for, but upon closer inspection, it mentions that the information is available in the \"following\" table. The table data is available as an image rather than as text, and hence, the chances are you will miss the information unless you can find a way to process images and their data.\n",
    "\n",
    "However, Let's feed the relevant text chunk across the data into the Gemini 1.0 Pro model and see if it can get your desired answer by considering all the chunks across the document. This is like basic text-based RAG implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ORCistIdDWoE",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We print a separator to indicate we’re about to generate a final answer based on matched text.\n",
    "print(\"\\n **** Result: ***** \\n\")\n",
    "\n",
    "# Explanation for Beginners:\n",
    "# 1) 'matching_results_text': This dictionary contains the chunks of text that were deemed most relevant to our query.\n",
    "# 2) We create a single 'context' string by joining all chunk_text values from the matched results, separated by newlines.\n",
    "# 3) 'instruction': We build a prompt instructing the model to answer only using the provided context. If the context doesn’t\n",
    "#    have the necessary info, we tell the model to respond with \"not available in the context.\"\n",
    "# 4) 'model_input': This is the final prompt we’ll pass to our model.\n",
    "\n",
    "# Combine the text chunks into a single context string for the model to reference.\n",
    "context = \"\\n\".join(\n",
    "    [value[\"chunk_text\"] for key, value in matching_results_text.items()]\n",
    ")\n",
    "\n",
    "instruction = f\"\"\"Answer the question with the given context.\n",
    "If the information is not available in the context, just return \"not available in the context\".\n",
    "Question: {query}\n",
    "Context: {context}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# 'model_input' is what we'll feed to the 'get_gemini_response' function.\n",
    "model_input = instruction\n",
    "\n",
    "# We generate a response with our text-only model (Gemini 1.0 Pro),\n",
    "# configured for a relatively deterministic output (temperature=0.2).\n",
    "# The function 'get_gemini_response' will stream partial outputs in real-time.\n",
    "get_gemini_response(\n",
    "    text_model,  # Our text-oriented Gemini 1.0 Pro model\n",
    "    model_input=model_input,\n",
    "    stream=True,\n",
    "    generation_config=GenerationConfig(temperature=0.2),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v8ux39zFqyLh"
   },
   "source": [
    "You can see that it returned:\n",
    "\n",
    "*\"The provided context does not include the details for basic and diluted net income per share of Class A, Class B, and Class C share for google.\n",
    "\"*\n",
    "\n",
    "This is expected as discussed previously. No other text chunk (total 3) had the information you sought.\n",
    "This is because the information is only available in the images rather than in the text part of the document. Next, let's see if you can solve this problem by leveraging Gemini 1.0 Pro Vision and Multimodal Embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2itkRuikq_g6"
   },
   "source": [
    "Note: We handcrafted examples in our document to simulate real-world cases where information is often embedded in charts, table, graphs, and other image-based elements and unavailable as plain text.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uXm271jdD-Rl"
   },
   "source": [
    "### Search similar images with text query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oPxwfyVrr9-G"
   },
   "source": [
    "Since plain text search didn't provide the desired answer and the information may be visually represented in a table or another image format, you will use multimodal capability of Gemini 1.0 Pro Vision model for the similar task. The goal here also is to find an image similar to the text query. You may also print the citations to verify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "0sRFH6tJlpXQ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We define a query asking for details about basic and diluted net income per share\n",
    "# for Google’s Class A, B, and C stock.\n",
    "#\n",
    "# Explanation for Beginners:\n",
    "# 1) The variable `query` stores a user-requested question in plain English.\n",
    "# 2) In subsequent cells, we can pass `query` to functions like \n",
    "#    `get_similar_text_from_query` for text-based matching \n",
    "#    or generate a model response with `get_gemini_response`.\n",
    "# 3) If our dataset includes information on Google's financial statements, \n",
    "#    these references can help us locate relevant text or images in the PDFs.\n",
    "\n",
    "query = \"I need details for basic and diluted net income per share of Class A, Class B, and Class C share for google?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "knj4qQ4xni24",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We look for images that are relevant to our user’s query about net income per share\n",
    "# for Google’s Class A, B, and C. We do this by comparing the query text embedding \n",
    "# with the text embedding of each image’s description (i.e., the \"text_embedding_from_image_description\" column).\n",
    "#\n",
    "# Explanation for Beginners:\n",
    "# 1) get_similar_image_from_query: Takes our text query and searches for matching images \n",
    "#    by comparing the query text embedding to each image's description embedding.\n",
    "# 2) text_metadata_df, image_metadata_df: Contain all our extracted text and image metadata, respectively.\n",
    "# 3) image_emb=False: We’re using a text-based query, not an image-based query.\n",
    "# 4) top_n=3: Return the top 3 most relevant images for inspection.\n",
    "# 5) embedding_size=1408: Matches the embedding dimension used earlier for our images.\n",
    "\n",
    "matching_results_image = get_similar_image_from_query(\n",
    "    text_metadata_df,\n",
    "    image_metadata_df,\n",
    "    query=query,\n",
    "    column_name=\"text_embedding_from_image_description\",  # Use image descriptions’ embeddings\n",
    "    image_emb=False,\n",
    "    top_n=3,\n",
    "    embedding_size=1408,\n",
    ")\n",
    "\n",
    "# We could optionally print citations for the matched images \n",
    "# with print_text_to_image_citation, but it's commented out here:\n",
    "# Markdown(print_text_to_image_citation(matching_results_image, print_top=True))\n",
    "\n",
    "print(\"\\n **** Result: ***** \\n\")\n",
    "\n",
    "# We display the top matching image inline in our environment. \n",
    "# \"image_object\" is typically a Vertex AI Image object or PIL image \n",
    "# that can be displayed in notebooks.\n",
    "display(matching_results_image[0][\"image_object\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SnFdFkWEtYrF"
   },
   "source": [
    "Bingo! It found exactly what you were looking for. You wanted the details on Google's Class A, B, and C shares' basic and diluted net income, and guess what? This image fits the bill perfectly thanks to its descriptive metadata using Gemini.\n",
    "\n",
    "You can also send the image and its description to Gemini 1.0 Pro Vision and get the answer as JSON:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ax6ooI0rP70",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"\\n **** Result: ***** \\n\")\n",
    "\n",
    "# Explanation for Beginners:\n",
    "# 1) We build a prompt that incorporates the top matching image (and its description) \n",
    "#    from our query-based image search.\n",
    "# 2) 'context' includes the image object and the image description, \n",
    "#    which can help the model answer the user’s query in a more visual-aware manner.\n",
    "# 3) 'instruction': We ask for a JSON-formatted answer, containing only the final response's value \n",
    "#    without extra text or commentary.\n",
    "# 4) 'model_input': This is the final prompt we’ll feed to our multimodal model (Gemini 1.5 Pro Flash).\n",
    "# 5) We wrap the final model response in 'Markdown(...)' to render it in a rich text format.\n",
    "\n",
    "context = f\"\"\"Image: {matching_results_image[0]['image_object']}\n",
    "Description: {matching_results_image[0]['image_description']}\n",
    "\"\"\"\n",
    "\n",
    "instruction = f\"\"\"Answer the question in JSON format with the given context of Image and its Description. Only include value.\n",
    "Question: {query}\n",
    "Context: {context}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "model_input = instruction\n",
    "\n",
    "# We make a call to our Gemini 1.5 Pro Flash model with streaming enabled. \n",
    "# The response is rendered as Markdown for improved readability.\n",
    "Markdown(\n",
    "    get_gemini_response(\n",
    "        multimodal_model_flash,    # Faster variant of the Gemini 1.5 Pro model\n",
    "        model_input=model_input,\n",
    "        stream=True,               # Stream partial text responses in real time\n",
    "        generation_config=GenerationConfig(\n",
    "            temperature=1          # Higher temperature for more creative/less deterministic output\n",
    "        ),\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eAxSk640rP70",
    "tags": []
   },
   "outputs": [],
   "source": [
    "## We display the citation details for the top matching image, including the file name,\n",
    "## page number, and the Gemini-generated image description. By looking at the \n",
    "## \"image description,\" we can see how it matched our text query.\n",
    "\n",
    "## Explanation for Beginners:\n",
    "## 1) The function `print_text_to_image_citation` prints out metadata for images \n",
    "##    that were matched to our query—like where in the PDF the image came from,\n",
    "##    and the text description that was generated.\n",
    "## 2) Setting `print_top=True` means we'll see only the top-matching image.\n",
    "\n",
    "Markdown(\n",
    "    print_text_to_image_citation(\n",
    "        matching_results_image,\n",
    "        print_top=True\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oDd9rE4NrRod"
   },
   "source": [
    "## Image Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJL6ElyEy4mc"
   },
   "source": [
    "### Search similar image with image query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ReKjHleFxUu9"
   },
   "source": [
    "Imagine searching for images, but instead of typing words, you use an actual image as the clue. You have a table with numbers about the cost of revenue for two years, and you want to find other images that look like it, from the same document or across multiple documents.\n",
    "\n",
    "Think of it like searching with a mini-map instead of a written address. It's a different way to ask, \"Show me more stuff like this\". So, instead of typing \"cost of revenue 2020 2021 table\", you show a picture of that table and say, \"Find me more like this\"\n",
    "\n",
    "For demonstration purposes, we will only be finding similar images that show the cost of revenue or similar values in a single document below. However, you can scale this design pattern to match (find relevant images) across multiple documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DJhhS5eZw7QI",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We specify a local file path to an image called \"tac_table_revenue.png.\" \n",
    "# The goal is to see if this image (e.g., a table) can be matched against \n",
    "# images in our metadata to find similar tables.\n",
    "\n",
    "# Explanation for Beginners:\n",
    "# 1) image_query_path: Holds the path to our user-provided image file.\n",
    "# 2) The image might be a table screenshot or an excerpt from the same PDF source.\n",
    "#    We'll attempt to find similar images in our existing metadata.\n",
    "# 3) Image.load_from_file(...): This function from Vertex AI’s Image class allows us \n",
    "#    to load the image so we can display it or embed it for similarity queries.\n",
    "\n",
    "image_query_path = \"tac_table_revenue.png\"\n",
    "\n",
    "# We print a short message indicating we’re about to show the user’s input image.\n",
    "print(\"***Input image from user:***\")\n",
    "\n",
    "# Finally, we display the input image inline (in a Jupyter notebook, for instance).\n",
    "# This gives us a visual reference of the user’s query image before searching \n",
    "# for similar images in the metadata.\n",
    "Image.load_from_file(image_query_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3zBTtGChTmrd"
   },
   "source": [
    "You expect to find tables (as images) that are similar in terms of \"Other/Total cost of revenues.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nZcU7vZC-8vr",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We attempt to find images within our existing metadata that resemble a user-provided image\n",
    "# (image_query_path). We do this by comparing the embeddings of our query image to the \n",
    "# \"mm_embedding_from_img_only\" embeddings stored in image_metadata_df.\n",
    "#\n",
    "# Explanation for Beginners:\n",
    "# 1) text_metadata_df, image_metadata_df: DataFrames where we store text and image data, respectively.\n",
    "# 2) query (optional): If we want to combine text filtering with image similarity, we can keep this \n",
    "#    parameter. Otherwise, it can be left empty or be a generic text.\n",
    "# 3) image_emb=True: Informs the function that we are using an image-based query, not a text-based query.\n",
    "# 4) image_query_path=image_query_path: The local path to the user’s input image.\n",
    "# 5) column_name=\"mm_embedding_from_img_only\": This column stores embeddings that we generated for\n",
    "#    each image without any text context. We compare them to the query image’s embedding.\n",
    "# 6) top_n=3: We only want the top 3 matches to our query image.\n",
    "# 7) embedding_size=1408: This is the size (dimensionality) of the embeddings we generated for the \n",
    "#    images during preprocessing.\n",
    "\n",
    "matching_results_image = get_similar_image_from_query(\n",
    "    text_metadata_df,\n",
    "    image_metadata_df,\n",
    "    query=query,  # Optionally use the query text for filtering\n",
    "    column_name=\"mm_embedding_from_img_only\",  # Compare embeddings in this column to our query image\n",
    "    image_emb=True,  # We are performing an image-based query\n",
    "    image_query_path=image_query_path,  # The path to the input image from the user\n",
    "    top_n=3,  # Return up to 3 most similar images\n",
    "    embedding_size=1408,  # Size of the embeddings used for images\n",
    ")\n",
    "\n",
    "print(\"\\n **** Result: ***** \\n\")\n",
    "\n",
    "# We now display the top matching image. \"image_object\" typically holds a \n",
    "# PIL.Image object or a format compatible with IPython’s display methods.\n",
    "display(\n",
    "    matching_results_image[0][\"image_object\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uhT17rke15XY"
   },
   "source": [
    "It did find a similar-looking image (table), which gives more detail about different revenue, expenses, income, and a few more details based on the given image. More importantly, both tables show numbers related to the \"cost of revenue.\"\n",
    "\n",
    "You can also print the citation to see what it has matched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mksXQoezweg0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We display citation details for the top matching image found by our image query.\n",
    "# This reveals where in the original PDFs (file name, page number) the similar image \n",
    "# was discovered, along with a brief description if available.\n",
    "#\n",
    "# Explanation for Beginners:\n",
    "# 1) print_text_to_image_citation: \n",
    "#    A function that prints out metadata for each matched image,\n",
    "#    including file paths, page numbers, similarity scores, etc.\n",
    "# 2) print_top=True: \n",
    "#    Instructs the function to only show the details of the single highest-scoring match.\n",
    "\n",
    "print_text_to_image_citation(\n",
    "    matching_results_image,\n",
    "    print_top=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VJWnhDJwI-uO",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We display the top two matched images (index 0 and 1) returned by our image query.\n",
    "# The 'display_images' function will show each image inline, resizing them to 50% \n",
    "# of their original dimensions for easier viewing.\n",
    "#\n",
    "# Explanation for Beginners:\n",
    "# 1) matched_results_image is a list (or dict) of matched images, \n",
    "#    each containing metadata like file path, similarity scores, etc.\n",
    "# 2) matching_results_image[0][\"img_path\"] is the file path for the top matched image.\n",
    "# 3) The second image is matching_results_image[1][\"img_path\"].\n",
    "# 4) 'resize_ratio=0.5' scales the images down by half, making them more manageable \n",
    "#    in a notebook or console environment.\n",
    "\n",
    "print(\"---------------Matched Images------------------\\n\")\n",
    "display_images(\n",
    "    [\n",
    "        matching_results_image[0][\"img_path\"],  # File path for the top image\n",
    "        matching_results_image[1][\"img_path\"],  # File path for the second best match\n",
    "    ],\n",
    "    resize_ratio=0.5,  # Resize images to 50% of original size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JvwZIgD84CNc"
   },
   "source": [
    "The ability to identify similar text and images based on user input, using Gemini and embeddings, forms a crucial foundation for development of multimodal RAG systems, which you explore in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lUnsv5Co6pJF"
   },
   "source": [
    "### Comparative reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1AFbqHiz5vvo"
   },
   "source": [
    "Next, let's apply what you have done so far to doing comparative reasoning.\n",
    "\n",
    "For this example:\n",
    "\n",
    "Step 1: You will search all the images for a specific query\n",
    "\n",
    "Step 2: Send those images to Gemini 1.0 Pro Vision to ask multiple questions, where it has to compare and provide you with answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "E6AHCSwojyX0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We search for images displaying a \"Google Class A cumulative 5-year total return\" graph.\n",
    "# This time, we compare the user's text query to image descriptions rather than performing\n",
    "# an image-based query.\n",
    "\n",
    "# Explanation for Beginners:\n",
    "# 1) get_similar_image_from_query: Our function that identifies images most relevant \n",
    "#    to a given query.\n",
    "# 2) text_metadata_df, image_metadata_df: DataFrames containing embedded text and image data.\n",
    "# 3) query: The user’s text query describing the type of image they seek.\n",
    "# 4) column_name=\"text_embedding_from_image_description\": We match the query against \n",
    "#    each image’s text description embedding.\n",
    "# 5) image_emb=False: Tells the function we are using text as our query, not an image.\n",
    "# 6) top_n=3: Returns the top 3 images most relevant to the query.\n",
    "# 7) embedding_size=1408: The dimensionality of the embeddings we used for images.\n",
    "\n",
    "matching_results_image_query_1 = get_similar_image_from_query(\n",
    "    text_metadata_df,\n",
    "    image_metadata_df,\n",
    "    query=\"Show me all the graphs that shows Google Class A cumulative 5-year total return\",\n",
    "    column_name=\"text_embedding_from_image_description\",\n",
    "    image_emb=False,\n",
    "    top_n=3,\n",
    "    embedding_size=1408,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3FRXk-n0rP71",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We display the matched images that are most relevant to our text query about \n",
    "# \"graphs showing Google Class A cumulative 5-year total return.\"\n",
    "#\n",
    "# Explanation for Beginners:\n",
    "# 1) matching_results_image_query_1 is a dictionary or list of images \n",
    "#    that our system found most relevant to the query.\n",
    "# 2) The function display_images takes a list of image paths (or PIL Image objects)\n",
    "#    and renders them inline, optionally resizing them to a fraction (50%) \n",
    "#    of their original size for better viewing in a notebook.\n",
    "# 3) matching_results_image_query_1[0][\"img_path\"] is the file path of the top \n",
    "#    matching image. Similarly, matching_results_image_query_1[1][\"img_path\"] \n",
    "#    is the second-best match.\n",
    "\n",
    "print(\"---------------Matched Images------------------\\n\")\n",
    "display_images(\n",
    "    [\n",
    "        matching_results_image_query_1[0][\"img_path\"],\n",
    "        matching_results_image_query_1[1][\"img_path\"],\n",
    "    ],\n",
    "    resize_ratio=0.5,  # Resize images to 50% of their original dimensions\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fSR_JWkSC_7p",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We build a prompt that instructs the Gemini model (Gemini 1.5 Pro) to:\n",
    "# 1) Compare two images labeled as Image_1 and Image_2.\n",
    "# 2) Use the extracted text (extracted by Gemini for each image) as additional context.\n",
    "# 3) Answer specific questions about Class A shares, differences between graphs, \n",
    "#    and how they compare to certain indices like S&P 500.\n",
    "#\n",
    "# Explanation for Beginners:\n",
    "# 1) matching_results_image_query_1: This dictionary holds the top matches for our \n",
    "#    text-based image query. Each matched image has its own \"image_object\" and \"image_description.\"\n",
    "# 2) We create 'prompt': This variable combines both images and their descriptions \n",
    "#    under \"Context,\" then asks a set of questions. The instructions ask the model to \n",
    "#    carefully think through the steps in bullet points, providing an explainable answer.\n",
    "# 3) Finally, we feed this 'prompt' as the input list to get_gemini_response, along with:\n",
    "#    - The model instance (multimodal_model).\n",
    "#    - A GenerationConfig object specifying temperature=1 for a more creative, flexible output.\n",
    "# 4) 'rich_Markdown' is used to format the model’s streamed response in Markdown for better readability.\n",
    "\n",
    "prompt = f\"\"\" Instructions: Compare the images and the Gemini extracted text provided as Context: to answer Question:\n",
    "Make sure to think thoroughly before answering the question and put the necessary steps to arrive at the answer in bullet points for easy explainability.\n",
    "\n",
    "Context:\n",
    "Image_1: {matching_results_image_query_1[0][\"image_object\"]}\n",
    "gemini_extracted_text_1: {matching_results_image_query_1[0]['image_description']}\n",
    "Image_2: {matching_results_image_query_1[1][\"image_object\"]}\n",
    "gemini_extracted_text_2: {matching_results_image_query_1[2]['image_description']}\n",
    "\n",
    "Question:\n",
    " - Key findings of Class A share?\n",
    " - What are the critical differences between the graphs for Class A Share?\n",
    " - What are the key findings of Class A shares concerning the S&P 500?\n",
    " - Which index best matches Class A share performance closely where Google is not already a part? Explain the reasoning.\n",
    " - Identify key chart patterns in both graphs.\n",
    " - Which index best matches Class A share performance closely where Google is not already a part? Explain the reasoning.\n",
    "\"\"\"\n",
    "\n",
    "rich_Markdown(\n",
    "    get_gemini_response(\n",
    "        multimodal_model,  # Our Gemini 1.5 Pro model for text + image analysis\n",
    "        model_input=[prompt],\n",
    "        stream=True,  # Stream partial text responses in real-time\n",
    "        generation_config=GenerationConfig(\n",
    "            temperature=1  # Higher temperature for a more creative, expansive response\n",
    "        ),\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7dhazjyNLSGT"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ Disclaimer: This is not a real investment advise and should not be taken seriously!! ⚠️</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "efJPPrzRhvIT"
   },
   "source": [
    "## Multimodal retrieval augmented generation (RAG)\n",
    "\n",
    "Let's bring everything together to implement multimodal RAG. You will use all the elements that you've explored in previous sections to implement the multimodal RAG. These are the steps:\n",
    "\n",
    "* **Step 1:** The user gives a query in text format where the expected information is available in the document and is embedded in images and text.\n",
    "* **Step 2:** Find all text chunks from the pages in the documents using a method similar to the one you explored in `Text Search`.\n",
    "* **Step 3:** Find all similar images from the pages based on the user query matched with `image_description` using a method identical to the one you explored in `Image Search`.\n",
    "* **Step 4:** Combine all similar text and images found in steps 2 and 3 as `context_text` and `context_images`.\n",
    "* **Step 5:** With the help of Gemini, we can pass the user query with text and image context found in steps 2 & 3. You can also add a specific instruction the model should remember while answering the user query.\n",
    "* **Step 6:** Gemini produces the answer, and you can print the citations to check all relevant text and images used to address the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EI62Hzuw_0_b"
   },
   "source": [
    "### Step 1: User query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "XvTKFwOPHLQ_",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We define a text query containing various questions related to Google’s Class A shares,\n",
    "# financial metrics, and the impact of Covid. No images are being passed this time.\n",
    "#\n",
    "# Explanation for Beginners:\n",
    "# 1) 'query' is a variable that holds our user’s complex set of questions, which we can\n",
    "#    pass to functions like 'get_similar_text_from_query' to retrieve relevant information\n",
    "#    from our text metadata.\n",
    "# 2) These questions focus on financial and operational aspects such as revenue, \n",
    "#    operating expenses, net income, and the effect of Covid in 2020.\n",
    "# 3) The user might also inquire about financial definitions (e.g., deferred income taxes),\n",
    "#    or numerical changes in data (like a 41% increase in revenue).\n",
    "\n",
    "query = \"\"\"Questions:\n",
    " - What are the critical difference between various graphs for Class A Share?\n",
    " - Which index best matches Class A share performance closely where Google is not already a part? Explain the reasoning.\n",
    " - Identify key chart patterns for Google Class A shares.\n",
    " - What is cost of revenues, operating expenses and net income for 2020. Do mention the percentage change\n",
    " - What was the effect of Covid in the 2020 financial year?\n",
    " - What are the total revenues for APAC and USA for 2021?\n",
    " - What is deferred income taxes?\n",
    " - How do you compute net income per share?\n",
    " - What drove percentage change in the consolidated revenue and cost of revenue for the year 2021 and was there any effect of Covid?\n",
    " - What is the cause of 41% increase in revenue from 2020 to 2021 and how much is dollar change?\n",
    " \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UUqlkKUaYvZA"
   },
   "source": [
    "### Step 2: Get all relevant text chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "r65yBb5gR_NG",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We retrieve the top 10 text chunks that are most relevant to our new query,\n",
    "# which contains multiple questions regarding Google Class A shares, revenues,\n",
    "# operating expenses, the impact of COVID, etc.\n",
    "#\n",
    "# Explanation for Beginners:\n",
    "# 1) get_similar_text_from_query: This function compares our 'query' \n",
    "#    with all the text chunks in 'text_metadata_df' using embedded vector similarity.\n",
    "# 2) text_metadata_df: DataFrame containing extracted and embedded PDF text.\n",
    "# 3) column_name=\"text_embedding_chunk\": Specifies we are searching against\n",
    "#    chunk-level text embeddings.\n",
    "# 4) top_n=10: Returns the top 10 chunks that best match the query.\n",
    "# 5) chunk_text=True: Returns individual chunk text instead of entire page text.\n",
    "\n",
    "matching_results_chunks_data = get_similar_text_from_query(\n",
    "    query,\n",
    "    text_metadata_df,\n",
    "    column_name=\"text_embedding_chunk\",\n",
    "    top_n=10,\n",
    "    chunk_text=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mIgXgVIpYzxj"
   },
   "source": [
    "### Step 3: Get all relevant images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "wzu5Gf4yR_J4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We search for images that might also be relevant to our text-based query by \n",
    "# matching the query against the image descriptions (their text embeddings).\n",
    "#\n",
    "# Explanation for Beginners:\n",
    "# 1) get_similar_image_from_query: Searches for images whose descriptions (or embedded text) \n",
    "#    best match the user's query.\n",
    "# 2) text_metadata_df, image_metadata_df: Our DataFrames containing extracted text and image metadata, respectively.\n",
    "# 3) query: The user's textual query about finances, Class A shares, Covid impact, etc.\n",
    "# 4) column_name=\"text_embedding_from_image_description\": We compare the query’s embeddings \n",
    "#    to each image’s description embeddings.\n",
    "# 5) image_emb=False: Indicates we're using a text query, not an image-based query.\n",
    "# 6) top_n=10: Return the top 10 most relevant images.\n",
    "# 7) embedding_size=1408: The size of the embeddings we used for images.\n",
    "\n",
    "matching_results_image_fromdescription_data = get_similar_image_from_query(\n",
    "    text_metadata_df,\n",
    "    image_metadata_df,\n",
    "    query=query,\n",
    "    column_name=\"text_embedding_from_image_description\",\n",
    "    image_emb=False,\n",
    "    top_n=10,\n",
    "    embedding_size=1408,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RhUpWlGAY2uG"
   },
   "source": [
    "### Step 4: Create context_text and context_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "B_EEuuLCe6Y5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We gather the text chunks and images deemed relevant by our similarity searches.\n",
    "# Explanation for Beginners:\n",
    "# 1) matching_results_chunks_data and matching_results_image_fromdescription_data:\n",
    "#    These dictionaries contain the most relevant text chunks and images, respectively,\n",
    "#    based on our user query.\n",
    "# 2) context_text: We collect the actual chunk text from each matching result\n",
    "#    and then join them together (separated by newlines).\n",
    "# 3) context_images: We combine each relevant image and its Gemini-generated caption\n",
    "#    into a single list that could be passed to a model or used for further analysis.\n",
    "\n",
    "# Combine all the selected relevant text chunks.\n",
    "context_text = []\n",
    "for key, value in matching_results_chunks_data.items():\n",
    "    context_text.append(value[\"chunk_text\"])\n",
    "\n",
    "# Join chunk texts with a newline separator so we have a coherent context block.\n",
    "final_context_text = \"\\n\".join(context_text)\n",
    "\n",
    "# Combine all the relevant images and their descriptions generated by Gemini.\n",
    "context_images = []\n",
    "for key, value in matching_results_image_fromdescription_data.items():\n",
    "    # We extend our list with an identifier (\"Image:\"), the image object,\n",
    "    # a label for the caption (\"Caption:\"), and the text description.\n",
    "    context_images.extend([\n",
    "        \"Image: \",\n",
    "        value[\"image_object\"],\n",
    "        \"Caption: \",\n",
    "        value[\"image_description\"]\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rHrtodcBAEu9"
   },
   "source": [
    "### Step 5: Pass context to Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aZuhtJu7fW4n",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We build a prompt that instructs the Gemini model to:\n",
    "# 1) Analyze both text (final_context_text) and images (context_images) together.\n",
    "# 2) Answer a list of questions (contained in 'query') in a thorough, step-by-step manner.\n",
    "# 3) Return \"Not enough context to answer\" if the context doesn't provide the necessary information.\n",
    "#\n",
    "# Explanation for Beginners:\n",
    "# 1) 'prompt': We embed our text context and image context, along with instructions \n",
    "#    on how to respond. This ensures the model has comprehensive information \n",
    "#    when generating its answer.\n",
    "# 2) final_context_text: A joined string of relevant text chunks.\n",
    "# 3) context_images: A list combining image objects and their captions, \n",
    "#    which the model can also interpret.\n",
    "# 4) 'model_input': We pass this single prompt into 'get_gemini_response' as a list.\n",
    "# 5) 'generation_config=GenerationConfig(temperature=1)': \n",
    "#    A higher temperature often produces more creative or exploratory answers.\n",
    "\n",
    "prompt = f\"\"\" Instructions: Compare the images and the text provided as Context: to answer multiple Question:\n",
    "Make sure to think thoroughly before answering the question and put the necessary steps to arrive at the answer in bullet points for easy explainability.\n",
    "If unsure, respond, \"Not enough context to answer\".\n",
    "\n",
    "Context:\n",
    " - Text Context:\n",
    " {final_context_text}\n",
    " - Image Context:\n",
    " {context_images}\n",
    "\n",
    "{query}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# We call 'get_gemini_response' with our prompt (as the sole item in model_input) \n",
    "# and enable streaming so we can receive the response in real time. \n",
    "# Then we wrap it in `rich_Markdown` for nicer formatting in a Jupyter environment.\n",
    "rich_Markdown(\n",
    "    get_gemini_response(\n",
    "        multimodal_model,\n",
    "        model_input=[prompt],\n",
    "        stream=True,\n",
    "        generation_config=GenerationConfig(temperature=1),\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L0FtXYl1fzKh"
   },
   "source": [
    "### Step 6: Print citations and references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IYRLQ47or1I8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We display the first four images that were deemed relevant by our query-to-description \n",
    "# matching. Each image might provide context that supports the model’s answers.\n",
    "#\n",
    "# Explanation for Beginners:\n",
    "# 1) matching_results_image_fromdescription_data is a collection of images (and \n",
    "#    their metadata) identified as being closely related to our text query.\n",
    "# 2) display_images takes a list of paths (or PIL Image objects) \n",
    "#    and displays them inline in a Jupyter environment.\n",
    "# 3) resize_ratio=0.5 scales the images to half of their original size, \n",
    "#    making them more manageable for viewing.\n",
    "\n",
    "print(\"---------------Matched Images------------------\\n\")\n",
    "\n",
    "display_images(\n",
    "    [\n",
    "        matching_results_image_fromdescription_data[0][\"img_path\"],  # Path to the top-matching image\n",
    "        matching_results_image_fromdescription_data[1][\"img_path\"],  # 2nd most relevant\n",
    "        matching_results_image_fromdescription_data[2][\"img_path\"],  # 3rd most relevant\n",
    "        matching_results_image_fromdescription_data[3][\"img_path\"],  # 4th most relevant\n",
    "    ],\n",
    "    resize_ratio=0.5,  # Scale images to 50% of their original size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "buwd_gp6HJ5K",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We print out citations for all matched images (print_top=False), which provides\n",
    "# detailed metadata about each image, such as:\n",
    "#   - The file name of the PDF\n",
    "#   - The page number where the image was extracted\n",
    "#   - The path to the image file\n",
    "#   - A short description generated by Gemini (the \"image_description\")\n",
    "#\n",
    "# Explanation for Beginners:\n",
    "# 1) print_text_to_image_citation: This function displays relevant metadata\n",
    "#    for each image, helping us see where the images originated and why\n",
    "#    they are considered relevant.\n",
    "# 2) print_top=False: We show citations for all matched images instead\n",
    "#    of just the highest-scoring match.\n",
    "\n",
    "print_text_to_image_citation(\n",
    "    matching_results_image_fromdescription_data,\n",
    "    print_top=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "06vYM4MOHJ1-",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We print citations for the matched text chunks, providing information about:\n",
    "#   - File names\n",
    "#   - Page numbers\n",
    "#   - The actual text chunks\n",
    "#\n",
    "# Explanation for Beginners:\n",
    "# 1) print_text_to_text_citation: Displays helpful context about each matched text chunk, \n",
    "#    such as which PDF file and page it came from.\n",
    "# 2) print_top=False: Means we show all matches rather than just the best one.\n",
    "# 3) chunk_text=True: We want to see the actual text chunk in addition to the metadata.\n",
    "\n",
    "print_text_to_text_citation(\n",
    "    matching_results_chunks_data,\n",
    "    print_top=False,\n",
    "    chunk_text=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KwNrHCqbi3xi"
   },
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05jynhZnkgxn"
   },
   "source": [
    "Congratulations on making it through this multimodal RAG notebook!\n",
    "\n",
    "While multimodal RAG can be quite powerful, note that it can face some limitations:\n",
    "\n",
    "* **Data dependency:** Needs high-quality paired text and visuals.\n",
    "* **Computationally demanding:** Processing multimodal data is resource-intensive.\n",
    "* **Domain specific:** Models trained on general data may not shine in specialized fields like medicine.\n",
    "* **Black box:** Understanding how these models work can be tricky, hindering trust and adoption.\n",
    "\n",
    "\n",
    "Despite these challenges, multimodal RAG represents a significant step towards search and retrieval systems that can handle diverse, multimodal data."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "intro_multimodal_rag.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m126",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m126"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
